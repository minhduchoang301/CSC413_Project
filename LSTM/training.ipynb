{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn-7uAudb2H2"
      },
      "source": [
        "## Documentation and Summary\n",
        "- Multivariate input and output: Using x_{1,2,...,T} and y_{1, 2,…, T-1} to predict y_T (4 dimensions to 1 dimension)\n",
        "- The data is normalized before training and testing\n",
        "- I have experimented different hyperparameters and the selected values here is: hidden states =20, lstm layer (stacked)= 1, batch size = 16, timestep = 100, learning rate = 0.001\n",
        "- Outstanding problem:  NA\n",
        "- Loss function (and also used as metrics here): MSE or MLE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gxSqpjdMqkz"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bqL-2OkQMsO9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rUSqgsBgp4oM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ohTYVs6Q5D"
      },
      "source": [
        "## Data Prepration: Visualize Data and Delete the date column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "vJdBUh4Capdz",
        "outputId": "5ecd9476-7b68-4635-8fe3-3bf5abdc7ec4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date         Open         High          Low        Close  \\\n",
              "0     2010-07-16     0.049510     0.049510     0.049510     0.049510   \n",
              "1     2010-07-17     0.049510     0.085850     0.059410     0.085840   \n",
              "2     2010-07-18     0.085840     0.093070     0.077230     0.080800   \n",
              "3     2010-07-19     0.080800     0.081810     0.074260     0.074740   \n",
              "4     2010-07-20     0.074740     0.079210     0.066340     0.079210   \n",
              "...          ...          ...          ...          ...          ...   \n",
              "3196  2019-04-16  5212.810059  5270.759766  5190.810059  5236.259766   \n",
              "3197  2019-04-17  5236.259766  5317.779785  5232.990234  5289.750000   \n",
              "3198  2019-04-18  5289.750000  5353.009766  5212.180176  5295.529785   \n",
              "3199  2019-04-19  5295.529785  5366.600098  5274.450195  5326.180176   \n",
              "3200  2019-04-20  5326.180176  5348.589844  5232.770020  5305.740234   \n",
              "\n",
              "        Adj Close     Volume  \n",
              "0        0.049510          0  \n",
              "1        0.085840          5  \n",
              "2        0.080800         49  \n",
              "3        0.074740         20  \n",
              "4        0.079210         42  \n",
              "...           ...        ...  \n",
              "3196  5236.259766  194699629  \n",
              "3197  5289.750000  187312150  \n",
              "3198  5295.529785  158699049  \n",
              "3199  5326.180176  123577001  \n",
              "3200  5305.740234  134681237  \n",
              "\n",
              "[3201 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3161c97-deb6-4796-9d49-44332526b5bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-07-16</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-07-17</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.085850</td>\n",
              "      <td>0.059410</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-07-18</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.093070</td>\n",
              "      <td>0.077230</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-07-19</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.081810</td>\n",
              "      <td>0.074260</td>\n",
              "      <td>0.074740</td>\n",
              "      <td>0.074740</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-07-20</td>\n",
              "      <td>0.074740</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>0.066340</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>2019-04-16</td>\n",
              "      <td>5212.810059</td>\n",
              "      <td>5270.759766</td>\n",
              "      <td>5190.810059</td>\n",
              "      <td>5236.259766</td>\n",
              "      <td>5236.259766</td>\n",
              "      <td>194699629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>2019-04-17</td>\n",
              "      <td>5236.259766</td>\n",
              "      <td>5317.779785</td>\n",
              "      <td>5232.990234</td>\n",
              "      <td>5289.750000</td>\n",
              "      <td>5289.750000</td>\n",
              "      <td>187312150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>2019-04-18</td>\n",
              "      <td>5289.750000</td>\n",
              "      <td>5353.009766</td>\n",
              "      <td>5212.180176</td>\n",
              "      <td>5295.529785</td>\n",
              "      <td>5295.529785</td>\n",
              "      <td>158699049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3199</th>\n",
              "      <td>2019-04-19</td>\n",
              "      <td>5295.529785</td>\n",
              "      <td>5366.600098</td>\n",
              "      <td>5274.450195</td>\n",
              "      <td>5326.180176</td>\n",
              "      <td>5326.180176</td>\n",
              "      <td>123577001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>2019-04-20</td>\n",
              "      <td>5326.180176</td>\n",
              "      <td>5348.589844</td>\n",
              "      <td>5232.770020</td>\n",
              "      <td>5305.740234</td>\n",
              "      <td>5305.740234</td>\n",
              "      <td>134681237</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3201 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3161c97-deb6-4796-9d49-44332526b5bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3161c97-deb6-4796-9d49-44332526b5bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3161c97-deb6-4796-9d49-44332526b5bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/bitcoin_yahoo.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L-EQpru7QmTz"
      },
      "outputs": [],
      "source": [
        "del df['Date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SZTZWm1qOmPC"
      },
      "outputs": [],
      "source": [
        "df_train = df[:int(0.9*len(df))]\n",
        "df_valid = df[int(0.9*len(df)):int(0.95*len(df))]\n",
        "df_test = df[int(0.95*len(df)):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Lrp-_tz6P9OG",
        "outputId": "5bbc991d-8226-457f-9669-23228e27d172"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Open         High          Low        Close    Adj Close  \\\n",
              "0        0.049510     0.049510     0.049510     0.049510     0.049510   \n",
              "1        0.049510     0.085850     0.059410     0.085840     0.085840   \n",
              "2        0.085840     0.093070     0.077230     0.080800     0.080800   \n",
              "3        0.080800     0.081810     0.074260     0.074740     0.074740   \n",
              "4        0.074740     0.079210     0.066340     0.079210     0.079210   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "2875  7393.020020  7608.870117  7349.520020  7502.149902  7502.149902   \n",
              "2876  7501.740234  7614.660156  7370.270020  7530.549805  7530.549805   \n",
              "2877  7530.549805  7697.339844  7467.790039  7643.259766  7643.259766   \n",
              "2878  7643.259766  7774.959961  7606.759766  7719.750000  7719.750000   \n",
              "2879  7719.729980  7760.729980  7469.209961  7503.200195  7503.200195   \n",
              "\n",
              "         Volume  \n",
              "0             0  \n",
              "1             5  \n",
              "2            49  \n",
              "3            20  \n",
              "4            42  \n",
              "...         ...  \n",
              "2875  458223966  \n",
              "2876  458687659  \n",
              "2877  362414878  \n",
              "2878  332313005  \n",
              "2879  427448262  \n",
              "\n",
              "[2880 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb8c99f9-1d0a-42a8-bcb9-32aa73312857\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.085850</td>\n",
              "      <td>0.059410</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.085840</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085840</td>\n",
              "      <td>0.093070</td>\n",
              "      <td>0.077230</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.081810</td>\n",
              "      <td>0.074260</td>\n",
              "      <td>0.074740</td>\n",
              "      <td>0.074740</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.074740</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>0.066340</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2875</th>\n",
              "      <td>7393.020020</td>\n",
              "      <td>7608.870117</td>\n",
              "      <td>7349.520020</td>\n",
              "      <td>7502.149902</td>\n",
              "      <td>7502.149902</td>\n",
              "      <td>458223966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2876</th>\n",
              "      <td>7501.740234</td>\n",
              "      <td>7614.660156</td>\n",
              "      <td>7370.270020</td>\n",
              "      <td>7530.549805</td>\n",
              "      <td>7530.549805</td>\n",
              "      <td>458687659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2877</th>\n",
              "      <td>7530.549805</td>\n",
              "      <td>7697.339844</td>\n",
              "      <td>7467.790039</td>\n",
              "      <td>7643.259766</td>\n",
              "      <td>7643.259766</td>\n",
              "      <td>362414878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2878</th>\n",
              "      <td>7643.259766</td>\n",
              "      <td>7774.959961</td>\n",
              "      <td>7606.759766</td>\n",
              "      <td>7719.750000</td>\n",
              "      <td>7719.750000</td>\n",
              "      <td>332313005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2879</th>\n",
              "      <td>7719.729980</td>\n",
              "      <td>7760.729980</td>\n",
              "      <td>7469.209961</td>\n",
              "      <td>7503.200195</td>\n",
              "      <td>7503.200195</td>\n",
              "      <td>427448262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2880 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb8c99f9-1d0a-42a8-bcb9-32aa73312857')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb8c99f9-1d0a-42a8-bcb9-32aa73312857 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb8c99f9-1d0a-42a8-bcb9-32aa73312857');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6kHZG8kmP_zC",
        "outputId": "3b39d81c-d674-4a27-f04f-83ce8ae98e0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Open         High          Low        Close    Adj Close  \\\n",
              "2880  7503.319824  7672.700195  7394.970215  7629.399902  7629.399902   \n",
              "2881  7629.399902  7701.350098  7504.979980  7661.790039  7661.790039   \n",
              "2882  7661.790039  7755.919922  7651.129883  7700.109863  7700.109863   \n",
              "2883  7700.399902  7709.609863  7559.500000  7627.520020  7627.520020   \n",
              "2884  7627.560059  7691.740234  7495.720215  7513.689941  7513.689941   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "3035  6479.720215  6562.790039  6476.589844  6529.680176  6529.680176   \n",
              "3036  6530.939941  6542.419922  6438.270020  6446.060059  6446.060059   \n",
              "3037  6446.060059  6457.970215  6352.450195  6377.990234  6377.990234   \n",
              "3038  6377.990234  6419.919922  6376.169922  6396.370117  6396.370117   \n",
              "3039  6396.390137  6414.180176  6326.379883  6408.180176  6408.180176   \n",
              "\n",
              "         Volume  \n",
              "2880  440337235  \n",
              "2881  396116252  \n",
              "2882  342475552  \n",
              "2883  310119047  \n",
              "2884  218013881  \n",
              "...         ...  \n",
              "3035  360052788  \n",
              "3036  273205386  \n",
              "3037  237893036  \n",
              "3038   94313328  \n",
              "3039  130156001  \n",
              "\n",
              "[160 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f915b01d-9a97-4f6b-ac22-a415f75a6e6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2880</th>\n",
              "      <td>7503.319824</td>\n",
              "      <td>7672.700195</td>\n",
              "      <td>7394.970215</td>\n",
              "      <td>7629.399902</td>\n",
              "      <td>7629.399902</td>\n",
              "      <td>440337235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2881</th>\n",
              "      <td>7629.399902</td>\n",
              "      <td>7701.350098</td>\n",
              "      <td>7504.979980</td>\n",
              "      <td>7661.790039</td>\n",
              "      <td>7661.790039</td>\n",
              "      <td>396116252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2882</th>\n",
              "      <td>7661.790039</td>\n",
              "      <td>7755.919922</td>\n",
              "      <td>7651.129883</td>\n",
              "      <td>7700.109863</td>\n",
              "      <td>7700.109863</td>\n",
              "      <td>342475552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2883</th>\n",
              "      <td>7700.399902</td>\n",
              "      <td>7709.609863</td>\n",
              "      <td>7559.500000</td>\n",
              "      <td>7627.520020</td>\n",
              "      <td>7627.520020</td>\n",
              "      <td>310119047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2884</th>\n",
              "      <td>7627.560059</td>\n",
              "      <td>7691.740234</td>\n",
              "      <td>7495.720215</td>\n",
              "      <td>7513.689941</td>\n",
              "      <td>7513.689941</td>\n",
              "      <td>218013881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3035</th>\n",
              "      <td>6479.720215</td>\n",
              "      <td>6562.790039</td>\n",
              "      <td>6476.589844</td>\n",
              "      <td>6529.680176</td>\n",
              "      <td>6529.680176</td>\n",
              "      <td>360052788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3036</th>\n",
              "      <td>6530.939941</td>\n",
              "      <td>6542.419922</td>\n",
              "      <td>6438.270020</td>\n",
              "      <td>6446.060059</td>\n",
              "      <td>6446.060059</td>\n",
              "      <td>273205386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3037</th>\n",
              "      <td>6446.060059</td>\n",
              "      <td>6457.970215</td>\n",
              "      <td>6352.450195</td>\n",
              "      <td>6377.990234</td>\n",
              "      <td>6377.990234</td>\n",
              "      <td>237893036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3038</th>\n",
              "      <td>6377.990234</td>\n",
              "      <td>6419.919922</td>\n",
              "      <td>6376.169922</td>\n",
              "      <td>6396.370117</td>\n",
              "      <td>6396.370117</td>\n",
              "      <td>94313328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3039</th>\n",
              "      <td>6396.390137</td>\n",
              "      <td>6414.180176</td>\n",
              "      <td>6326.379883</td>\n",
              "      <td>6408.180176</td>\n",
              "      <td>6408.180176</td>\n",
              "      <td>130156001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f915b01d-9a97-4f6b-ac22-a415f75a6e6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f915b01d-9a97-4f6b-ac22-a415f75a6e6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f915b01d-9a97-4f6b-ac22-a415f75a6e6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sBMcR_j4QFzz",
        "outputId": "aeeb492e-cf60-4bcb-b4ca-2682b0064e9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Open         High          Low        Close    Adj Close  \\\n",
              "3040  6408.180176  6435.410156  6350.870117  6375.080078  6375.080078   \n",
              "3041  6375.080078  6389.430176  6315.580078  6339.169922  6339.169922   \n",
              "3042  6339.169922  6375.700195  5469.930176  5741.470215  5741.470215   \n",
              "3043  5740.509766  5767.060059  5300.750000  5647.500000  5647.500000   \n",
              "3044  5647.500000  5676.540039  5487.870117  5586.270020  5586.270020   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "3196  5212.810059  5270.759766  5190.810059  5236.259766  5236.259766   \n",
              "3197  5236.259766  5317.779785  5232.990234  5289.750000  5289.750000   \n",
              "3198  5289.750000  5353.009766  5212.180176  5295.529785  5295.529785   \n",
              "3199  5295.529785  5366.600098  5274.450195  5326.180176  5326.180176   \n",
              "3200  5326.180176  5348.589844  5232.770020  5305.740234  5305.740234   \n",
              "\n",
              "          Volume  \n",
              "3040   181575610  \n",
              "3041   227689745  \n",
              "3042  1045469380  \n",
              "3043   820864013  \n",
              "3044   397196182  \n",
              "...          ...  \n",
              "3196   194699629  \n",
              "3197   187312150  \n",
              "3198   158699049  \n",
              "3199   123577001  \n",
              "3200   134681237  \n",
              "\n",
              "[161 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd31b207-6ae5-44a8-ac71-25ef4a873e9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3040</th>\n",
              "      <td>6408.180176</td>\n",
              "      <td>6435.410156</td>\n",
              "      <td>6350.870117</td>\n",
              "      <td>6375.080078</td>\n",
              "      <td>6375.080078</td>\n",
              "      <td>181575610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3041</th>\n",
              "      <td>6375.080078</td>\n",
              "      <td>6389.430176</td>\n",
              "      <td>6315.580078</td>\n",
              "      <td>6339.169922</td>\n",
              "      <td>6339.169922</td>\n",
              "      <td>227689745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3042</th>\n",
              "      <td>6339.169922</td>\n",
              "      <td>6375.700195</td>\n",
              "      <td>5469.930176</td>\n",
              "      <td>5741.470215</td>\n",
              "      <td>5741.470215</td>\n",
              "      <td>1045469380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3043</th>\n",
              "      <td>5740.509766</td>\n",
              "      <td>5767.060059</td>\n",
              "      <td>5300.750000</td>\n",
              "      <td>5647.500000</td>\n",
              "      <td>5647.500000</td>\n",
              "      <td>820864013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3044</th>\n",
              "      <td>5647.500000</td>\n",
              "      <td>5676.540039</td>\n",
              "      <td>5487.870117</td>\n",
              "      <td>5586.270020</td>\n",
              "      <td>5586.270020</td>\n",
              "      <td>397196182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>5212.810059</td>\n",
              "      <td>5270.759766</td>\n",
              "      <td>5190.810059</td>\n",
              "      <td>5236.259766</td>\n",
              "      <td>5236.259766</td>\n",
              "      <td>194699629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>5236.259766</td>\n",
              "      <td>5317.779785</td>\n",
              "      <td>5232.990234</td>\n",
              "      <td>5289.750000</td>\n",
              "      <td>5289.750000</td>\n",
              "      <td>187312150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>5289.750000</td>\n",
              "      <td>5353.009766</td>\n",
              "      <td>5212.180176</td>\n",
              "      <td>5295.529785</td>\n",
              "      <td>5295.529785</td>\n",
              "      <td>158699049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3199</th>\n",
              "      <td>5295.529785</td>\n",
              "      <td>5366.600098</td>\n",
              "      <td>5274.450195</td>\n",
              "      <td>5326.180176</td>\n",
              "      <td>5326.180176</td>\n",
              "      <td>123577001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>5326.180176</td>\n",
              "      <td>5348.589844</td>\n",
              "      <td>5232.770020</td>\n",
              "      <td>5305.740234</td>\n",
              "      <td>5305.740234</td>\n",
              "      <td>134681237</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>161 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd31b207-6ae5-44a8-ac71-25ef4a873e9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd31b207-6ae5-44a8-ac71-25ef4a873e9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd31b207-6ae5-44a8-ac71-25ef4a873e9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nSA22V6bOixj"
      },
      "outputs": [],
      "source": [
        "# sq_train = df_train.to_numpy()\n",
        "# sq_test = df_test.to_numpy()\n",
        "# sq_valid = df_valid.to_numpy()\n",
        "sq = df.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2jPk2QHepCbk"
      },
      "outputs": [],
      "source": [
        "def split_sequences(seq, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(seq) - n_steps - 1):\n",
        "        seq_x, seq_y = seq[i:(i + n_steps), :], seq[i + n_steps, 3]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t5auE6C4Ocdn"
      },
      "outputs": [],
      "source": [
        "class MV_LSTM(torch.nn.Module):\n",
        "    def __init__(self,n_features,seq_length):\n",
        "        super(MV_LSTM, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.seq_len = seq_length\n",
        "        self.n_hidden = 32 # number of hidden state's features\n",
        "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
        "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
        "                                 hidden_size = self.n_hidden,\n",
        "                                 num_layers = self.n_layers, \n",
        "                                 batch_first = True)\n",
        "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
        "        \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden).cuda()\n",
        "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden).cuda()\n",
        "        self.hidden = (hidden_state, cell_state)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):  \n",
        "        #x = torch.from_numpy(x)      \n",
        "        batch_size, seq_len, _  = x.size()\n",
        "        \n",
        "        lstm_out, self.hidden = self.l_lstm(x, self.hidden)\n",
        "        # lstm_out(with batch_first = True) is \n",
        "        # (batch_size,seq_len,num_directions * hidden_size)\n",
        "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
        "        # .contiguous() -> solves tensor compatibility error\n",
        "        x = lstm_out.contiguous().view(batch_size,-1)\n",
        "        return self.l_linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU7yaBS-OfQI",
        "outputId": "3b1c7ecd-6004-40e3-e4ad-e13a4032caa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2694, 30, 6) (2694,)\n",
            "(476, 30, 6) (476,)\n"
          ]
        }
      ],
      "source": [
        "n_features = 6 # this is number of parallel inputs\n",
        "n_timesteps = 30 # this is number of timesteps\n",
        "\n",
        "# convert dataset into input/output\n",
        "X, y = split_sequences(sq, n_timesteps)\n",
        "# X_train, y_train = split_sequences(sq_train, n_timesteps)\n",
        "# X_valid, y_valid = split_sequences(sq_valid, n_timesteps)\n",
        "# X_test, y_test = split_sequences(sq_test, n_timesteps)\n",
        "\n",
        "# define the proportion of data to allocate to train, validation, and test sets\n",
        "train_prop = 0.85\n",
        "# val_prop = 0.15\n",
        "# test_prop = 0.15\n",
        "\n",
        "# shuffle the indices of X and y\n",
        "indices = np.random.permutation(len(X))\n",
        "\n",
        "# calculate the number of samples for each set based on the proportions\n",
        "train_size = int(train_prop * len(X))\n",
        "# val_size = int(val_prop * len(X))\n",
        "# test_size = len(X) - train_size - val_size\n",
        "\n",
        "# divide the shuffled indices into train, validation, and test sets\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "# val_indices = indices[train_size:train_size+val_size]\n",
        "##test_indices = indices[train_size+val_size:]\n",
        "\n",
        "# create the train, validation, and test sets from the shuffled indices\n",
        "X_train = X[train_indices]\n",
        "y_train = y[train_indices]\n",
        "X_valid = X[val_indices]\n",
        "y_valid = y[val_indices]\n",
        "# X_test = X[test_indices]\n",
        "# y_test = y[test_indices]\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_valid.shape, y_valid.shape)\n",
        "# print(X_test.shape, y_test.shape)\n",
        "\n",
        "# create NN\n",
        "mv_net = MV_LSTM(n_features,n_timesteps)\n",
        "criterion = torch.nn.MSELoss(reduction='mean') # reduction='sum' created huge loss value\n",
        "optimizer = torch.optim.Adam(mv_net.parameters(), lr=0.001)\n",
        "\n",
        "train_episodes = 1500\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD_ZBHenptZv",
        "outputId": "6e5502de-06fa-4b64-f019-e3f0b2f1cee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mv_net.to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGfSNlHroGvF",
        "outputId": "f0b4e5c0-9017-4237-f0a3-1963cf00fa5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step :  0 training loss :  11659329.095238095\n",
            "step :  0 validation loss :  9717971.666666666\n",
            "step :  1 training loss :  11657648.761904761\n",
            "step :  1 validation loss :  9719035.666666666\n",
            "step :  2 training loss :  11653668.30952381\n",
            "step :  2 validation loss :  9712515.0\n",
            "step :  3 training loss :  11647339.595238095\n",
            "step :  3 validation loss :  9715098.833333334\n",
            "step :  4 training loss :  11645212.476190476\n",
            "step :  4 validation loss :  9708040.166666666\n",
            "step :  5 training loss :  11650339.547619049\n",
            "step :  5 validation loss :  9714720.666666666\n",
            "step :  6 training loss :  11639151.333333334\n",
            "step :  6 validation loss :  9691226.166666666\n",
            "step :  7 training loss :  11623914.88095238\n",
            "step :  7 validation loss :  9684519.333333334\n",
            "step :  8 training loss :  11645538.38095238\n",
            "step :  8 validation loss :  9712780.666666666\n",
            "step :  9 training loss :  11646389.69047619\n",
            "step :  9 validation loss :  9722905.833333334\n",
            "step :  10 training loss :  11654815.214285715\n",
            "step :  10 validation loss :  9711461.166666666\n",
            "step :  11 training loss :  11638632.61904762\n",
            "step :  11 validation loss :  9695026.166666666\n",
            "step :  12 training loss :  11627534.666666666\n",
            "step :  12 validation loss :  9696541.0\n",
            "step :  13 training loss :  11634919.261904761\n",
            "step :  13 validation loss :  9705892.666666666\n",
            "step :  14 training loss :  11634929.952380951\n",
            "step :  14 validation loss :  9696179.0\n",
            "step :  15 training loss :  11619759.333333334\n",
            "step :  15 validation loss :  9678550.666666666\n",
            "step :  16 training loss :  11612969.047619049\n",
            "step :  16 validation loss :  9706045.5\n",
            "step :  17 training loss :  11632227.285714285\n",
            "step :  17 validation loss :  9716377.5\n",
            "step :  18 training loss :  11640387.547619049\n",
            "step :  18 validation loss :  9694368.5\n",
            "step :  19 training loss :  11616029.761904761\n",
            "step :  19 validation loss :  9675331.0\n",
            "step :  20 training loss :  11624790.57142857\n",
            "step :  20 validation loss :  9712646.5\n",
            "step :  21 training loss :  11638480.976190476\n",
            "step :  21 validation loss :  9695103.0\n",
            "step :  22 training loss :  11619528.38095238\n",
            "step :  22 validation loss :  9678097.333333334\n",
            "step :  23 training loss :  11622047.42857143\n",
            "step :  23 validation loss :  9693550.0\n",
            "step :  24 training loss :  11615223.595238095\n",
            "step :  24 validation loss :  9672946.666666666\n",
            "step :  25 training loss :  11593229.904761905\n",
            "step :  25 validation loss :  9652608.833333334\n",
            "step :  26 training loss :  11570545.595238095\n",
            "step :  26 validation loss :  9648638.333333334\n",
            "step :  27 training loss :  11562944.333333334\n",
            "step :  27 validation loss :  9625248.333333334\n",
            "step :  28 training loss :  11535985.714285715\n",
            "step :  28 validation loss :  9601586.666666666\n",
            "step :  29 training loss :  11561091.92857143\n",
            "step :  29 validation loss :  9650628.0\n",
            "step :  30 training loss :  11567335.357142856\n",
            "step :  30 validation loss :  9644056.666666666\n",
            "step :  31 training loss :  11564208.666666666\n",
            "step :  31 validation loss :  9631646.166666666\n",
            "step :  32 training loss :  11541541.19047619\n",
            "step :  32 validation loss :  9604804.833333334\n",
            "step :  33 training loss :  11518503.142857144\n",
            "step :  33 validation loss :  9607844.333333334\n",
            "step :  34 training loss :  11527511.07142857\n",
            "step :  34 validation loss :  9610131.833333334\n",
            "step :  35 training loss :  11519977.357142856\n",
            "step :  35 validation loss :  9588652.666666666\n",
            "step :  36 training loss :  11507435.166666666\n",
            "step :  36 validation loss :  9594224.333333334\n",
            "step :  37 training loss :  11539692.642857144\n",
            "step :  37 validation loss :  9631562.166666666\n",
            "step :  38 training loss :  11573363.404761905\n",
            "step :  38 validation loss :  9642568.666666666\n",
            "step :  39 training loss :  11562400.976190476\n",
            "step :  39 validation loss :  9631148.5\n",
            "step :  40 training loss :  11549582.785714285\n",
            "step :  40 validation loss :  9619978.166666666\n",
            "step :  41 training loss :  11541014.42857143\n",
            "step :  41 validation loss :  9610183.333333334\n",
            "step :  42 training loss :  11521590.785714285\n",
            "step :  42 validation loss :  9591626.166666666\n",
            "step :  43 training loss :  11502462.285714285\n",
            "step :  43 validation loss :  9574833.0\n",
            "step :  44 training loss :  11514126.738095239\n",
            "step :  44 validation loss :  9611677.833333334\n",
            "step :  45 training loss :  11539499.38095238\n",
            "step :  45 validation loss :  9597468.166666666\n",
            "step :  46 training loss :  11509570.11904762\n",
            "step :  46 validation loss :  9595517.666666666\n",
            "step :  47 training loss :  11508273.785714285\n",
            "step :  47 validation loss :  9582817.5\n",
            "step :  48 training loss :  11512217.57142857\n",
            "step :  48 validation loss :  9611235.333333334\n",
            "step :  49 training loss :  11527586.285714285\n",
            "step :  49 validation loss :  9601180.666666666\n",
            "step :  50 training loss :  11516298.238095239\n",
            "step :  50 validation loss :  9604806.0\n",
            "step :  51 training loss :  11521033.38095238\n",
            "step :  51 validation loss :  9636414.0\n",
            "step :  52 training loss :  11555097.19047619\n",
            "step :  52 validation loss :  9624437.833333334\n",
            "step :  53 training loss :  11541793.30952381\n",
            "step :  53 validation loss :  9613026.666666666\n",
            "step :  54 training loss :  11529153.547619049\n",
            "step :  54 validation loss :  9602169.0\n",
            "step :  55 training loss :  11517070.714285715\n",
            "step :  55 validation loss :  9591746.0\n",
            "step :  56 training loss :  11505426.761904761\n",
            "step :  56 validation loss :  9581668.666666666\n",
            "step :  57 training loss :  11494139.095238095\n",
            "step :  57 validation loss :  9571879.666666666\n",
            "step :  58 training loss :  11483148.19047619\n",
            "step :  58 validation loss :  9562332.666666666\n",
            "step :  59 training loss :  11472406.595238095\n",
            "step :  59 validation loss :  9552974.666666666\n",
            "step :  60 training loss :  11496946.07142857\n",
            "step :  60 validation loss :  9583362.166666666\n",
            "step :  61 training loss :  11506796.976190476\n",
            "step :  61 validation loss :  9572057.166666666\n",
            "step :  62 training loss :  11482500.333333334\n",
            "step :  62 validation loss :  9560866.666666666\n",
            "step :  63 training loss :  11469727.476190476\n",
            "step :  63 validation loss :  9549654.0\n",
            "step :  64 training loss :  11486453.976190476\n",
            "step :  64 validation loss :  9582842.333333334\n",
            "step :  65 training loss :  11606735.547619049\n",
            "step :  65 validation loss :  9677633.333333334\n",
            "step :  66 training loss :  11605262.404761905\n",
            "step :  66 validation loss :  9671833.333333334\n",
            "step :  67 training loss :  11598713.30952381\n",
            "step :  67 validation loss :  9666076.666666666\n",
            "step :  68 training loss :  11592212.88095238\n",
            "step :  68 validation loss :  9660363.666666666\n",
            "step :  69 training loss :  11585762.404761905\n",
            "step :  69 validation loss :  9654698.5\n",
            "step :  70 training loss :  11579359.238095239\n",
            "step :  70 validation loss :  9649072.333333334\n",
            "step :  71 training loss :  11572998.261904761\n",
            "step :  71 validation loss :  9643481.166666666\n",
            "step :  72 training loss :  11593903.92857143\n",
            "step :  72 validation loss :  9663684.5\n",
            "step :  73 training loss :  11590292.904761905\n",
            "step :  73 validation loss :  9659415.0\n",
            "step :  74 training loss :  11585468.261904761\n",
            "step :  74 validation loss :  9655174.333333334\n",
            "step :  75 training loss :  11580676.095238095\n",
            "step :  75 validation loss :  9650961.166666666\n",
            "step :  76 training loss :  11581062.666666666\n",
            "step :  76 validation loss :  9646868.0\n",
            "step :  77 training loss :  11571341.88095238\n",
            "step :  77 validation loss :  9642721.0\n",
            "step :  78 training loss :  11566669.833333334\n",
            "step :  78 validation loss :  9638600.833333334\n",
            "step :  79 training loss :  11561961.5\n",
            "step :  79 validation loss :  9634485.166666666\n",
            "step :  80 training loss :  11557278.523809524\n",
            "step :  80 validation loss :  9630398.166666666\n",
            "step :  81 training loss :  11552659.833333334\n",
            "step :  81 validation loss :  9626348.166666666\n",
            "step :  82 training loss :  11548060.19047619\n",
            "step :  82 validation loss :  9622289.166666666\n",
            "step :  83 training loss :  11543479.333333334\n",
            "step :  83 validation loss :  9618275.0\n",
            "step :  84 training loss :  11538924.88095238\n",
            "step :  84 validation loss :  9614265.5\n",
            "step :  85 training loss :  11534391.88095238\n",
            "step :  85 validation loss :  9610291.0\n",
            "step :  86 training loss :  11529944.42857143\n",
            "step :  86 validation loss :  9606328.333333334\n",
            "step :  87 training loss :  11525393.30952381\n",
            "step :  87 validation loss :  9602381.0\n",
            "step :  88 training loss :  11520918.19047619\n",
            "step :  88 validation loss :  9598463.666666666\n",
            "step :  89 training loss :  11516482.61904762\n",
            "step :  89 validation loss :  9594553.166666666\n",
            "step :  90 training loss :  11512043.904761905\n",
            "step :  90 validation loss :  9590658.333333334\n",
            "step :  91 training loss :  11507627.547619049\n",
            "step :  91 validation loss :  9586778.166666666\n",
            "step :  92 training loss :  11503227.833333334\n",
            "step :  92 validation loss :  9582914.333333334\n",
            "step :  93 training loss :  11498843.88095238\n",
            "step :  93 validation loss :  9579064.833333334\n",
            "step :  94 training loss :  11494476.0\n",
            "step :  94 validation loss :  9575228.666666666\n",
            "step :  95 training loss :  11490123.166666666\n",
            "step :  95 validation loss :  9571407.0\n",
            "step :  96 training loss :  11485784.19047619\n",
            "step :  96 validation loss :  9567596.333333334\n",
            "step :  97 training loss :  11504553.047619049\n",
            "step :  97 validation loss :  9603780.0\n",
            "step :  98 training loss :  11523627.69047619\n",
            "step :  98 validation loss :  9601894.5\n",
            "step :  99 training loss :  11521486.19047619\n",
            "step :  99 validation loss :  9600008.833333334\n",
            "step :  100 training loss :  11519346.738095239\n",
            "step :  100 validation loss :  9598127.0\n",
            "step :  101 training loss :  11517210.452380951\n",
            "step :  101 validation loss :  9596247.333333334\n",
            "step :  102 training loss :  11515077.595238095\n",
            "step :  102 validation loss :  9594371.5\n",
            "step :  103 training loss :  11512947.404761905\n",
            "step :  103 validation loss :  9592496.666666666\n",
            "step :  104 training loss :  11510820.5\n",
            "step :  104 validation loss :  9590625.0\n",
            "step :  105 training loss :  11508696.333333334\n",
            "step :  105 validation loss :  9588757.0\n",
            "step :  106 training loss :  11506575.095238095\n",
            "step :  106 validation loss :  9586890.833333334\n",
            "step :  107 training loss :  11504456.30952381\n",
            "step :  107 validation loss :  9585026.0\n",
            "step :  108 training loss :  11504351.547619049\n",
            "step :  108 validation loss :  9589416.333333334\n",
            "step :  109 training loss :  11502486.976190476\n",
            "step :  109 validation loss :  9578577.0\n",
            "step :  110 training loss :  11490453.904761905\n",
            "step :  110 validation loss :  9568698.333333334\n",
            "step :  111 training loss :  11480004.0\n",
            "step :  111 validation loss :  9560150.5\n",
            "step :  112 training loss :  11481785.07142857\n",
            "step :  112 validation loss :  9576777.833333334\n",
            "step :  113 training loss :  11489140.714285715\n",
            "step :  113 validation loss :  9567932.5\n",
            "step :  114 training loss :  11479141.261904761\n",
            "step :  114 validation loss :  9603195.0\n",
            "step :  115 training loss :  11518821.11904762\n",
            "step :  115 validation loss :  9592682.833333334\n",
            "step :  116 training loss :  11505567.833333334\n",
            "step :  116 validation loss :  9581064.5\n",
            "step :  117 training loss :  11493021.92857143\n",
            "step :  117 validation loss :  9570618.166666666\n",
            "step :  118 training loss :  11481658.452380951\n",
            "step :  118 validation loss :  9561049.5\n",
            "step :  119 training loss :  11471141.404761905\n",
            "step :  119 validation loss :  9552107.0\n",
            "step :  120 training loss :  11466057.547619049\n",
            "step :  120 validation loss :  9550045.333333334\n",
            "step :  121 training loss :  11460008.904761905\n",
            "step :  121 validation loss :  9543572.666666666\n",
            "step :  122 training loss :  11499454.5\n",
            "step :  122 validation loss :  9585838.5\n",
            "step :  123 training loss :  11502024.047619049\n",
            "step :  123 validation loss :  9579858.166666666\n",
            "step :  124 training loss :  11494114.261904761\n",
            "step :  124 validation loss :  9573713.5\n",
            "step :  125 training loss :  11487164.857142856\n",
            "step :  125 validation loss :  9567640.5\n",
            "step :  126 training loss :  11480311.38095238\n",
            "step :  126 validation loss :  9561658.333333334\n",
            "step :  127 training loss :  11473556.761904761\n",
            "step :  127 validation loss :  9555761.166666666\n",
            "step :  128 training loss :  11466894.214285715\n",
            "step :  128 validation loss :  9549941.5\n",
            "step :  129 training loss :  11460317.285714285\n",
            "step :  129 validation loss :  9544196.0\n",
            "step :  130 training loss :  11469946.476190476\n",
            "step :  130 validation loss :  9553805.5\n",
            "step :  131 training loss :  11464883.642857144\n",
            "step :  131 validation loss :  9548311.0\n",
            "step :  132 training loss :  11458564.976190476\n",
            "step :  132 validation loss :  9542738.166666666\n",
            "step :  133 training loss :  11452246.30952381\n",
            "step :  133 validation loss :  9537207.166666666\n",
            "step :  134 training loss :  11445982.69047619\n",
            "step :  134 validation loss :  9531729.166666666\n",
            "step :  135 training loss :  11439777.69047619\n",
            "step :  135 validation loss :  9526303.333333334\n",
            "step :  136 training loss :  11433628.5\n",
            "step :  136 validation loss :  9520925.0\n",
            "step :  137 training loss :  11427532.404761905\n",
            "step :  137 validation loss :  9515594.666666666\n",
            "step :  138 training loss :  11421486.142857144\n",
            "step :  138 validation loss :  9510306.0\n",
            "step :  139 training loss :  11415486.952380951\n",
            "step :  139 validation loss :  9505059.166666666\n",
            "step :  140 training loss :  11409532.238095239\n",
            "step :  140 validation loss :  9499849.5\n",
            "step :  141 training loss :  11403614.547619049\n",
            "step :  141 validation loss :  9494663.0\n",
            "step :  142 training loss :  11400870.738095239\n",
            "step :  142 validation loss :  9508625.5\n",
            "step :  143 training loss :  11414724.5\n",
            "step :  143 validation loss :  9505244.666666666\n",
            "step :  144 training loss :  11410779.595238095\n",
            "step :  144 validation loss :  9501886.333333334\n",
            "step :  145 training loss :  11407013.238095239\n",
            "step :  145 validation loss :  9498634.666666666\n",
            "step :  146 training loss :  11403371.261904761\n",
            "step :  146 validation loss :  9495301.666666666\n",
            "step :  147 training loss :  11425469.476190476\n",
            "step :  147 validation loss :  9531763.5\n",
            "step :  148 training loss :  11439274.261904761\n",
            "step :  148 validation loss :  9522802.0\n",
            "step :  149 training loss :  11426231.42857143\n",
            "step :  149 validation loss :  9511414.833333334\n",
            "step :  150 training loss :  11414194.5\n",
            "step :  150 validation loss :  9501772.333333334\n",
            "step :  151 training loss :  11418413.452380951\n",
            "step :  151 validation loss :  9518901.833333334\n",
            "step :  152 training loss :  11426330.452380951\n",
            "step :  152 validation loss :  9515601.0\n",
            "step :  153 training loss :  11422575.833333334\n",
            "step :  153 validation loss :  9512308.666666666\n",
            "step :  154 training loss :  11418831.142857144\n",
            "step :  154 validation loss :  9509024.5\n",
            "step :  155 training loss :  11415096.547619049\n",
            "step :  155 validation loss :  9505751.0\n",
            "step :  156 training loss :  11411371.57142857\n",
            "step :  156 validation loss :  9502486.5\n",
            "step :  157 training loss :  11407656.738095239\n",
            "step :  157 validation loss :  9499229.0\n",
            "step :  158 training loss :  11403951.07142857\n",
            "step :  158 validation loss :  9495980.333333334\n",
            "step :  159 training loss :  11400254.61904762\n",
            "step :  159 validation loss :  9492739.333333334\n",
            "step :  160 training loss :  11396567.0\n",
            "step :  160 validation loss :  9489507.333333334\n",
            "step :  161 training loss :  11392888.19047619\n",
            "step :  161 validation loss :  9486281.666666666\n",
            "step :  162 training loss :  11389217.80952381\n",
            "step :  162 validation loss :  9483063.166666666\n",
            "step :  163 training loss :  11385556.0\n",
            "step :  163 validation loss :  9479854.5\n",
            "step :  164 training loss :  11381901.714285715\n",
            "step :  164 validation loss :  9476650.5\n",
            "step :  165 training loss :  11378255.761904761\n",
            "step :  165 validation loss :  9473454.333333334\n",
            "step :  166 training loss :  11374616.976190476\n",
            "step :  166 validation loss :  9470264.0\n",
            "step :  167 training loss :  11370985.92857143\n",
            "step :  167 validation loss :  9467081.5\n",
            "step :  168 training loss :  11367361.642857144\n",
            "step :  168 validation loss :  9463907.5\n",
            "step :  169 training loss :  11363745.357142856\n",
            "step :  169 validation loss :  9460736.166666666\n",
            "step :  170 training loss :  11360135.0\n",
            "step :  170 validation loss :  9457569.166666666\n",
            "step :  171 training loss :  11356529.833333334\n",
            "step :  171 validation loss :  9454407.833333334\n",
            "step :  172 training loss :  11352932.0\n",
            "step :  172 validation loss :  9451253.833333334\n",
            "step :  173 training loss :  11349340.30952381\n",
            "step :  173 validation loss :  9448105.333333334\n",
            "step :  174 training loss :  11345756.595238095\n",
            "step :  174 validation loss :  9444961.5\n",
            "step :  175 training loss :  11342178.142857144\n",
            "step :  175 validation loss :  9441826.666666666\n",
            "step :  176 training loss :  11338607.023809524\n",
            "step :  176 validation loss :  9438697.333333334\n",
            "step :  177 training loss :  11335042.38095238\n",
            "step :  177 validation loss :  9435574.666666666\n",
            "step :  178 training loss :  11331485.57142857\n",
            "step :  178 validation loss :  9432462.833333334\n",
            "step :  179 training loss :  11327933.952380951\n",
            "step :  179 validation loss :  9429348.5\n",
            "step :  180 training loss :  11354415.333333334\n",
            "step :  180 validation loss :  9453991.833333334\n",
            "step :  181 training loss :  11348395.214285715\n",
            "step :  181 validation loss :  9443548.333333334\n",
            "step :  182 training loss :  11336943.88095238\n",
            "step :  182 validation loss :  9433988.666666666\n",
            "step :  183 training loss :  11326484.714285715\n",
            "step :  183 validation loss :  9425203.666666666\n",
            "step :  184 training loss :  11316785.357142856\n",
            "step :  184 validation loss :  9416986.5\n",
            "step :  185 training loss :  11307646.38095238\n",
            "step :  185 validation loss :  9409193.833333334\n",
            "step :  186 training loss :  11298938.80952381\n",
            "step :  186 validation loss :  9401736.333333334\n",
            "step :  187 training loss :  11290574.452380951\n",
            "step :  187 validation loss :  9394543.666666666\n",
            "step :  188 training loss :  11303671.904761905\n",
            "step :  188 validation loss :  9425948.5\n",
            "step :  189 training loss :  11319162.714285715\n",
            "step :  189 validation loss :  9420424.166666666\n",
            "step :  190 training loss :  11312942.523809524\n",
            "step :  190 validation loss :  9415055.0\n",
            "step :  191 training loss :  11306889.38095238\n",
            "step :  191 validation loss :  9409831.333333334\n",
            "step :  192 training loss :  11301001.547619049\n",
            "step :  192 validation loss :  9404743.0\n",
            "step :  193 training loss :  11295253.80952381\n",
            "step :  193 validation loss :  9399776.5\n",
            "step :  194 training loss :  11289646.166666666\n",
            "step :  194 validation loss :  9394918.166666666\n",
            "step :  195 training loss :  11284129.61904762\n",
            "step :  195 validation loss :  9390128.333333334\n",
            "step :  196 training loss :  11278708.38095238\n",
            "step :  196 validation loss :  9385430.666666666\n",
            "step :  197 training loss :  11273376.214285715\n",
            "step :  197 validation loss :  9380800.833333334\n",
            "step :  198 training loss :  11268126.833333334\n",
            "step :  198 validation loss :  9376242.166666666\n",
            "step :  199 training loss :  11272239.261904761\n",
            "step :  199 validation loss :  9383279.166666666\n",
            "step :  200 training loss :  11271605.11904762\n",
            "step :  200 validation loss :  9379881.166666666\n",
            "step :  201 training loss :  11271557.595238095\n",
            "step :  201 validation loss :  9392020.833333334\n",
            "step :  202 training loss :  11276363.857142856\n",
            "step :  202 validation loss :  9379028.333333334\n",
            "step :  203 training loss :  11261988.761904761\n",
            "step :  203 validation loss :  9367345.333333334\n",
            "step :  204 training loss :  11249531.261904761\n",
            "step :  204 validation loss :  9357186.166666666\n",
            "step :  205 training loss :  11238485.023809524\n",
            "step :  205 validation loss :  9415280.333333334\n",
            "step :  206 training loss :  11305472.30952381\n",
            "step :  206 validation loss :  9407100.833333334\n",
            "step :  207 training loss :  11296460.714285715\n",
            "step :  207 validation loss :  9399479.166666666\n",
            "step :  208 training loss :  11287985.214285715\n",
            "step :  208 validation loss :  9392255.666666666\n",
            "step :  209 training loss :  11279909.5\n",
            "step :  209 validation loss :  9385344.5\n",
            "step :  210 training loss :  11272154.761904761\n",
            "step :  210 validation loss :  9378690.5\n",
            "step :  211 training loss :  11264666.0\n",
            "step :  211 validation loss :  9372249.166666666\n",
            "step :  212 training loss :  11257404.214285715\n",
            "step :  212 validation loss :  9365992.333333334\n",
            "step :  213 training loss :  11250337.19047619\n",
            "step :  213 validation loss :  9359896.333333334\n",
            "step :  214 training loss :  11243440.92857143\n",
            "step :  214 validation loss :  9353940.166666666\n",
            "step :  215 training loss :  11236695.69047619\n",
            "step :  215 validation loss :  9348110.833333334\n",
            "step :  216 training loss :  11230086.666666666\n",
            "step :  216 validation loss :  9425000.5\n",
            "step :  217 training loss :  11312696.30952381\n",
            "step :  217 validation loss :  9409094.0\n",
            "step :  218 training loss :  11294560.92857143\n",
            "step :  218 validation loss :  9394232.0\n",
            "step :  219 training loss :  11278727.833333334\n",
            "step :  219 validation loss :  9381304.833333334\n",
            "step :  220 training loss :  11264671.285714285\n",
            "step :  220 validation loss :  9369595.333333334\n",
            "step :  221 training loss :  11251759.07142857\n",
            "step :  221 validation loss :  9358706.5\n",
            "step :  222 training loss :  11239649.142857144\n",
            "step :  222 validation loss :  9348418.833333334\n",
            "step :  223 training loss :  11228143.952380951\n",
            "step :  223 validation loss :  9338598.166666666\n",
            "step :  224 training loss :  11217117.19047619\n",
            "step :  224 validation loss :  9329157.833333334\n",
            "step :  225 training loss :  11206486.92857143\n",
            "step :  225 validation loss :  9320034.5\n",
            "step :  226 training loss :  11196189.357142856\n",
            "step :  226 validation loss :  9311185.0\n",
            "step :  227 training loss :  11186178.57142857\n",
            "step :  227 validation loss :  9302570.0\n",
            "step :  228 training loss :  11176419.357142856\n",
            "step :  228 validation loss :  9294156.166666666\n",
            "step :  229 training loss :  11166882.69047619\n",
            "step :  229 validation loss :  9285938.0\n",
            "step :  230 training loss :  11157545.69047619\n",
            "step :  230 validation loss :  9277876.333333334\n",
            "step :  231 training loss :  11148386.642857144\n",
            "step :  231 validation loss :  9269972.666666666\n",
            "step :  232 training loss :  11189187.833333334\n",
            "step :  232 validation loss :  9436056.666666666\n",
            "step :  233 training loss :  11329852.333333334\n",
            "step :  233 validation loss :  9428540.833333334\n",
            "step :  234 training loss :  11313372.5\n",
            "step :  234 validation loss :  9406034.0\n",
            "step :  235 training loss :  11291643.047619049\n",
            "step :  235 validation loss :  9391870.166666666\n",
            "step :  236 training loss :  11275839.285714285\n",
            "step :  236 validation loss :  9378527.666666666\n",
            "step :  237 training loss :  11261123.261904761\n",
            "step :  237 validation loss :  9366094.333333334\n",
            "step :  238 training loss :  11247294.11904762\n",
            "step :  238 validation loss :  9354321.833333334\n",
            "step :  239 training loss :  11234119.452380951\n",
            "step :  239 validation loss :  9343053.0\n",
            "step :  240 training loss :  11221456.5\n",
            "step :  240 validation loss :  9332189.5\n",
            "step :  241 training loss :  11209208.761904761\n",
            "step :  241 validation loss :  9321659.666666666\n",
            "step :  242 training loss :  11233489.238095239\n",
            "step :  242 validation loss :  9359470.333333334\n",
            "step :  243 training loss :  11242282.476190476\n",
            "step :  243 validation loss :  9352214.5\n",
            "step :  244 training loss :  11234016.166666666\n",
            "step :  244 validation loss :  9345030.5\n",
            "step :  245 training loss :  11225830.666666666\n",
            "step :  245 validation loss :  9337917.5\n",
            "step :  246 training loss :  11217721.833333334\n",
            "step :  246 validation loss :  9330872.0\n",
            "step :  247 training loss :  11209687.785714285\n",
            "step :  247 validation loss :  9323893.166666666\n",
            "step :  248 training loss :  11201724.238095239\n",
            "step :  248 validation loss :  9316974.166666666\n",
            "step :  249 training loss :  11193827.11904762\n",
            "step :  249 validation loss :  9310115.166666666\n",
            "step :  250 training loss :  11185993.738095239\n",
            "step :  250 validation loss :  9303310.166666666\n",
            "step :  251 training loss :  11178219.61904762\n",
            "step :  251 validation loss :  9296554.0\n",
            "step :  252 training loss :  11170495.523809524\n",
            "step :  252 validation loss :  9289837.666666666\n",
            "step :  253 training loss :  11162825.19047619\n",
            "step :  253 validation loss :  9283181.0\n",
            "step :  254 training loss :  11155220.92857143\n",
            "step :  254 validation loss :  9276579.166666666\n",
            "step :  255 training loss :  11147672.80952381\n",
            "step :  255 validation loss :  9270026.333333334\n",
            "step :  256 training loss :  11184017.833333334\n",
            "step :  256 validation loss :  9416660.666666666\n",
            "step :  257 training loss :  11147664.92857143\n",
            "step :  257 validation loss :  9257574.0\n",
            "step :  258 training loss :  11125942.785714285\n",
            "step :  258 validation loss :  9251177.666666666\n",
            "step :  259 training loss :  11118576.166666666\n",
            "step :  259 validation loss :  9244783.333333334\n",
            "step :  260 training loss :  11111250.642857144\n",
            "step :  260 validation loss :  9238420.166666666\n",
            "step :  261 training loss :  11103955.857142856\n",
            "step :  261 validation loss :  9232015.833333334\n",
            "step :  262 training loss :  11096667.547619049\n",
            "step :  262 validation loss :  9225682.833333334\n",
            "step :  263 training loss :  11183519.166666666\n",
            "step :  263 validation loss :  9220051.666666666\n",
            "step :  264 training loss :  11083130.19047619\n",
            "step :  264 validation loss :  9214013.5\n",
            "step :  265 training loss :  11076059.714285715\n",
            "step :  265 validation loss :  9207821.333333334\n",
            "step :  266 training loss :  11068950.0\n",
            "step :  266 validation loss :  9201657.166666666\n",
            "step :  267 training loss :  11061892.785714285\n",
            "step :  267 validation loss :  9195591.333333334\n",
            "step :  268 training loss :  11054236.761904761\n",
            "step :  268 validation loss :  9184907.5\n",
            "step :  269 training loss :  11037327.404761905\n",
            "step :  269 validation loss :  9169801.833333334\n",
            "step :  270 training loss :  11030748.857142856\n",
            "step :  270 validation loss :  9182598.833333334\n",
            "step :  271 training loss :  11030912.833333334\n",
            "step :  271 validation loss :  9160735.666666666\n",
            "step :  272 training loss :  11010529.976190476\n",
            "step :  272 validation loss :  9150992.333333334\n",
            "step :  273 training loss :  11011415.166666666\n",
            "step :  273 validation loss :  9221900.666666666\n",
            "step :  274 training loss :  11079858.833333334\n",
            "step :  274 validation loss :  9206497.166666666\n",
            "step :  275 training loss :  11062677.095238095\n",
            "step :  275 validation loss :  9192048.166666666\n",
            "step :  276 training loss :  11046529.785714285\n",
            "step :  276 validation loss :  9178439.666666666\n",
            "step :  277 training loss :  11031226.69047619\n",
            "step :  277 validation loss :  9165477.333333334\n",
            "step :  278 training loss :  11016593.88095238\n",
            "step :  278 validation loss :  9153049.5\n",
            "step :  279 training loss :  11002516.92857143\n",
            "step :  279 validation loss :  9141066.666666666\n",
            "step :  280 training loss :  10988907.57142857\n",
            "step :  280 validation loss :  9129466.666666666\n",
            "step :  281 training loss :  10975712.92857143\n",
            "step :  281 validation loss :  9118194.666666666\n",
            "step :  282 training loss :  10962879.38095238\n",
            "step :  282 validation loss :  9107225.166666666\n",
            "step :  283 training loss :  10950360.42857143\n",
            "step :  283 validation loss :  9096530.0\n",
            "step :  284 training loss :  10942914.047619049\n",
            "step :  284 validation loss :  9127862.333333334\n",
            "step :  285 training loss :  11018538.92857143\n",
            "step :  285 validation loss :  9163855.333333334\n",
            "step :  286 training loss :  11015374.714285715\n",
            "step :  286 validation loss :  9152745.833333334\n",
            "step :  287 training loss :  11002649.30952381\n",
            "step :  287 validation loss :  9141852.333333334\n",
            "step :  288 training loss :  10990271.19047619\n",
            "step :  288 validation loss :  9131305.666666666\n",
            "step :  289 training loss :  10978258.476190476\n",
            "step :  289 validation loss :  9121061.166666666\n",
            "step :  290 training loss :  10966546.023809524\n",
            "step :  290 validation loss :  9111067.166666666\n",
            "step :  291 training loss :  10955100.452380951\n",
            "step :  291 validation loss :  9101293.333333334\n",
            "step :  292 training loss :  10932980.333333334\n",
            "step :  292 validation loss :  9068288.666666666\n",
            "step :  293 training loss :  10969808.0\n",
            "step :  293 validation loss :  9344797.333333334\n",
            "step :  294 training loss :  11189679.547619049\n",
            "step :  294 validation loss :  9287950.666666666\n",
            "step :  295 training loss :  11146655.07142857\n",
            "step :  295 validation loss :  9257072.166666666\n",
            "step :  296 training loss :  11163713.214285715\n",
            "step :  296 validation loss :  9282317.333333334\n",
            "step :  297 training loss :  11144427.166666666\n",
            "step :  297 validation loss :  9258399.333333334\n",
            "step :  298 training loss :  11117751.333333334\n",
            "step :  298 validation loss :  9235989.0\n",
            "step :  299 training loss :  11092816.857142856\n",
            "step :  299 validation loss :  9215016.333333334\n",
            "step :  300 training loss :  11087238.238095239\n",
            "step :  300 validation loss :  9212072.5\n",
            "step :  301 training loss :  11067224.357142856\n",
            "step :  301 validation loss :  9194378.666666666\n",
            "step :  302 training loss :  11047311.833333334\n",
            "step :  302 validation loss :  9177391.833333334\n",
            "step :  303 training loss :  11028155.595238095\n",
            "step :  303 validation loss :  9174339.666666666\n",
            "step :  304 training loss :  11020166.57142857\n",
            "step :  304 validation loss :  9149258.0\n",
            "step :  305 training loss :  10991401.476190476\n",
            "step :  305 validation loss :  9125442.833333334\n",
            "step :  306 training loss :  10974878.30952381\n",
            "step :  306 validation loss :  9143820.0\n",
            "step :  307 training loss :  10981912.404761905\n",
            "step :  307 validation loss :  9114008.333333334\n",
            "step :  308 training loss :  10948716.333333334\n",
            "step :  308 validation loss :  9086736.166666666\n",
            "step :  309 training loss :  10960288.857142856\n",
            "step :  309 validation loss :  9206119.0\n",
            "step :  310 training loss :  11057142.761904761\n",
            "step :  310 validation loss :  9182145.666666666\n",
            "step :  311 training loss :  11029657.07142857\n",
            "step :  311 validation loss :  9158692.0\n",
            "step :  312 training loss :  11003204.333333334\n",
            "step :  312 validation loss :  9136295.0\n",
            "step :  313 training loss :  10977894.666666666\n",
            "step :  313 validation loss :  9114838.5\n",
            "step :  314 training loss :  10953582.5\n",
            "step :  314 validation loss :  9094180.666666666\n",
            "step :  315 training loss :  10930119.07142857\n",
            "step :  315 validation loss :  9074226.0\n",
            "step :  316 training loss :  10915012.30952381\n",
            "step :  316 validation loss :  9069262.0\n",
            "step :  317 training loss :  11023710.595238095\n",
            "step :  317 validation loss :  9343468.166666666\n",
            "step :  318 training loss :  11305951.07142857\n",
            "step :  318 validation loss :  9413165.5\n",
            "step :  319 training loss :  11340661.714285715\n",
            "step :  319 validation loss :  9436345.666666666\n",
            "step :  320 training loss :  11320798.166666666\n",
            "step :  320 validation loss :  9412530.333333334\n",
            "step :  321 training loss :  11294317.595238095\n",
            "step :  321 validation loss :  9390030.0\n",
            "step :  322 training loss :  11310333.857142856\n",
            "step :  322 validation loss :  9422712.0\n",
            "step :  323 training loss :  11307918.88095238\n",
            "step :  323 validation loss :  9403683.166666666\n",
            "step :  324 training loss :  11384339.57142857\n",
            "step :  324 validation loss :  9533774.5\n",
            "step :  325 training loss :  11436161.80952381\n",
            "step :  325 validation loss :  9517583.0\n",
            "step :  326 training loss :  11418004.238095239\n",
            "step :  326 validation loss :  9501902.666666666\n",
            "step :  327 training loss :  11400415.523809524\n",
            "step :  327 validation loss :  9486710.166666666\n",
            "step :  328 training loss :  11383345.61904762\n",
            "step :  328 validation loss :  9471951.0\n",
            "step :  329 training loss :  11366734.785714285\n",
            "step :  329 validation loss :  9457577.0\n",
            "step :  330 training loss :  11350534.57142857\n",
            "step :  330 validation loss :  9443550.333333334\n",
            "step :  331 training loss :  11334705.261904761\n",
            "step :  331 validation loss :  9429837.5\n",
            "step :  332 training loss :  11319213.357142856\n",
            "step :  332 validation loss :  9416413.5\n",
            "step :  333 training loss :  11330484.357142856\n",
            "step :  333 validation loss :  9472042.0\n",
            "step :  334 training loss :  11367814.476190476\n",
            "step :  334 validation loss :  9459417.666666666\n",
            "step :  335 training loss :  11353564.404761905\n",
            "step :  335 validation loss :  9447046.666666666\n",
            "step :  336 training loss :  11339571.11904762\n",
            "step :  336 validation loss :  9434893.166666666\n",
            "step :  337 training loss :  11325809.904761905\n",
            "step :  337 validation loss :  9422938.666666666\n",
            "step :  338 training loss :  11312264.595238095\n",
            "step :  338 validation loss :  9411172.333333334\n",
            "step :  339 training loss :  11298916.5\n",
            "step :  339 validation loss :  9399579.333333334\n",
            "step :  340 training loss :  11285748.61904762\n",
            "step :  340 validation loss :  9388145.833333334\n",
            "step :  341 training loss :  11298518.357142856\n",
            "step :  341 validation loss :  9376877.833333334\n",
            "step :  342 training loss :  11259960.57142857\n",
            "step :  342 validation loss :  9365781.0\n",
            "step :  343 training loss :  11247326.857142856\n",
            "step :  343 validation loss :  9354799.333333334\n",
            "step :  344 training loss :  11373203.0\n",
            "step :  344 validation loss :  9494002.333333334\n",
            "step :  345 training loss :  11393621.095238095\n",
            "step :  345 validation loss :  9482695.833333334\n",
            "step :  346 training loss :  11380665.11904762\n",
            "step :  346 validation loss :  9471308.666666666\n",
            "step :  347 training loss :  11367751.80952381\n",
            "step :  347 validation loss :  9460024.333333334\n",
            "step :  348 training loss :  11354987.142857144\n",
            "step :  348 validation loss :  9448891.333333334\n",
            "step :  349 training loss :  11342369.285714285\n",
            "step :  349 validation loss :  9437892.333333334\n",
            "step :  350 training loss :  11329890.952380951\n",
            "step :  350 validation loss :  9427015.666666666\n",
            "step :  351 training loss :  11317541.595238095\n",
            "step :  351 validation loss :  9416255.0\n",
            "step :  352 training loss :  11305315.92857143\n",
            "step :  352 validation loss :  9405605.333333334\n",
            "step :  353 training loss :  11293205.976190476\n",
            "step :  353 validation loss :  9395059.166666666\n",
            "step :  354 training loss :  11281193.047619049\n",
            "step :  354 validation loss :  9384582.833333334\n",
            "step :  355 training loss :  11269287.857142856\n",
            "step :  355 validation loss :  9374231.833333334\n",
            "step :  356 training loss :  11257503.761904761\n",
            "step :  356 validation loss :  9363976.166666666\n",
            "step :  357 training loss :  11245821.0\n",
            "step :  357 validation loss :  9353807.166666666\n",
            "step :  358 training loss :  11234231.714285715\n",
            "step :  358 validation loss :  9343724.0\n",
            "step :  359 training loss :  11222732.19047619\n",
            "step :  359 validation loss :  9333736.833333334\n",
            "step :  360 training loss :  11376765.904761905\n",
            "step :  360 validation loss :  9617844.0\n",
            "step :  361 training loss :  11528532.11904762\n",
            "step :  361 validation loss :  9596110.166666666\n",
            "step :  362 training loss :  11504906.5\n",
            "step :  362 validation loss :  9576417.833333334\n",
            "step :  363 training loss :  11513343.904761905\n",
            "step :  363 validation loss :  10133044.666666666\n",
            "step :  364 training loss :  11770847.261904761\n",
            "step :  364 validation loss :  9542095.666666666\n",
            "step :  365 training loss :  11445776.095238095\n",
            "step :  365 validation loss :  9526193.5\n",
            "step :  366 training loss :  11427963.38095238\n",
            "step :  366 validation loss :  9510805.5\n",
            "step :  367 training loss :  11431430.714285715\n",
            "step :  367 validation loss :  9583079.166666666\n",
            "step :  368 training loss :  11503157.976190476\n",
            "step :  368 validation loss :  9570472.666666666\n",
            "step :  369 training loss :  11468399.30952381\n",
            "step :  369 validation loss :  9537788.833333334\n",
            "step :  370 training loss :  11433519.666666666\n",
            "step :  370 validation loss :  9508991.833333334\n",
            "step :  371 training loss :  11402196.738095239\n",
            "step :  371 validation loss :  9482694.833333334\n",
            "step :  372 training loss :  11419266.642857144\n",
            "step :  372 validation loss :  9625968.5\n",
            "step :  373 training loss :  11528405.19047619\n",
            "step :  373 validation loss :  9587106.666666666\n",
            "step :  374 training loss :  11485551.19047619\n",
            "step :  374 validation loss :  9550937.166666666\n",
            "step :  375 training loss :  11475643.238095239\n",
            "step :  375 validation loss :  9621851.333333334\n",
            "step :  376 training loss :  11528262.88095238\n",
            "step :  376 validation loss :  9591318.666666666\n",
            "step :  377 training loss :  11494458.92857143\n",
            "step :  377 validation loss :  9562361.333333334\n",
            "step :  378 training loss :  11462315.904761905\n",
            "step :  378 validation loss :  9534777.333333334\n",
            "step :  379 training loss :  11431584.80952381\n",
            "step :  379 validation loss :  9508352.666666666\n",
            "step :  380 training loss :  11406628.166666666\n",
            "step :  380 validation loss :  9490906.0\n",
            "step :  381 training loss :  11404584.595238095\n",
            "step :  381 validation loss :  9496192.333333334\n",
            "step :  382 training loss :  11391318.261904761\n",
            "step :  382 validation loss :  9475932.333333334\n",
            "step :  383 training loss :  11368251.023809524\n",
            "step :  383 validation loss :  9456120.0\n",
            "step :  384 training loss :  11347699.07142857\n",
            "step :  384 validation loss :  9446930.0\n",
            "step :  385 training loss :  11400697.0\n",
            "step :  385 validation loss :  9548956.666666666\n",
            "step :  386 training loss :  11447323.452380951\n",
            "step :  386 validation loss :  9521853.666666666\n",
            "step :  387 training loss :  11417355.476190476\n",
            "step :  387 validation loss :  9496401.333333334\n",
            "step :  388 training loss :  11461481.952380951\n",
            "step :  388 validation loss :  9472495.333333334\n",
            "step :  389 training loss :  11362651.30952381\n",
            "step :  389 validation loss :  9449685.5\n",
            "step :  390 training loss :  11382298.952380951\n",
            "step :  390 validation loss :  9478922.0\n",
            "step :  391 training loss :  11371288.023809524\n",
            "step :  391 validation loss :  9458455.833333334\n",
            "step :  392 training loss :  11459425.404761905\n",
            "step :  392 validation loss :  9758663.333333334\n",
            "step :  393 training loss :  11688726.238095239\n",
            "step :  393 validation loss :  9737714.333333334\n",
            "step :  394 training loss :  11665089.047619049\n",
            "step :  394 validation loss :  9716998.0\n",
            "step :  395 training loss :  11641934.80952381\n",
            "step :  395 validation loss :  9696805.0\n",
            "step :  396 training loss :  11619329.92857143\n",
            "step :  396 validation loss :  9677096.833333334\n",
            "step :  397 training loss :  11597245.595238095\n",
            "step :  397 validation loss :  9657836.0\n",
            "step :  398 training loss :  11575634.047619049\n",
            "step :  398 validation loss :  9638984.666666666\n",
            "step :  399 training loss :  11554458.261904761\n",
            "step :  399 validation loss :  9620505.0\n",
            "step :  400 training loss :  11552955.07142857\n",
            "step :  400 validation loss :  9617117.666666666\n",
            "step :  401 training loss :  11524272.261904761\n",
            "step :  401 validation loss :  9589300.333333334\n",
            "step :  402 training loss :  11494098.0\n",
            "step :  402 validation loss :  9563919.5\n",
            "step :  403 training loss :  11501810.57142857\n",
            "step :  403 validation loss :  9572475.833333334\n",
            "step :  404 training loss :  11485657.047619049\n",
            "step :  404 validation loss :  9545508.833333334\n",
            "step :  405 training loss :  11445489.80952381\n",
            "step :  405 validation loss :  9547315.666666666\n",
            "step :  406 training loss :  11444132.023809524\n",
            "step :  406 validation loss :  9539790.166666666\n",
            "step :  407 training loss :  11458946.57142857\n",
            "step :  407 validation loss :  9598599.666666666\n",
            "step :  408 training loss :  11500828.047619049\n",
            "step :  408 validation loss :  9566260.5\n",
            "step :  409 training loss :  11468055.904761905\n",
            "step :  409 validation loss :  9535925.666666666\n",
            "step :  410 training loss :  11431598.285714285\n",
            "step :  410 validation loss :  9507188.333333334\n",
            "step :  411 training loss :  11404540.30952381\n",
            "step :  411 validation loss :  9483518.166666666\n",
            "step :  412 training loss :  11372327.19047619\n",
            "step :  412 validation loss :  9455526.166666666\n",
            "step :  413 training loss :  11350156.523809524\n",
            "step :  413 validation loss :  9468059.833333334\n",
            "step :  414 training loss :  11356687.738095239\n",
            "step :  414 validation loss :  9443619.5\n",
            "step :  415 training loss :  11329222.047619049\n",
            "step :  415 validation loss :  9419948.0\n",
            "step :  416 training loss :  11302587.833333334\n",
            "step :  416 validation loss :  9396986.0\n",
            "step :  417 training loss :  11276712.61904762\n",
            "step :  417 validation loss :  9374682.833333334\n",
            "step :  418 training loss :  11309940.547619049\n",
            "step :  418 validation loss :  9429119.333333334\n",
            "step :  419 training loss :  11314672.642857144\n",
            "step :  419 validation loss :  9408884.5\n",
            "step :  420 training loss :  11292347.904761905\n",
            "step :  420 validation loss :  9404799.0\n",
            "step :  421 training loss :  11287335.857142856\n",
            "step :  421 validation loss :  9385527.833333334\n",
            "step :  422 training loss :  11393664.57142857\n",
            "step :  422 validation loss :  9521530.166666666\n",
            "step :  423 training loss :  11420557.19047619\n",
            "step :  423 validation loss :  9502083.0\n",
            "step :  424 training loss :  11398330.333333334\n",
            "step :  424 validation loss :  9482586.0\n",
            "step :  425 training loss :  11376215.047619049\n",
            "step :  425 validation loss :  9463420.666666666\n",
            "step :  426 training loss :  11354531.80952381\n",
            "step :  426 validation loss :  9444572.666666666\n",
            "step :  427 training loss :  11333197.023809524\n",
            "step :  427 validation loss :  9426032.833333334\n",
            "step :  428 training loss :  11387530.42857143\n",
            "step :  428 validation loss :  9542470.833333334\n",
            "step :  429 training loss :  11454588.666666666\n",
            "step :  429 validation loss :  9564856.0\n",
            "step :  430 training loss :  11480891.38095238\n",
            "step :  430 validation loss :  9563453.666666666\n",
            "step :  431 training loss :  11471359.476190476\n",
            "step :  431 validation loss :  9549840.166666666\n",
            "step :  432 training loss :  11455964.80952381\n",
            "step :  432 validation loss :  9536387.5\n",
            "step :  433 training loss :  11440749.142857144\n",
            "step :  433 validation loss :  9523097.5\n",
            "step :  434 training loss :  11425709.0\n",
            "step :  434 validation loss :  9509962.333333334\n",
            "step :  435 training loss :  11410837.07142857\n",
            "step :  435 validation loss :  9496977.0\n",
            "step :  436 training loss :  11396127.142857144\n",
            "step :  436 validation loss :  9484137.5\n",
            "step :  437 training loss :  11381571.952380951\n",
            "step :  437 validation loss :  9471435.5\n",
            "step :  438 training loss :  11367166.11904762\n",
            "step :  438 validation loss :  9458868.0\n",
            "step :  439 training loss :  11352903.952380951\n",
            "step :  439 validation loss :  9446428.666666666\n",
            "step :  440 training loss :  11338778.88095238\n",
            "step :  440 validation loss :  9434114.333333334\n",
            "step :  441 training loss :  11324776.19047619\n",
            "step :  441 validation loss :  9421915.166666666\n",
            "step :  442 training loss :  11329019.142857144\n",
            "step :  442 validation loss :  9426874.666666666\n",
            "step :  443 training loss :  11310909.785714285\n",
            "step :  443 validation loss :  9404988.333333334\n",
            "step :  444 training loss :  11287199.19047619\n",
            "step :  444 validation loss :  9385311.0\n",
            "step :  445 training loss :  11265531.19047619\n",
            "step :  445 validation loss :  9367070.333333334\n",
            "step :  446 training loss :  11245161.857142856\n",
            "step :  446 validation loss :  9349790.0\n",
            "step :  447 training loss :  11317883.761904761\n",
            "step :  447 validation loss :  9679295.666666666\n",
            "step :  448 training loss :  11599866.19047619\n",
            "step :  448 validation loss :  9659513.666666666\n",
            "step :  449 training loss :  11576238.595238095\n",
            "step :  449 validation loss :  9638304.166666666\n",
            "step :  450 training loss :  11552374.61904762\n",
            "step :  450 validation loss :  9617512.333333334\n",
            "step :  451 training loss :  11529075.42857143\n",
            "step :  451 validation loss :  9597261.833333334\n",
            "step :  452 training loss :  11506354.904761905\n",
            "step :  452 validation loss :  9577505.833333334\n",
            "step :  453 training loss :  11484156.023809524\n",
            "step :  453 validation loss :  9558199.0\n",
            "step :  454 training loss :  11462429.047619049\n",
            "step :  454 validation loss :  9539297.166666666\n",
            "step :  455 training loss :  11441130.57142857\n",
            "step :  455 validation loss :  9520764.833333334\n",
            "step :  456 training loss :  11420226.285714285\n",
            "step :  456 validation loss :  9502577.666666666\n",
            "step :  457 training loss :  11399687.88095238\n",
            "step :  457 validation loss :  9484707.5\n",
            "step :  458 training loss :  11379487.30952381\n",
            "step :  458 validation loss :  9467137.833333334\n",
            "step :  459 training loss :  11365396.30952381\n",
            "step :  459 validation loss :  9470326.0\n",
            "step :  460 training loss :  11364142.952380951\n",
            "step :  460 validation loss :  9454673.0\n",
            "step :  461 training loss :  11346392.857142856\n",
            "step :  461 validation loss :  9439226.833333334\n",
            "step :  462 training loss :  11328872.61904762\n",
            "step :  462 validation loss :  9423989.5\n",
            "step :  463 training loss :  11311573.738095239\n",
            "step :  463 validation loss :  9408949.0\n",
            "step :  464 training loss :  11319717.547619049\n",
            "step :  464 validation loss :  9482592.0\n",
            "step :  465 training loss :  11378841.285714285\n",
            "step :  465 validation loss :  9468009.666666666\n",
            "step :  466 training loss :  11361829.952380951\n",
            "step :  466 validation loss :  9452968.333333334\n",
            "step :  467 training loss :  11344713.976190476\n",
            "step :  467 validation loss :  9438040.5\n",
            "step :  468 training loss :  11327762.261904761\n",
            "step :  468 validation loss :  9423282.0\n",
            "step :  469 training loss :  11310996.285714285\n",
            "step :  469 validation loss :  9408693.5\n",
            "step :  470 training loss :  11294499.785714285\n",
            "step :  470 validation loss :  9389605.166666666\n",
            "step :  471 training loss :  11306620.07142857\n",
            "step :  471 validation loss :  9425969.666666666\n",
            "step :  472 training loss :  11311116.357142856\n",
            "step :  472 validation loss :  9406312.166666666\n",
            "step :  473 training loss :  11289470.69047619\n",
            "step :  473 validation loss :  9388034.5\n",
            "step :  474 training loss :  11269105.666666666\n",
            "step :  474 validation loss :  9370680.833333334\n",
            "step :  475 training loss :  11249636.0\n",
            "step :  475 validation loss :  9354006.833333334\n",
            "step :  476 training loss :  11230841.952380951\n",
            "step :  476 validation loss :  9337861.666666666\n",
            "step :  477 training loss :  11212583.80952381\n",
            "step :  477 validation loss :  9322149.0\n",
            "step :  478 training loss :  11194774.30952381\n",
            "step :  478 validation loss :  9306801.333333334\n",
            "step :  479 training loss :  11177326.452380951\n",
            "step :  479 validation loss :  9291676.5\n",
            "step :  480 training loss :  11159943.547619049\n",
            "step :  480 validation loss :  9276243.666666666\n",
            "step :  481 training loss :  11214725.238095239\n",
            "step :  481 validation loss :  9326456.5\n",
            "step :  482 training loss :  11199216.166666666\n",
            "step :  482 validation loss :  9310202.0\n",
            "step :  483 training loss :  11180345.904761905\n",
            "step :  483 validation loss :  9293233.166666666\n",
            "step :  484 training loss :  11232239.523809524\n",
            "step :  484 validation loss :  9369523.0\n",
            "step :  485 training loss :  11249496.07142857\n",
            "step :  485 validation loss :  9354687.166666666\n",
            "step :  486 training loss :  11232602.642857144\n",
            "step :  486 validation loss :  9340492.0\n",
            "step :  487 training loss :  11216386.214285715\n",
            "step :  487 validation loss :  9326132.166666666\n",
            "step :  488 training loss :  11199980.833333334\n",
            "step :  488 validation loss :  9311884.666666666\n",
            "step :  489 training loss :  11183691.095238095\n",
            "step :  489 validation loss :  9297626.5\n",
            "step :  490 training loss :  11375814.738095239\n",
            "step :  490 validation loss :  9730985.333333334\n",
            "step :  491 training loss :  11661280.761904761\n",
            "step :  491 validation loss :  9717121.666666666\n",
            "step :  492 training loss :  11645574.19047619\n",
            "step :  492 validation loss :  9703278.166666666\n",
            "step :  493 training loss :  11629974.714285715\n",
            "step :  493 validation loss :  9689575.333333334\n",
            "step :  494 training loss :  11614537.142857144\n",
            "step :  494 validation loss :  9676022.333333334\n",
            "step :  495 training loss :  11659546.142857144\n",
            "step :  495 validation loss :  9747875.166666666\n",
            "step :  496 training loss :  11681462.88095238\n",
            "step :  496 validation loss :  9736013.333333334\n",
            "step :  497 training loss :  11668090.761904761\n",
            "step :  497 validation loss :  9724242.333333334\n",
            "step :  498 training loss :  11654827.666666666\n",
            "step :  498 validation loss :  9712574.666666666\n",
            "step :  499 training loss :  11641677.642857144\n",
            "step :  499 validation loss :  9701010.833333334\n",
            "step :  500 training loss :  11628638.095238095\n",
            "step :  500 validation loss :  9689545.0\n",
            "step :  501 training loss :  11615705.357142856\n",
            "step :  501 validation loss :  9678177.333333334\n",
            "step :  502 training loss :  11602876.404761905\n",
            "step :  502 validation loss :  9666901.333333334\n",
            "step :  503 training loss :  11590147.38095238\n",
            "step :  503 validation loss :  9655717.0\n",
            "step :  504 training loss :  11577515.976190476\n",
            "step :  504 validation loss :  9644620.833333334\n",
            "step :  505 training loss :  11564978.095238095\n",
            "step :  505 validation loss :  9633604.833333334\n",
            "step :  506 training loss :  11625624.904761905\n",
            "step :  506 validation loss :  9742096.666666666\n",
            "step :  507 training loss :  11675601.642857144\n",
            "step :  507 validation loss :  9731452.333333334\n",
            "step :  508 training loss :  11663577.904761905\n",
            "step :  508 validation loss :  9720865.0\n",
            "step :  509 training loss :  11651653.69047619\n",
            "step :  509 validation loss :  9710384.0\n",
            "step :  510 training loss :  11650207.785714285\n",
            "step :  510 validation loss :  9712200.666666666\n",
            "step :  511 training loss :  11642341.714285715\n",
            "step :  511 validation loss :  9702630.0\n",
            "step :  512 training loss :  11631568.666666666\n",
            "step :  512 validation loss :  9693166.666666666\n",
            "step :  513 training loss :  11620900.714285715\n",
            "step :  513 validation loss :  9683791.666666666\n",
            "step :  514 training loss :  11610324.11904762\n",
            "step :  514 validation loss :  9674496.666666666\n",
            "step :  515 training loss :  11599834.523809524\n",
            "step :  515 validation loss :  9665279.5\n",
            "step :  516 training loss :  11589428.714285715\n",
            "step :  516 validation loss :  9656135.666666666\n",
            "step :  517 training loss :  11579102.333333334\n",
            "step :  517 validation loss :  9647063.666666666\n",
            "step :  518 training loss :  11578255.214285715\n",
            "step :  518 validation loss :  9671405.166666666\n",
            "step :  519 training loss :  11597223.523809524\n",
            "step :  519 validation loss :  9663807.0\n",
            "step :  520 training loss :  11588633.547619049\n",
            "step :  520 validation loss :  9656253.166666666\n",
            "step :  521 training loss :  11580096.666666666\n",
            "step :  521 validation loss :  9648748.666666666\n",
            "step :  522 training loss :  11571614.642857144\n",
            "step :  522 validation loss :  9641294.333333334\n",
            "step :  523 training loss :  11569468.285714285\n",
            "step :  523 validation loss :  9755236.333333334\n",
            "step :  524 training loss :  11687448.404761905\n",
            "step :  524 validation loss :  9748456.666666666\n",
            "step :  525 training loss :  11684469.595238095\n",
            "step :  525 validation loss :  9740847.666666666\n",
            "step :  526 training loss :  11675819.452380951\n",
            "step :  526 validation loss :  9733193.0\n",
            "step :  527 training loss :  11667181.595238095\n",
            "step :  527 validation loss :  9725578.5\n",
            "step :  528 training loss :  11658591.785714285\n",
            "step :  528 validation loss :  9718011.0\n",
            "step :  529 training loss :  11650052.976190476\n",
            "step :  529 validation loss :  9710490.333333334\n",
            "step :  530 training loss :  11641563.285714285\n",
            "step :  530 validation loss :  9703013.666666666\n",
            "step :  531 training loss :  11633121.61904762\n",
            "step :  531 validation loss :  9695580.0\n",
            "step :  532 training loss :  11624726.047619049\n",
            "step :  532 validation loss :  9688188.0\n",
            "step :  533 training loss :  11616374.80952381\n",
            "step :  533 validation loss :  9680836.5\n",
            "step :  534 training loss :  11608066.69047619\n",
            "step :  534 validation loss :  9673523.833333334\n",
            "step :  535 training loss :  11599800.547619049\n",
            "step :  535 validation loss :  9666248.333333334\n",
            "step :  536 training loss :  11601194.452380951\n",
            "step :  536 validation loss :  9659082.666666666\n",
            "step :  537 training loss :  11583482.047619049\n",
            "step :  537 validation loss :  9651894.666666666\n",
            "step :  538 training loss :  11575340.0\n",
            "step :  538 validation loss :  9644727.5\n",
            "step :  539 training loss :  11591972.30952381\n",
            "step :  539 validation loss :  9671818.0\n",
            "step :  540 training loss :  11591790.333333334\n",
            "step :  540 validation loss :  9653872.0\n",
            "step :  541 training loss :  11572746.42857143\n",
            "step :  541 validation loss :  9638216.666666666\n",
            "step :  542 training loss :  11555882.738095239\n",
            "step :  542 validation loss :  9624070.666666666\n",
            "step :  543 training loss :  11540258.333333334\n",
            "step :  543 validation loss :  9610505.0\n",
            "step :  544 training loss :  11526541.476190476\n",
            "step :  544 validation loss :  9599932.666666666\n",
            "step :  545 training loss :  11514717.976190476\n",
            "step :  545 validation loss :  9589799.166666666\n",
            "step :  546 training loss :  11503329.11904762\n",
            "step :  546 validation loss :  9579877.5\n",
            "step :  547 training loss :  11492146.69047619\n",
            "step :  547 validation loss :  9570120.666666666\n",
            "step :  548 training loss :  11599122.166666666\n",
            "step :  548 validation loss :  9701465.5\n",
            "step :  549 training loss :  11622690.714285715\n",
            "step :  549 validation loss :  9678527.666666666\n",
            "step :  550 training loss :  11597846.19047619\n",
            "step :  550 validation loss :  9657660.833333334\n",
            "step :  551 training loss :  11575104.952380951\n",
            "step :  551 validation loss :  9638364.166666666\n",
            "step :  552 training loss :  11553854.61904762\n",
            "step :  552 validation loss :  9620174.5\n",
            "step :  553 training loss :  11533684.404761905\n",
            "step :  553 validation loss :  9602819.166666666\n",
            "step :  554 training loss :  11569558.023809524\n",
            "step :  554 validation loss :  9643316.5\n",
            "step :  555 training loss :  11560633.547619049\n",
            "step :  555 validation loss :  9627065.5\n",
            "step :  556 training loss :  11542278.07142857\n",
            "step :  556 validation loss :  9611049.833333334\n",
            "step :  557 training loss :  11524316.142857144\n",
            "step :  557 validation loss :  9595436.333333334\n",
            "step :  558 training loss :  11520603.92857143\n",
            "step :  558 validation loss :  9596449.0\n",
            "step :  559 training loss :  11507305.61904762\n",
            "step :  559 validation loss :  9580052.166666666\n",
            "step :  560 training loss :  11488851.452380951\n",
            "step :  560 validation loss :  9563986.666666666\n",
            "step :  561 training loss :  11470813.833333334\n",
            "step :  561 validation loss :  9548331.0\n",
            "step :  562 training loss :  11453181.404761905\n",
            "step :  562 validation loss :  9532994.333333334\n",
            "step :  563 training loss :  11507935.785714285\n",
            "step :  563 validation loss :  9669300.333333334\n",
            "step :  564 training loss :  11576896.30952381\n",
            "step :  564 validation loss :  9648158.333333334\n",
            "step :  565 training loss :  11592013.19047619\n",
            "step :  565 validation loss :  9659965.666666666\n",
            "step :  566 training loss :  11576783.857142856\n",
            "step :  566 validation loss :  9638890.666666666\n",
            "step :  567 training loss :  11553404.261904761\n",
            "step :  567 validation loss :  9618799.666666666\n",
            "step :  568 training loss :  11531105.285714285\n",
            "step :  568 validation loss :  9599595.333333334\n",
            "step :  569 training loss :  11509700.261904761\n",
            "step :  569 validation loss :  9581105.333333334\n",
            "step :  570 training loss :  11489018.642857144\n",
            "step :  570 validation loss :  9574101.666666666\n",
            "step :  571 training loss :  11482355.42857143\n",
            "step :  571 validation loss :  9558042.833333334\n",
            "step :  572 training loss :  11463372.88095238\n",
            "step :  572 validation loss :  9541125.0\n",
            "step :  573 training loss :  11444233.11904762\n",
            "step :  573 validation loss :  9524450.666666666\n",
            "step :  574 training loss :  11411645.714285715\n",
            "step :  574 validation loss :  9467300.0\n",
            "step :  575 training loss :  11356104.285714285\n",
            "step :  575 validation loss :  9443509.166666666\n",
            "step :  576 training loss :  11365480.42857143\n",
            "step :  576 validation loss :  9599026.333333334\n",
            "step :  577 training loss :  11505257.595238095\n",
            "step :  577 validation loss :  9572146.666666666\n",
            "step :  578 training loss :  11472939.642857144\n",
            "step :  578 validation loss :  9543737.666666666\n",
            "step :  579 training loss :  11441614.595238095\n",
            "step :  579 validation loss :  9517148.5\n",
            "step :  580 training loss :  11415422.476190476\n",
            "step :  580 validation loss :  9501554.333333334\n",
            "step :  581 training loss :  11401633.57142857\n",
            "step :  581 validation loss :  9490162.666666666\n",
            "step :  582 training loss :  11382415.714285715\n",
            "step :  582 validation loss :  9466751.5\n",
            "step :  583 training loss :  11356196.904761905\n",
            "step :  583 validation loss :  9444250.333333334\n",
            "step :  584 training loss :  11330998.523809524\n",
            "step :  584 validation loss :  9422620.5\n",
            "step :  585 training loss :  11306697.857142856\n",
            "step :  585 validation loss :  9401671.166666666\n",
            "step :  586 training loss :  11283127.88095238\n",
            "step :  586 validation loss :  9381380.333333334\n",
            "step :  587 training loss :  11260151.452380951\n",
            "step :  587 validation loss :  9601421.333333334\n",
            "step :  588 training loss :  11508770.952380951\n",
            "step :  588 validation loss :  9582304.0\n",
            "step :  589 training loss :  11489806.80952381\n",
            "step :  589 validation loss :  9563317.666666666\n",
            "step :  590 training loss :  11468430.69047619\n",
            "step :  590 validation loss :  9544739.833333334\n",
            "step :  591 training loss :  11461428.976190476\n",
            "step :  591 validation loss :  9554863.0\n",
            "step :  592 training loss :  11460029.476190476\n",
            "step :  592 validation loss :  9538496.333333334\n",
            "step :  593 training loss :  11441577.07142857\n",
            "step :  593 validation loss :  9522446.333333334\n",
            "step :  594 training loss :  11423484.333333334\n",
            "step :  594 validation loss :  9506707.833333334\n",
            "step :  595 training loss :  11453753.69047619\n",
            "step :  595 validation loss :  9540873.833333334\n",
            "step :  596 training loss :  11444794.30952381\n",
            "step :  596 validation loss :  9525633.666666666\n",
            "step :  597 training loss :  11427397.095238095\n",
            "step :  597 validation loss :  9510399.0\n",
            "step :  598 training loss :  11432561.452380951\n",
            "step :  598 validation loss :  9523458.0\n",
            "step :  599 training loss :  11454281.0\n",
            "step :  599 validation loss :  9511321.0\n",
            "step :  600 training loss :  11412915.714285715\n",
            "step :  600 validation loss :  9499303.833333334\n",
            "step :  601 training loss :  11399312.38095238\n",
            "step :  601 validation loss :  9487435.666666666\n",
            "step :  602 training loss :  11385864.333333334\n",
            "step :  602 validation loss :  9475703.333333334\n",
            "step :  603 training loss :  11372553.547619049\n",
            "step :  603 validation loss :  9464093.333333334\n",
            "step :  604 training loss :  11359369.238095239\n",
            "step :  604 validation loss :  9452584.0\n",
            "step :  605 training loss :  11346249.785714285\n",
            "step :  605 validation loss :  9542774.166666666\n",
            "step :  606 training loss :  11443512.333333334\n",
            "step :  606 validation loss :  9520678.333333334\n",
            "step :  607 training loss :  11418384.023809524\n",
            "step :  607 validation loss :  9499658.666666666\n",
            "step :  608 training loss :  11395650.523809524\n",
            "step :  608 validation loss :  9480682.0\n",
            "step :  609 training loss :  11407757.285714285\n",
            "step :  609 validation loss :  9506824.666666666\n",
            "step :  610 training loss :  11443508.904761905\n",
            "step :  610 validation loss :  9584240.333333334\n",
            "step :  611 training loss :  11494741.88095238\n",
            "step :  611 validation loss :  9570100.166666666\n",
            "step :  612 training loss :  11478634.738095239\n",
            "step :  612 validation loss :  9556036.5\n",
            "step :  613 training loss :  11462851.30952381\n",
            "step :  613 validation loss :  9542357.666666666\n",
            "step :  614 training loss :  11447487.07142857\n",
            "step :  614 validation loss :  9931022.166666666\n",
            "step :  615 training loss :  11452971.976190476\n",
            "step :  615 validation loss :  9516128.333333334\n",
            "step :  616 training loss :  11433946.19047619\n",
            "step :  616 validation loss :  9515114.333333334\n",
            "step :  617 training loss :  11411980.333333334\n",
            "step :  617 validation loss :  9493580.0\n",
            "step :  618 training loss :  11399931.11904762\n",
            "step :  618 validation loss :  9480534.833333334\n",
            "step :  619 training loss :  11375151.42857143\n",
            "step :  619 validation loss :  9463926.5\n",
            "step :  620 training loss :  11396427.547619049\n",
            "step :  620 validation loss :  9491512.333333334\n",
            "step :  621 training loss :  11391884.904761905\n",
            "step :  621 validation loss :  9482292.833333334\n",
            "step :  622 training loss :  11381442.595238095\n",
            "step :  622 validation loss :  9473192.666666666\n",
            "step :  623 training loss :  11389857.666666666\n",
            "step :  623 validation loss :  9506631.333333334\n",
            "step :  624 training loss :  11409823.642857144\n",
            "step :  624 validation loss :  9498685.333333334\n",
            "step :  625 training loss :  11400776.642857144\n",
            "step :  625 validation loss :  9490771.333333334\n",
            "step :  626 training loss :  11400715.833333334\n",
            "step :  626 validation loss :  9537408.0\n",
            "step :  627 training loss :  11445306.0\n",
            "step :  627 validation loss :  9530096.0\n",
            "step :  628 training loss :  11436615.904761905\n",
            "step :  628 validation loss :  9522312.666666666\n",
            "step :  629 training loss :  11427756.285714285\n",
            "step :  629 validation loss :  9514552.166666666\n",
            "step :  630 training loss :  11418957.57142857\n",
            "step :  630 validation loss :  9506863.833333334\n",
            "step :  631 training loss :  11410241.42857143\n",
            "step :  631 validation loss :  9499251.833333334\n",
            "step :  632 training loss :  11401605.238095239\n",
            "step :  632 validation loss :  9491707.5\n",
            "step :  633 training loss :  11393044.61904762\n",
            "step :  633 validation loss :  9484232.333333334\n",
            "step :  634 training loss :  11384556.19047619\n",
            "step :  634 validation loss :  9476820.666666666\n",
            "step :  635 training loss :  11376137.285714285\n",
            "step :  635 validation loss :  9469470.833333334\n",
            "step :  636 training loss :  11367785.42857143\n",
            "step :  636 validation loss :  9462178.333333334\n",
            "step :  637 training loss :  11359496.904761905\n",
            "step :  637 validation loss :  9454942.666666666\n",
            "step :  638 training loss :  11351269.07142857\n",
            "step :  638 validation loss :  9447760.5\n",
            "step :  639 training loss :  11448923.30952381\n",
            "step :  639 validation loss :  9440814.833333334\n",
            "step :  640 training loss :  11335458.357142856\n",
            "step :  640 validation loss :  9434085.0\n",
            "step :  641 training loss :  11327565.07142857\n",
            "step :  641 validation loss :  9427084.833333334\n",
            "step :  642 training loss :  11319575.88095238\n",
            "step :  642 validation loss :  9420107.833333334\n",
            "step :  643 training loss :  11311620.714285715\n",
            "step :  643 validation loss :  9413164.833333334\n",
            "step :  644 training loss :  11303708.404761905\n",
            "step :  644 validation loss :  9406264.333333334\n",
            "step :  645 training loss :  11297456.166666666\n",
            "step :  645 validation loss :  9401448.333333334\n",
            "step :  646 training loss :  11289975.714285715\n",
            "step :  646 validation loss :  9393937.0\n",
            "step :  647 training loss :  11281384.738095239\n",
            "step :  647 validation loss :  9386439.666666666\n",
            "step :  648 training loss :  11272830.547619049\n",
            "step :  648 validation loss :  9378987.833333334\n",
            "step :  649 training loss :  11264326.30952381\n",
            "step :  649 validation loss :  9371577.666666666\n",
            "step :  650 training loss :  11255869.666666666\n",
            "step :  650 validation loss :  9364214.333333334\n",
            "step :  651 training loss :  11247442.023809524\n",
            "step :  651 validation loss :  9356865.5\n",
            "step :  652 training loss :  11239075.857142856\n",
            "step :  652 validation loss :  9349585.666666666\n",
            "step :  653 training loss :  11230753.976190476\n",
            "step :  653 validation loss :  9342326.333333334\n",
            "step :  654 training loss :  11259339.238095239\n",
            "step :  654 validation loss :  9396838.166666666\n",
            "step :  655 training loss :  11285924.047619049\n",
            "step :  655 validation loss :  9391522.833333334\n",
            "step :  656 training loss :  11279848.714285715\n",
            "step :  656 validation loss :  9386216.833333334\n",
            "step :  657 training loss :  11273788.785714285\n",
            "step :  657 validation loss :  9380927.0\n",
            "step :  658 training loss :  11267745.666666666\n",
            "step :  658 validation loss :  9375653.333333334\n",
            "step :  659 training loss :  11261720.142857144\n",
            "step :  659 validation loss :  9370397.166666666\n",
            "step :  660 training loss :  11255713.666666666\n",
            "step :  660 validation loss :  9365156.833333334\n",
            "step :  661 training loss :  11249724.80952381\n",
            "step :  661 validation loss :  9359933.833333334\n",
            "step :  662 training loss :  11243748.761904761\n",
            "step :  662 validation loss :  9354725.833333334\n",
            "step :  663 training loss :  11237789.61904762\n",
            "step :  663 validation loss :  9349516.0\n",
            "step :  664 training loss :  11231797.785714285\n",
            "step :  664 validation loss :  9344176.0\n",
            "step :  665 training loss :  11333073.357142856\n",
            "step :  665 validation loss :  9577633.0\n",
            "step :  666 training loss :  11490158.595238095\n",
            "step :  666 validation loss :  9568728.0\n",
            "step :  667 training loss :  11479691.80952381\n",
            "step :  667 validation loss :  9559409.333333334\n",
            "step :  668 training loss :  11469148.11904762\n",
            "step :  668 validation loss :  9550201.333333334\n",
            "step :  669 training loss :  11458759.404761905\n",
            "step :  669 validation loss :  9541140.166666666\n",
            "step :  670 training loss :  11448531.166666666\n",
            "step :  670 validation loss :  9532212.833333334\n",
            "step :  671 training loss :  11438459.595238095\n",
            "step :  671 validation loss :  9523431.833333334\n",
            "step :  672 training loss :  11428531.785714285\n",
            "step :  672 validation loss :  9514772.666666666\n",
            "step :  673 training loss :  11418723.642857144\n",
            "step :  673 validation loss :  9506209.666666666\n",
            "step :  674 training loss :  11409028.30952381\n",
            "step :  674 validation loss :  9497764.333333334\n",
            "step :  675 training loss :  11399458.80952381\n",
            "step :  675 validation loss :  9489415.833333334\n",
            "step :  676 training loss :  11389993.07142857\n",
            "step :  676 validation loss :  9481156.0\n",
            "step :  677 training loss :  11380621.38095238\n",
            "step :  677 validation loss :  9472977.333333334\n",
            "step :  678 training loss :  11371331.452380951\n",
            "step :  678 validation loss :  9464862.5\n",
            "step :  679 training loss :  11362107.642857144\n",
            "step :  679 validation loss :  9456799.0\n",
            "step :  680 training loss :  11352957.357142856\n",
            "step :  680 validation loss :  9448823.0\n",
            "step :  681 training loss :  11343895.30952381\n",
            "step :  681 validation loss :  9440918.0\n",
            "step :  682 training loss :  11334883.761904761\n",
            "step :  682 validation loss :  9432923.833333334\n",
            "step :  683 training loss :  11326232.595238095\n",
            "step :  683 validation loss :  9425457.333333334\n",
            "step :  684 training loss :  11317340.11904762\n",
            "step :  684 validation loss :  9417775.333333334\n",
            "step :  685 training loss :  11308628.642857144\n",
            "step :  685 validation loss :  9410169.666666666\n",
            "step :  686 training loss :  11299931.023809524\n",
            "step :  686 validation loss :  9402548.5\n",
            "step :  687 training loss :  11291213.92857143\n",
            "step :  687 validation loss :  9395079.166666666\n",
            "step :  688 training loss :  11282763.095238095\n",
            "step :  688 validation loss :  9387620.5\n",
            "step :  689 training loss :  11274237.404761905\n",
            "step :  689 validation loss :  9380179.166666666\n",
            "step :  690 training loss :  11265701.023809524\n",
            "step :  690 validation loss :  9372683.5\n",
            "step :  691 training loss :  11372427.261904761\n",
            "step :  691 validation loss :  9695543.666666666\n",
            "step :  692 training loss :  11619392.61904762\n",
            "step :  692 validation loss :  9678662.833333334\n",
            "step :  693 training loss :  11601038.357142856\n",
            "step :  693 validation loss :  9663319.833333334\n",
            "step :  694 training loss :  11584466.285714285\n",
            "step :  694 validation loss :  9649360.666666666\n",
            "step :  695 training loss :  11569149.142857144\n",
            "step :  695 validation loss :  9636276.666666666\n",
            "step :  696 training loss :  11554654.785714285\n",
            "step :  696 validation loss :  9623801.166666666\n",
            "step :  697 training loss :  11540759.333333334\n",
            "step :  697 validation loss :  9611788.333333334\n",
            "step :  698 training loss :  11527327.857142856\n",
            "step :  698 validation loss :  9600140.666666666\n",
            "step :  699 training loss :  11514271.714285715\n",
            "step :  699 validation loss :  9588796.166666666\n",
            "step :  700 training loss :  11501529.238095239\n",
            "step :  700 validation loss :  9577711.666666666\n",
            "step :  701 training loss :  11489057.19047619\n",
            "step :  701 validation loss :  9566850.666666666\n",
            "step :  702 training loss :  11476820.857142856\n",
            "step :  702 validation loss :  9556188.333333334\n",
            "step :  703 training loss :  11464791.30952381\n",
            "step :  703 validation loss :  9545701.166666666\n",
            "step :  704 training loss :  11452951.333333334\n",
            "step :  704 validation loss :  9535376.666666666\n",
            "step :  705 training loss :  11441279.69047619\n",
            "step :  705 validation loss :  9525193.666666666\n",
            "step :  706 training loss :  11497415.61904762\n",
            "step :  706 validation loss :  9596400.333333334\n",
            "step :  707 training loss :  11509776.07142857\n",
            "step :  707 validation loss :  9584572.333333334\n",
            "step :  708 training loss :  11496426.30952381\n",
            "step :  708 validation loss :  9572931.166666666\n",
            "step :  709 training loss :  11483318.238095239\n",
            "step :  709 validation loss :  9561514.333333334\n",
            "step :  710 training loss :  11470453.261904761\n",
            "step :  710 validation loss :  9550307.166666666\n",
            "step :  711 training loss :  11457811.0\n",
            "step :  711 validation loss :  9539287.333333334\n",
            "step :  712 training loss :  11445368.92857143\n",
            "step :  712 validation loss :  9528440.666666666\n",
            "step :  713 training loss :  11433109.42857143\n",
            "step :  713 validation loss :  9517750.666666666\n",
            "step :  714 training loss :  11433254.714285715\n",
            "step :  714 validation loss :  9523487.833333334\n",
            "step :  715 training loss :  11426001.19047619\n",
            "step :  715 validation loss :  9510045.5\n",
            "step :  716 training loss :  11417902.976190476\n",
            "step :  716 validation loss :  9517089.0\n",
            "step :  717 training loss :  11417241.547619049\n",
            "step :  717 validation loss :  9500769.5\n",
            "step :  718 training loss :  11400011.357142856\n",
            "step :  718 validation loss :  9484983.5\n",
            "step :  719 training loss :  11480897.666666666\n",
            "step :  719 validation loss :  9589169.166666666\n",
            "step :  720 training loss :  11497684.238095239\n",
            "step :  720 validation loss :  9570438.666666666\n",
            "step :  721 training loss :  11476905.61904762\n",
            "step :  721 validation loss :  9552613.666666666\n",
            "step :  722 training loss :  11457058.476190476\n",
            "step :  722 validation loss :  9535541.333333334\n",
            "step :  723 training loss :  11437987.5\n",
            "step :  723 validation loss :  9519100.333333334\n",
            "step :  724 training loss :  11440261.61904762\n",
            "step :  724 validation loss :  9531327.666666666\n",
            "step :  725 training loss :  11434187.833333334\n",
            "step :  725 validation loss :  9516413.5\n",
            "step :  726 training loss :  11416958.095238095\n",
            "step :  726 validation loss :  9501275.666666666\n",
            "step :  727 training loss :  11399873.61904762\n",
            "step :  727 validation loss :  9486448.666666666\n",
            "step :  728 training loss :  11383160.142857144\n",
            "step :  728 validation loss :  9471957.333333334\n",
            "step :  729 training loss :  11366806.61904762\n",
            "step :  729 validation loss :  9457773.333333334\n",
            "step :  730 training loss :  11350783.80952381\n",
            "step :  730 validation loss :  9443874.166666666\n",
            "step :  731 training loss :  11335064.285714285\n",
            "step :  731 validation loss :  9430235.5\n",
            "step :  732 training loss :  11319624.642857144\n",
            "step :  732 validation loss :  9416839.166666666\n",
            "step :  733 training loss :  11304443.142857144\n",
            "step :  733 validation loss :  9403663.0\n",
            "step :  734 training loss :  11371996.404761905\n",
            "step :  734 validation loss :  9556383.166666666\n",
            "step :  735 training loss :  11464219.523809524\n",
            "step :  735 validation loss :  9544440.5\n",
            "step :  736 training loss :  11450707.904761905\n",
            "step :  736 validation loss :  9532644.333333334\n",
            "step :  737 training loss :  11437383.11904762\n",
            "step :  737 validation loss :  9521030.666666666\n",
            "step :  738 training loss :  11424257.88095238\n",
            "step :  738 validation loss :  9509589.5\n",
            "step :  739 training loss :  11411319.523809524\n",
            "step :  739 validation loss :  9498313.333333334\n",
            "step :  740 training loss :  11398558.19047619\n",
            "step :  740 validation loss :  9487191.166666666\n",
            "step :  741 training loss :  11385962.761904761\n",
            "step :  741 validation loss :  9476215.5\n",
            "step :  742 training loss :  11373524.30952381\n",
            "step :  742 validation loss :  9465376.666666666\n",
            "step :  743 training loss :  11361234.095238095\n",
            "step :  743 validation loss :  9454670.333333334\n",
            "step :  744 training loss :  11349085.547619049\n",
            "step :  744 validation loss :  9444085.333333334\n",
            "step :  745 training loss :  11337066.952380951\n",
            "step :  745 validation loss :  9433603.5\n",
            "step :  746 training loss :  11325142.785714285\n",
            "step :  746 validation loss :  9423156.666666666\n",
            "step :  747 training loss :  11313262.11904762\n",
            "step :  747 validation loss :  9413069.5\n",
            "step :  748 training loss :  11301885.88095238\n",
            "step :  748 validation loss :  9403012.666666666\n",
            "step :  749 training loss :  11290409.61904762\n",
            "step :  749 validation loss :  9392995.666666666\n",
            "step :  750 training loss :  11279004.476190476\n",
            "step :  750 validation loss :  9383040.833333334\n",
            "step :  751 training loss :  11277287.738095239\n",
            "step :  751 validation loss :  9387923.833333334\n",
            "step :  752 training loss :  11297140.904761905\n",
            "step :  752 validation loss :  9412278.5\n",
            "step :  753 training loss :  11297241.80952381\n",
            "step :  753 validation loss :  9395529.0\n",
            "step :  754 training loss :  11278551.42857143\n",
            "step :  754 validation loss :  9379753.5\n",
            "step :  755 training loss :  11261098.30952381\n",
            "step :  755 validation loss :  9364998.666666666\n",
            "step :  756 training loss :  11244640.30952381\n",
            "step :  756 validation loss :  9350985.333333334\n",
            "step :  757 training loss :  11228915.07142857\n",
            "step :  757 validation loss :  9337535.333333334\n",
            "step :  758 training loss :  11213755.785714285\n",
            "step :  758 validation loss :  9324529.333333334\n",
            "step :  759 training loss :  11275861.595238095\n",
            "step :  759 validation loss :  9395147.666666666\n",
            "step :  760 training loss :  11274289.30952381\n",
            "step :  760 validation loss :  9372744.0\n",
            "step :  761 training loss :  11250011.785714285\n",
            "step :  761 validation loss :  9352657.833333334\n",
            "step :  762 training loss :  11227929.714285715\n",
            "step :  762 validation loss :  9334118.333333334\n",
            "step :  763 training loss :  11207305.023809524\n",
            "step :  763 validation loss :  9316636.833333334\n",
            "step :  764 training loss :  11187719.761904761\n",
            "step :  764 validation loss :  9299953.333333334\n",
            "step :  765 training loss :  11168936.904761905\n",
            "step :  765 validation loss :  9283895.666666666\n",
            "step :  766 training loss :  11150796.5\n",
            "step :  766 validation loss :  9268334.833333334\n",
            "step :  767 training loss :  11141864.69047619\n",
            "step :  767 validation loss :  9282756.5\n",
            "step :  768 training loss :  11145996.238095239\n",
            "step :  768 validation loss :  9260636.666666666\n",
            "step :  769 training loss :  11122115.238095239\n",
            "step :  769 validation loss :  9241178.0\n",
            "step :  770 training loss :  11099702.857142856\n",
            "step :  770 validation loss :  9222156.333333334\n",
            "step :  771 training loss :  11185646.904761905\n",
            "step :  771 validation loss :  10159193.166666666\n",
            "step :  772 training loss :  12125572.80952381\n",
            "step :  772 validation loss :  10112498.5\n",
            "step :  773 training loss :  12078405.80952381\n",
            "step :  773 validation loss :  10076238.166666666\n",
            "step :  774 training loss :  12038408.42857143\n",
            "step :  774 validation loss :  10041172.333333334\n",
            "step :  775 training loss :  12000477.642857144\n",
            "step :  775 validation loss :  10008640.0\n",
            "step :  776 training loss :  11965128.166666666\n",
            "step :  776 validation loss :  9978178.666666666\n",
            "step :  777 training loss :  11931840.523809524\n",
            "step :  777 validation loss :  9949375.333333334\n",
            "step :  778 training loss :  11910350.61904762\n",
            "step :  778 validation loss :  9921895.0\n",
            "step :  779 training loss :  11869877.69047619\n",
            "step :  779 validation loss :  9895501.333333334\n",
            "step :  780 training loss :  11840609.19047619\n",
            "step :  780 validation loss :  9869965.666666666\n",
            "step :  781 training loss :  11812268.07142857\n",
            "step :  781 validation loss :  9845232.166666666\n",
            "step :  782 training loss :  11806755.261904761\n",
            "step :  782 validation loss :  9821575.166666666\n",
            "step :  783 training loss :  11763746.047619049\n",
            "step :  783 validation loss :  9798736.333333334\n",
            "step :  784 training loss :  11733031.547619049\n",
            "step :  784 validation loss :  9776042.833333334\n",
            "step :  785 training loss :  11707593.0\n",
            "step :  785 validation loss :  9753765.166666666\n",
            "step :  786 training loss :  11682659.57142857\n",
            "step :  786 validation loss :  9731962.333333334\n",
            "step :  787 training loss :  11658258.261904761\n",
            "step :  787 validation loss :  9885262.166666666\n",
            "step :  788 training loss :  11870302.92857143\n",
            "step :  788 validation loss :  10125369.166666666\n",
            "step :  789 training loss :  12101149.88095238\n",
            "step :  789 validation loss :  10103181.166666666\n",
            "step :  790 training loss :  12075138.142857144\n",
            "step :  790 validation loss :  10079561.666666666\n",
            "step :  791 training loss :  12048773.595238095\n",
            "step :  791 validation loss :  10056233.166666666\n",
            "step :  792 training loss :  12022834.595238095\n",
            "step :  792 validation loss :  10033346.666666666\n",
            "step :  793 training loss :  11997372.285714285\n",
            "step :  793 validation loss :  10010879.166666666\n",
            "step :  794 training loss :  11972354.69047619\n",
            "step :  794 validation loss :  9988796.5\n",
            "step :  795 training loss :  11965761.285714285\n",
            "step :  795 validation loss :  9997192.0\n",
            "step :  796 training loss :  11956706.69047619\n",
            "step :  796 validation loss :  9974525.333333334\n",
            "step :  797 training loss :  11903075.904761905\n",
            "step :  797 validation loss :  9764556.666666666\n",
            "step :  798 training loss :  11572323.595238095\n",
            "step :  798 validation loss :  9619544.166666666\n",
            "step :  799 training loss :  11534198.57142857\n",
            "step :  799 validation loss :  9604358.666666666\n",
            "step :  800 training loss :  11530844.61904762\n",
            "step :  800 validation loss :  9603741.166666666\n",
            "step :  801 training loss :  11516902.214285715\n",
            "step :  801 validation loss :  9589734.666666666\n",
            "step :  802 training loss :  11517600.523809524\n",
            "step :  802 validation loss :  9598232.0\n",
            "step :  803 training loss :  11504738.80952381\n",
            "step :  803 validation loss :  9573878.666666666\n",
            "step :  804 training loss :  11478424.666666666\n",
            "step :  804 validation loss :  9551928.333333334\n",
            "step :  805 training loss :  11454424.214285715\n",
            "step :  805 validation loss :  9531610.666666666\n",
            "step :  806 training loss :  11431933.857142856\n",
            "step :  806 validation loss :  9512376.833333334\n",
            "step :  807 training loss :  11435295.095238095\n",
            "step :  807 validation loss :  9519576.5\n",
            "step :  808 training loss :  11419408.047619049\n",
            "step :  808 validation loss :  9502400.166666666\n",
            "step :  809 training loss :  11760492.42857143\n",
            "step :  809 validation loss :  9923547.333333334\n",
            "step :  810 training loss :  11875854.523809524\n",
            "step :  810 validation loss :  9904426.0\n",
            "step :  811 training loss :  11854311.42857143\n",
            "step :  811 validation loss :  9885429.5\n",
            "step :  812 training loss :  11833116.023809524\n",
            "step :  812 validation loss :  9866837.833333334\n",
            "step :  813 training loss :  11812355.38095238\n",
            "step :  813 validation loss :  9848630.5\n",
            "step :  814 training loss :  11792006.80952381\n",
            "step :  814 validation loss :  9830787.166666666\n",
            "step :  815 training loss :  11772042.69047619\n",
            "step :  815 validation loss :  9813262.333333334\n",
            "step :  816 training loss :  11750003.976190476\n",
            "step :  816 validation loss :  9774486.666666666\n",
            "step :  817 training loss :  11708411.904761905\n",
            "step :  817 validation loss :  9756859.0\n",
            "step :  818 training loss :  11688532.761904761\n",
            "step :  818 validation loss :  9739372.833333334\n",
            "step :  819 training loss :  11668883.642857144\n",
            "step :  819 validation loss :  9722129.5\n",
            "step :  820 training loss :  11649501.38095238\n",
            "step :  820 validation loss :  9705129.0\n",
            "step :  821 training loss :  11644492.452380951\n",
            "step :  821 validation loss :  9712189.666666666\n",
            "step :  822 training loss :  11638875.285714285\n",
            "step :  822 validation loss :  9696340.0\n",
            "step :  823 training loss :  11621053.92857143\n",
            "step :  823 validation loss :  9680714.333333334\n",
            "step :  824 training loss :  11603449.595238095\n",
            "step :  824 validation loss :  9665270.0\n",
            "step :  825 training loss :  11626823.904761905\n",
            "step :  825 validation loss :  9732936.0\n",
            "step :  826 training loss :  11664083.595238095\n",
            "step :  826 validation loss :  9720185.166666666\n",
            "step :  827 training loss :  11649681.476190476\n",
            "step :  827 validation loss :  9707499.333333334\n",
            "step :  828 training loss :  11646067.5\n",
            "step :  828 validation loss :  9694989.833333334\n",
            "step :  829 training loss :  11621264.404761905\n",
            "step :  829 validation loss :  9682507.833333334\n",
            "step :  830 training loss :  11607147.476190476\n",
            "step :  830 validation loss :  9670083.0\n",
            "step :  831 training loss :  11593108.523809524\n",
            "step :  831 validation loss :  9657737.166666666\n",
            "step :  832 training loss :  11579153.523809524\n",
            "step :  832 validation loss :  9645472.333333334\n",
            "step :  833 training loss :  11565285.92857143\n",
            "step :  833 validation loss :  9633285.0\n",
            "step :  834 training loss :  11551500.857142856\n",
            "step :  834 validation loss :  9621174.166666666\n",
            "step :  835 training loss :  11537790.952380951\n",
            "step :  835 validation loss :  9609144.166666666\n",
            "step :  836 training loss :  11524177.38095238\n",
            "step :  836 validation loss :  9597174.833333334\n",
            "step :  837 training loss :  11510634.11904762\n",
            "step :  837 validation loss :  9585290.166666666\n",
            "step :  838 training loss :  11497169.547619049\n",
            "step :  838 validation loss :  9573474.666666666\n",
            "step :  839 training loss :  11483779.285714285\n",
            "step :  839 validation loss :  9561728.833333334\n",
            "step :  840 training loss :  11470458.047619049\n",
            "step :  840 validation loss :  9550046.166666666\n",
            "step :  841 training loss :  11468041.214285715\n",
            "step :  841 validation loss :  9565303.0\n",
            "step :  842 training loss :  11474850.0\n",
            "step :  842 validation loss :  9554258.333333334\n",
            "step :  843 training loss :  11462333.476190476\n",
            "step :  843 validation loss :  9543287.5\n",
            "step :  844 training loss :  11554600.714285715\n",
            "step :  844 validation loss :  9669975.666666666\n",
            "step :  845 training loss :  11594229.238095239\n",
            "step :  845 validation loss :  9659875.333333334\n",
            "step :  846 training loss :  11582791.452380951\n",
            "step :  846 validation loss :  9649806.5\n",
            "step :  847 training loss :  11571395.714285715\n",
            "step :  847 validation loss :  9639778.833333334\n",
            "step :  848 training loss :  11560056.952380951\n",
            "step :  848 validation loss :  9629813.333333334\n",
            "step :  849 training loss :  11548773.047619049\n",
            "step :  849 validation loss :  9619889.166666666\n",
            "step :  850 training loss :  11537536.738095239\n",
            "step :  850 validation loss :  9610012.0\n",
            "step :  851 training loss :  11526348.833333334\n",
            "step :  851 validation loss :  9600181.333333334\n",
            "step :  852 training loss :  11515209.57142857\n",
            "step :  852 validation loss :  9590396.666666666\n",
            "step :  853 training loss :  11504117.714285715\n",
            "step :  853 validation loss :  9580656.0\n",
            "step :  854 training loss :  11493071.476190476\n",
            "step :  854 validation loss :  9570958.666666666\n",
            "step :  855 training loss :  11482070.07142857\n",
            "step :  855 validation loss :  9561301.666666666\n",
            "step :  856 training loss :  11471111.095238095\n",
            "step :  856 validation loss :  9551686.0\n",
            "step :  857 training loss :  11460189.5\n",
            "step :  857 validation loss :  9542102.0\n",
            "step :  858 training loss :  11590058.142857144\n",
            "step :  858 validation loss :  9701531.333333334\n",
            "step :  859 training loss :  11626921.214285715\n",
            "step :  859 validation loss :  9685789.666666666\n",
            "step :  860 training loss :  11609135.357142856\n",
            "step :  860 validation loss :  9670265.333333334\n",
            "step :  861 training loss :  11591798.476190476\n",
            "step :  861 validation loss :  9655210.666666666\n",
            "step :  862 training loss :  11574960.19047619\n",
            "step :  862 validation loss :  9640569.166666666\n",
            "step :  863 training loss :  11558546.738095239\n",
            "step :  863 validation loss :  9626276.0\n",
            "step :  864 training loss :  11542493.57142857\n",
            "step :  864 validation loss :  9612282.666666666\n",
            "step :  865 training loss :  11526750.61904762\n",
            "step :  865 validation loss :  9598548.833333334\n",
            "step :  866 training loss :  11511279.285714285\n",
            "step :  866 validation loss :  9585045.833333334\n",
            "step :  867 training loss :  11496055.738095239\n",
            "step :  867 validation loss :  9571761.5\n",
            "step :  868 training loss :  11481066.595238095\n",
            "step :  868 validation loss :  9558674.0\n",
            "step :  869 training loss :  11466295.69047619\n",
            "step :  869 validation loss :  9545764.166666666\n",
            "step :  870 training loss :  11451690.38095238\n",
            "step :  870 validation loss :  9532999.666666666\n",
            "step :  871 training loss :  11437245.142857144\n",
            "step :  871 validation loss :  9520383.0\n",
            "step :  872 training loss :  11422953.761904761\n",
            "step :  872 validation loss :  9507903.5\n",
            "step :  873 training loss :  11408960.523809524\n",
            "step :  873 validation loss :  9495562.5\n",
            "step :  874 training loss :  11463793.42857143\n",
            "step :  874 validation loss :  9567322.0\n",
            "step :  875 training loss :  11477345.166666666\n",
            "step :  875 validation loss :  9556601.5\n",
            "step :  876 training loss :  11465199.92857143\n",
            "step :  876 validation loss :  9545968.333333334\n",
            "step :  877 training loss :  11453153.976190476\n",
            "step :  877 validation loss :  9535428.333333334\n",
            "step :  878 training loss :  11441206.952380951\n",
            "step :  878 validation loss :  9524974.666666666\n",
            "step :  879 training loss :  11429354.547619049\n",
            "step :  879 validation loss :  9514609.5\n",
            "step :  880 training loss :  11417593.904761905\n",
            "step :  880 validation loss :  9504327.833333334\n",
            "step :  881 training loss :  11405917.976190476\n",
            "step :  881 validation loss :  9494116.5\n",
            "step :  882 training loss :  11394374.833333334\n",
            "step :  882 validation loss :  9484020.333333334\n",
            "step :  883 training loss :  11382903.38095238\n",
            "step :  883 validation loss :  9474018.166666666\n",
            "step :  884 training loss :  11371517.095238095\n",
            "step :  884 validation loss :  9464052.5\n",
            "step :  885 training loss :  11360180.761904761\n",
            "step :  885 validation loss :  9454152.666666666\n",
            "step :  886 training loss :  11348912.904761905\n",
            "step :  886 validation loss :  9444318.166666666\n",
            "step :  887 training loss :  11337714.666666666\n",
            "step :  887 validation loss :  9434547.833333334\n",
            "step :  888 training loss :  11326585.023809524\n",
            "step :  888 validation loss :  9424838.833333334\n",
            "step :  889 training loss :  11315523.238095239\n",
            "step :  889 validation loss :  9415187.833333334\n",
            "step :  890 training loss :  11300764.30952381\n",
            "step :  890 validation loss :  9405641.5\n",
            "step :  891 training loss :  11293061.80952381\n",
            "step :  891 validation loss :  9396164.666666666\n",
            "step :  892 training loss :  11282836.19047619\n",
            "step :  892 validation loss :  9386686.666666666\n",
            "step :  893 training loss :  11272019.595238095\n",
            "step :  893 validation loss :  9377252.666666666\n",
            "step :  894 training loss :  11261250.57142857\n",
            "step :  894 validation loss :  9367859.166666666\n",
            "step :  895 training loss :  11250492.166666666\n",
            "step :  895 validation loss :  9358462.5\n",
            "step :  896 training loss :  11239741.595238095\n",
            "step :  896 validation loss :  9349059.0\n",
            "step :  897 training loss :  11504025.07142857\n",
            "step :  897 validation loss :  9638102.0\n",
            "step :  898 training loss :  11557971.88095238\n",
            "step :  898 validation loss :  9627748.833333334\n",
            "step :  899 training loss :  11546129.785714285\n",
            "step :  899 validation loss :  9617286.166666666\n",
            "step :  900 training loss :  11534276.547619049\n",
            "step :  900 validation loss :  9606873.333333334\n",
            "step :  901 training loss :  11522486.5\n",
            "step :  901 validation loss :  9596523.666666666\n",
            "step :  902 training loss :  11510765.142857144\n",
            "step :  902 validation loss :  9586237.833333334\n",
            "step :  903 training loss :  11499111.11904762\n",
            "step :  903 validation loss :  9576014.0\n",
            "step :  904 training loss :  11487523.261904761\n",
            "step :  904 validation loss :  9565850.833333334\n",
            "step :  905 training loss :  11475999.07142857\n",
            "step :  905 validation loss :  9555745.833333334\n",
            "step :  906 training loss :  11464537.023809524\n",
            "step :  906 validation loss :  9545698.5\n",
            "step :  907 training loss :  11433139.61904762\n",
            "step :  907 validation loss :  9536306.666666666\n",
            "step :  908 training loss :  11442632.714285715\n",
            "step :  908 validation loss :  9526578.0\n",
            "step :  909 training loss :  11431432.714285715\n",
            "step :  909 validation loss :  9581213.166666666\n",
            "step :  910 training loss :  11489061.547619049\n",
            "step :  910 validation loss :  9562085.333333334\n",
            "step :  911 training loss :  11466544.404761905\n",
            "step :  911 validation loss :  9542919.333333334\n",
            "step :  912 training loss :  11445778.595238095\n",
            "step :  912 validation loss :  9525515.5\n",
            "step :  913 training loss :  11404885.785714285\n",
            "step :  913 validation loss :  9484700.0\n",
            "step :  914 training loss :  11390272.904761905\n",
            "step :  914 validation loss :  9549846.166666666\n",
            "step :  915 training loss :  11453462.761904761\n",
            "step :  915 validation loss :  9531302.333333334\n",
            "step :  916 training loss :  11431614.523809524\n",
            "step :  916 validation loss :  9512065.666666666\n",
            "step :  917 training loss :  11410104.38095238\n",
            "step :  917 validation loss :  9493584.833333334\n",
            "step :  918 training loss :  11389451.833333334\n",
            "step :  918 validation loss :  9475830.666666666\n",
            "step :  919 training loss :  11369545.047619049\n",
            "step :  919 validation loss :  9458643.333333334\n",
            "step :  920 training loss :  11431611.69047619\n",
            "step :  920 validation loss :  9517371.666666666\n",
            "step :  921 training loss :  11418126.261904761\n",
            "step :  921 validation loss :  9502395.833333334\n",
            "step :  922 training loss :  11401275.047619049\n",
            "step :  922 validation loss :  9515446.666666666\n",
            "step :  923 training loss :  11416551.666666666\n",
            "step :  923 validation loss :  9501618.833333334\n",
            "step :  924 training loss :  11586067.80952381\n",
            "step :  924 validation loss :  9677870.666666666\n",
            "step :  925 training loss :  11600964.80952381\n",
            "step :  925 validation loss :  9663733.5\n",
            "step :  926 training loss :  11585013.5\n",
            "step :  926 validation loss :  9649770.833333334\n",
            "step :  927 training loss :  11569319.666666666\n",
            "step :  927 validation loss :  9636064.166666666\n",
            "step :  928 training loss :  11553904.095238095\n",
            "step :  928 validation loss :  9622600.666666666\n",
            "step :  929 training loss :  11538755.30952381\n",
            "step :  929 validation loss :  9609436.5\n",
            "step :  930 training loss :  11524463.857142856\n",
            "step :  930 validation loss :  9596328.666666666\n",
            "step :  931 training loss :  11509134.666666666\n",
            "step :  931 validation loss :  9583493.666666666\n",
            "step :  932 training loss :  11494680.642857144\n",
            "step :  932 validation loss :  9570834.833333334\n",
            "step :  933 training loss :  11481779.42857143\n",
            "step :  933 validation loss :  9570533.833333334\n",
            "step :  934 training loss :  11474289.07142857\n",
            "step :  934 validation loss :  9547431.833333334\n",
            "step :  935 training loss :  11448609.333333334\n",
            "step :  935 validation loss :  9526026.166666666\n",
            "step :  936 training loss :  11425412.095238095\n",
            "step :  936 validation loss :  9506601.833333334\n",
            "step :  937 training loss :  11434416.666666666\n",
            "step :  937 validation loss :  9526787.666666666\n",
            "step :  938 training loss :  11428119.761904761\n",
            "step :  938 validation loss :  9510184.0\n",
            "step :  939 training loss :  11408982.857142856\n",
            "step :  939 validation loss :  9493441.666666666\n",
            "step :  940 training loss :  11467465.333333334\n",
            "step :  940 validation loss :  9594837.0\n",
            "step :  941 training loss :  11506346.61904762\n",
            "step :  941 validation loss :  9581044.666666666\n",
            "step :  942 training loss :  11492306.833333334\n",
            "step :  942 validation loss :  9569162.833333334\n",
            "step :  943 training loss :  11478829.30952381\n",
            "step :  943 validation loss :  9557341.666666666\n",
            "step :  944 training loss :  11465445.023809524\n",
            "step :  944 validation loss :  9545627.333333334\n",
            "step :  945 training loss :  11452175.07142857\n",
            "step :  945 validation loss :  9534015.166666666\n",
            "step :  946 training loss :  11439015.142857144\n",
            "step :  946 validation loss :  9522503.5\n",
            "step :  947 training loss :  11473338.61904762\n",
            "step :  947 validation loss :  9647317.333333334\n",
            "step :  948 training loss :  11565967.404761905\n",
            "step :  948 validation loss :  9632308.0\n",
            "step :  949 training loss :  11548606.761904761\n",
            "step :  949 validation loss :  9616932.0\n",
            "step :  950 training loss :  11531271.857142856\n",
            "step :  950 validation loss :  9601810.666666666\n",
            "step :  951 training loss :  11549248.69047619\n",
            "step :  951 validation loss :  9665565.333333334\n",
            "step :  952 training loss :  11609347.738095239\n",
            "step :  952 validation loss :  9692274.333333334\n",
            "step :  953 training loss :  11620387.833333334\n",
            "step :  953 validation loss :  9679652.166666666\n",
            "step :  954 training loss :  11603863.92857143\n",
            "step :  954 validation loss :  9667157.5\n",
            "step :  955 training loss :  11589810.047619049\n",
            "step :  955 validation loss :  9654853.0\n",
            "step :  956 training loss :  11575950.261904761\n",
            "step :  956 validation loss :  9642711.0\n",
            "step :  957 training loss :  11587178.785714285\n",
            "step :  957 validation loss :  9662909.5\n",
            "step :  958 training loss :  11584315.642857144\n",
            "step :  958 validation loss :  9649351.5\n",
            "step :  959 training loss :  11569030.214285715\n",
            "step :  959 validation loss :  9635981.5\n",
            "step :  960 training loss :  11553992.642857144\n",
            "step :  960 validation loss :  9622846.666666666\n",
            "step :  961 training loss :  11539206.238095239\n",
            "step :  961 validation loss :  9609928.0\n",
            "step :  962 training loss :  11524648.238095239\n",
            "step :  962 validation loss :  9597207.333333334\n",
            "step :  963 training loss :  11510297.047619049\n",
            "step :  963 validation loss :  9584666.0\n",
            "step :  964 training loss :  11496137.023809524\n",
            "step :  964 validation loss :  9572290.833333334\n",
            "step :  965 training loss :  11482151.666666666\n",
            "step :  965 validation loss :  9560071.0\n",
            "step :  966 training loss :  11468335.261904761\n",
            "step :  966 validation loss :  9547993.666666666\n",
            "step :  967 training loss :  11454674.166666666\n",
            "step :  967 validation loss :  9536055.833333334\n",
            "step :  968 training loss :  11441141.19047619\n",
            "step :  968 validation loss :  9524230.833333334\n",
            "step :  969 training loss :  11427716.857142856\n",
            "step :  969 validation loss :  9512513.666666666\n",
            "step :  970 training loss :  11411986.61904762\n",
            "step :  970 validation loss :  9501015.166666666\n",
            "step :  971 training loss :  11401482.80952381\n",
            "step :  971 validation loss :  9489556.166666666\n",
            "step :  972 training loss :  11503391.095238095\n",
            "step :  972 validation loss :  9652731.166666666\n",
            "step :  973 training loss :  11573761.047619049\n",
            "step :  973 validation loss :  9640875.333333334\n",
            "step :  974 training loss :  11560161.261904761\n",
            "step :  974 validation loss :  9628844.0\n",
            "step :  975 training loss :  11546546.904761905\n",
            "step :  975 validation loss :  9616893.0\n",
            "step :  976 training loss :  11533036.07142857\n",
            "step :  976 validation loss :  9605043.5\n",
            "step :  977 training loss :  11519637.047619049\n",
            "step :  977 validation loss :  9593296.166666666\n",
            "step :  978 training loss :  11506346.666666666\n",
            "step :  978 validation loss :  9581646.333333334\n",
            "step :  979 training loss :  11493161.547619049\n",
            "step :  979 validation loss :  9570092.166666666\n",
            "step :  980 training loss :  11480076.595238095\n",
            "step :  980 validation loss :  9558628.0\n",
            "step :  981 training loss :  11467088.047619049\n",
            "step :  981 validation loss :  9547252.333333334\n",
            "step :  982 training loss :  11454193.42857143\n",
            "step :  982 validation loss :  9535961.5\n",
            "step :  983 training loss :  11441389.142857144\n",
            "step :  983 validation loss :  9524753.166666666\n",
            "step :  984 training loss :  11428671.80952381\n",
            "step :  984 validation loss :  9513623.666666666\n",
            "step :  985 training loss :  11416039.595238095\n",
            "step :  985 validation loss :  9502572.0\n",
            "step :  986 training loss :  11403488.738095239\n",
            "step :  986 validation loss :  9491596.5\n",
            "step :  987 training loss :  11391018.404761905\n",
            "step :  987 validation loss :  9480693.5\n",
            "step :  988 training loss :  11378625.42857143\n",
            "step :  988 validation loss :  9469859.833333334\n",
            "step :  989 training loss :  11366307.476190476\n",
            "step :  989 validation loss :  9459093.666666666\n",
            "step :  990 training loss :  11354063.904761905\n",
            "step :  990 validation loss :  9448398.0\n",
            "step :  991 training loss :  11405514.42857143\n",
            "step :  991 validation loss :  9550505.0\n",
            "step :  992 training loss :  11453317.785714285\n",
            "step :  992 validation loss :  9530933.166666666\n",
            "step :  993 training loss :  11431523.595238095\n",
            "step :  993 validation loss :  9512444.333333334\n",
            "step :  994 training loss :  11411157.642857144\n",
            "step :  994 validation loss :  9495142.833333334\n",
            "step :  995 training loss :  11391936.404761905\n",
            "step :  995 validation loss :  9478691.833333334\n",
            "step :  996 training loss :  11373544.476190476\n",
            "step :  996 validation loss :  9462875.166666666\n",
            "step :  997 training loss :  11355786.142857144\n",
            "step :  997 validation loss :  9447557.666666666\n",
            "step :  998 training loss :  11338535.80952381\n",
            "step :  998 validation loss :  9432649.666666666\n",
            "step :  999 training loss :  11321709.095238095\n",
            "step :  999 validation loss :  9418088.666666666\n",
            "step :  1000 training loss :  11305245.238095239\n",
            "step :  1000 validation loss :  9403829.666666666\n",
            "step :  1001 training loss :  11289097.92857143\n",
            "step :  1001 validation loss :  9389837.333333334\n",
            "step :  1002 training loss :  11273232.857142856\n",
            "step :  1002 validation loss :  9376083.333333334\n",
            "step :  1003 training loss :  11257620.642857144\n",
            "step :  1003 validation loss :  9362544.833333334\n",
            "step :  1004 training loss :  11242239.80952381\n",
            "step :  1004 validation loss :  9349208.666666666\n",
            "step :  1005 training loss :  11227071.047619049\n",
            "step :  1005 validation loss :  9336056.166666666\n",
            "step :  1006 training loss :  11212097.69047619\n",
            "step :  1006 validation loss :  9323074.333333334\n",
            "step :  1007 training loss :  11197307.095238095\n",
            "step :  1007 validation loss :  9310251.5\n",
            "step :  1008 training loss :  11182683.095238095\n",
            "step :  1008 validation loss :  9297567.666666666\n",
            "step :  1009 training loss :  11168215.904761905\n",
            "step :  1009 validation loss :  9285033.5\n",
            "step :  1010 training loss :  11153906.023809524\n",
            "step :  1010 validation loss :  9272635.333333334\n",
            "step :  1011 training loss :  11139740.047619049\n",
            "step :  1011 validation loss :  9260365.0\n",
            "step :  1012 training loss :  11125709.857142856\n",
            "step :  1012 validation loss :  9248215.5\n",
            "step :  1013 training loss :  11111809.976190476\n",
            "step :  1013 validation loss :  9236182.833333334\n",
            "step :  1014 training loss :  11098033.285714285\n",
            "step :  1014 validation loss :  9224261.5\n",
            "step :  1015 training loss :  11084366.61904762\n",
            "step :  1015 validation loss :  9212442.666666666\n",
            "step :  1016 training loss :  11073974.38095238\n",
            "step :  1016 validation loss :  9207552.833333334\n",
            "step :  1017 training loss :  11064820.261904761\n",
            "step :  1017 validation loss :  9195179.833333334\n",
            "step :  1018 training loss :  11050571.952380951\n",
            "step :  1018 validation loss :  9182848.166666666\n",
            "step :  1019 training loss :  11038847.5\n",
            "step :  1019 validation loss :  9171918.666666666\n",
            "step :  1020 training loss :  11018357.42857143\n",
            "step :  1020 validation loss :  9150291.833333334\n",
            "step :  1021 training loss :  10994595.523809524\n",
            "step :  1021 validation loss :  9130719.333333334\n",
            "step :  1022 training loss :  11081207.88095238\n",
            "step :  1022 validation loss :  9241241.5\n",
            "step :  1023 training loss :  11095520.738095239\n",
            "step :  1023 validation loss :  9214834.833333334\n",
            "step :  1024 training loss :  11066624.047619049\n",
            "step :  1024 validation loss :  9191002.833333334\n",
            "step :  1025 training loss :  11040234.642857144\n",
            "step :  1025 validation loss :  9168975.333333334\n",
            "step :  1026 training loss :  11015569.904761905\n",
            "step :  1026 validation loss :  9148216.833333334\n",
            "step :  1027 training loss :  11083423.166666666\n",
            "step :  1027 validation loss :  9374737.166666666\n",
            "step :  1028 training loss :  11252658.88095238\n",
            "step :  1028 validation loss :  9354904.666666666\n",
            "step :  1029 training loss :  11212477.357142856\n",
            "step :  1029 validation loss :  9336032.166666666\n",
            "step :  1030 training loss :  11211796.761904761\n",
            "step :  1030 validation loss :  9315055.666666666\n",
            "step :  1031 training loss :  11179607.047619049\n",
            "step :  1031 validation loss :  9287440.333333334\n",
            "step :  1032 training loss :  11149334.357142856\n",
            "step :  1032 validation loss :  9262250.0\n",
            "step :  1033 training loss :  11121360.285714285\n",
            "step :  1033 validation loss :  9238711.666666666\n",
            "step :  1034 training loss :  11094983.38095238\n",
            "step :  1034 validation loss :  9216373.666666666\n",
            "step :  1035 training loss :  11069808.714285715\n",
            "step :  1035 validation loss :  9194979.5\n",
            "step :  1036 training loss :  11045598.738095239\n",
            "step :  1036 validation loss :  9174364.666666666\n",
            "step :  1037 training loss :  11022191.42857143\n",
            "step :  1037 validation loss :  9154407.833333334\n",
            "step :  1038 training loss :  10999470.023809524\n",
            "step :  1038 validation loss :  9135020.333333334\n",
            "step :  1039 training loss :  10977345.07142857\n",
            "step :  1039 validation loss :  9116137.0\n",
            "step :  1040 training loss :  10961887.666666666\n",
            "step :  1040 validation loss :  9134704.333333334\n",
            "step :  1041 training loss :  10978303.238095239\n",
            "step :  1041 validation loss :  9118167.666666666\n",
            "step :  1042 training loss :  10959376.404761905\n",
            "step :  1042 validation loss :  9101971.666666666\n",
            "step :  1043 training loss :  10940789.92857143\n",
            "step :  1043 validation loss :  9086071.833333334\n",
            "step :  1044 training loss :  10922478.92857143\n",
            "step :  1044 validation loss :  9070402.5\n",
            "step :  1045 training loss :  10904459.80952381\n",
            "step :  1045 validation loss :  9054961.333333334\n",
            "step :  1046 training loss :  10886409.785714285\n",
            "step :  1046 validation loss :  9038498.333333334\n",
            "step :  1047 training loss :  11282291.19047619\n",
            "step :  1047 validation loss :  9555593.666666666\n",
            "step :  1048 training loss :  11453633.166666666\n",
            "step :  1048 validation loss :  9526481.166666666\n",
            "step :  1049 training loss :  11421872.69047619\n",
            "step :  1049 validation loss :  9499808.666666666\n",
            "step :  1050 training loss :  11392520.523809524\n",
            "step :  1050 validation loss :  9474903.333333334\n",
            "step :  1051 training loss :  11364843.476190476\n",
            "step :  1051 validation loss :  9451245.666666666\n",
            "step :  1052 training loss :  11338390.261904761\n",
            "step :  1052 validation loss :  9428541.166666666\n",
            "step :  1053 training loss :  11312897.5\n",
            "step :  1053 validation loss :  9406609.5\n",
            "step :  1054 training loss :  11288196.547619049\n",
            "step :  1054 validation loss :  9385331.5\n",
            "step :  1055 training loss :  11264176.38095238\n",
            "step :  1055 validation loss :  9364625.833333334\n",
            "step :  1056 training loss :  11349427.761904761\n",
            "step :  1056 validation loss :  9468445.666666666\n",
            "step :  1057 training loss :  11446995.595238095\n",
            "step :  1057 validation loss :  9524644.833333334\n",
            "step :  1058 training loss :  11419886.238095239\n",
            "step :  1058 validation loss :  9496090.5\n",
            "step :  1059 training loss :  11387056.214285715\n",
            "step :  1059 validation loss :  9468968.166666666\n",
            "step :  1060 training loss :  11356838.42857143\n",
            "step :  1060 validation loss :  9443100.166666666\n",
            "step :  1061 training loss :  11336128.5\n",
            "step :  1061 validation loss :  9434562.166666666\n",
            "step :  1062 training loss :  11318101.11904762\n",
            "step :  1062 validation loss :  9409557.0\n",
            "step :  1063 training loss :  11289866.904761905\n",
            "step :  1063 validation loss :  9385229.0\n",
            "step :  1064 training loss :  11262412.404761905\n",
            "step :  1064 validation loss :  9361560.666666666\n",
            "step :  1065 training loss :  11290485.30952381\n",
            "step :  1065 validation loss :  9429083.833333334\n",
            "step :  1066 training loss :  11310489.80952381\n",
            "step :  1066 validation loss :  9401643.5\n",
            "step :  1067 training loss :  11347605.833333334\n",
            "step :  1067 validation loss :  9440522.0\n",
            "step :  1068 training loss :  11325184.261904761\n",
            "step :  1068 validation loss :  9416080.666666666\n",
            "step :  1069 training loss :  11297783.761904761\n",
            "step :  1069 validation loss :  9392558.0\n",
            "step :  1070 training loss :  11271328.761904761\n",
            "step :  1070 validation loss :  9369812.5\n",
            "step :  1071 training loss :  11248836.0\n",
            "step :  1071 validation loss :  9408573.5\n",
            "step :  1072 training loss :  11291773.785714285\n",
            "step :  1072 validation loss :  9388799.333333334\n",
            "step :  1073 training loss :  11267862.333333334\n",
            "step :  1073 validation loss :  9367443.166666666\n",
            "step :  1074 training loss :  11243539.357142856\n",
            "step :  1074 validation loss :  9346376.333333334\n",
            "step :  1075 training loss :  11219652.80952381\n",
            "step :  1075 validation loss :  9325753.166666666\n",
            "step :  1076 training loss :  11196246.214285715\n",
            "step :  1076 validation loss :  9305549.833333334\n",
            "step :  1077 training loss :  11173438.214285715\n",
            "step :  1077 validation loss :  9285714.0\n",
            "step :  1078 training loss :  11150775.785714285\n",
            "step :  1078 validation loss :  9266351.666666666\n",
            "step :  1079 training loss :  11128678.595238095\n",
            "step :  1079 validation loss :  9247294.166666666\n",
            "step :  1080 training loss :  11106940.88095238\n",
            "step :  1080 validation loss :  9228571.666666666\n",
            "step :  1081 training loss :  11085561.904761905\n",
            "step :  1081 validation loss :  9210158.0\n",
            "step :  1082 training loss :  11068578.80952381\n",
            "step :  1082 validation loss :  9206130.166666666\n",
            "step :  1083 training loss :  11060325.738095239\n",
            "step :  1083 validation loss :  9188825.666666666\n",
            "step :  1084 training loss :  11040565.57142857\n",
            "step :  1084 validation loss :  9171835.0\n",
            "step :  1085 training loss :  11112000.976190476\n",
            "step :  1085 validation loss :  9241384.0\n",
            "step :  1086 training loss :  11300962.238095239\n",
            "step :  1086 validation loss :  9415318.5\n",
            "step :  1087 training loss :  11301605.523809524\n",
            "step :  1087 validation loss :  9400285.833333334\n",
            "step :  1088 training loss :  11284509.785714285\n",
            "step :  1088 validation loss :  9385431.166666666\n",
            "step :  1089 training loss :  11267614.833333334\n",
            "step :  1089 validation loss :  9370734.666666666\n",
            "step :  1090 training loss :  11250883.0\n",
            "step :  1090 validation loss :  9356219.666666666\n",
            "step :  1091 training loss :  11297627.142857144\n",
            "step :  1091 validation loss :  9423601.0\n",
            "step :  1092 training loss :  11310282.11904762\n",
            "step :  1092 validation loss :  9407059.5\n",
            "step :  1093 training loss :  11291367.61904762\n",
            "step :  1093 validation loss :  9390577.5\n",
            "step :  1094 training loss :  11272627.595238095\n",
            "step :  1094 validation loss :  9374307.333333334\n",
            "step :  1095 training loss :  11254126.69047619\n",
            "step :  1095 validation loss :  9358252.333333334\n",
            "step :  1096 training loss :  11235861.833333334\n",
            "step :  1096 validation loss :  9342412.833333334\n",
            "step :  1097 training loss :  11217824.92857143\n",
            "step :  1097 validation loss :  9326773.666666666\n",
            "step :  1098 training loss :  11200004.452380951\n",
            "step :  1098 validation loss :  9311329.166666666\n",
            "step :  1099 training loss :  11182392.38095238\n",
            "step :  1099 validation loss :  9296072.166666666\n",
            "step :  1100 training loss :  11164982.833333334\n",
            "step :  1100 validation loss :  9280998.166666666\n",
            "step :  1101 training loss :  11147768.666666666\n",
            "step :  1101 validation loss :  9266100.666666666\n",
            "step :  1102 training loss :  11134913.666666666\n",
            "step :  1102 validation loss :  9251408.0\n",
            "step :  1103 training loss :  11113986.61904762\n",
            "step :  1103 validation loss :  9236904.0\n",
            "step :  1104 training loss :  11097341.595238095\n",
            "step :  1104 validation loss :  9222500.0\n",
            "step :  1105 training loss :  11080833.166666666\n",
            "step :  1105 validation loss :  9208238.166666666\n",
            "step :  1106 training loss :  11064468.738095239\n",
            "step :  1106 validation loss :  9194102.333333334\n",
            "step :  1107 training loss :  11111256.261904761\n",
            "step :  1107 validation loss :  9248434.166666666\n",
            "step :  1108 training loss :  11111719.166666666\n",
            "step :  1108 validation loss :  9236015.333333334\n",
            "step :  1109 training loss :  11097477.19047619\n",
            "step :  1109 validation loss :  9223696.333333334\n",
            "step :  1110 training loss :  11083338.333333334\n",
            "step :  1110 validation loss :  9211473.166666666\n",
            "step :  1111 training loss :  11069301.357142856\n",
            "step :  1111 validation loss :  9199323.333333334\n",
            "step :  1112 training loss :  11055348.857142856\n",
            "step :  1112 validation loss :  9187254.666666666\n",
            "step :  1113 training loss :  11041499.88095238\n",
            "step :  1113 validation loss :  9175286.5\n",
            "step :  1114 training loss :  11027752.642857144\n",
            "step :  1114 validation loss :  9163397.166666666\n",
            "step :  1115 training loss :  11014125.023809524\n",
            "step :  1115 validation loss :  9151667.666666666\n",
            "step :  1116 training loss :  11000600.19047619\n",
            "step :  1116 validation loss :  9139957.833333334\n",
            "step :  1117 training loss :  10987157.023809524\n",
            "step :  1117 validation loss :  9128370.5\n",
            "step :  1118 training loss :  10973834.333333334\n",
            "step :  1118 validation loss :  9116887.666666666\n",
            "step :  1119 training loss :  10960610.88095238\n",
            "step :  1119 validation loss :  9105472.833333334\n",
            "step :  1120 training loss :  10947467.80952381\n",
            "step :  1120 validation loss :  9094104.666666666\n",
            "step :  1121 training loss :  10959430.69047619\n",
            "step :  1121 validation loss :  9120584.0\n",
            "step :  1122 training loss :  10965860.952380951\n",
            "step :  1122 validation loss :  9110757.0\n",
            "step :  1123 training loss :  10954532.07142857\n",
            "step :  1123 validation loss :  9100988.166666666\n",
            "step :  1124 training loss :  10943269.285714285\n",
            "step :  1124 validation loss :  9091285.666666666\n",
            "step :  1125 training loss :  10932079.11904762\n",
            "step :  1125 validation loss :  9081652.166666666\n",
            "step :  1126 training loss :  10920962.07142857\n",
            "step :  1126 validation loss :  9072095.666666666\n",
            "step :  1127 training loss :  10909916.30952381\n",
            "step :  1127 validation loss :  9062579.166666666\n",
            "step :  1128 training loss :  10962454.30952381\n",
            "step :  1128 validation loss :  9125196.0\n",
            "step :  1129 training loss :  10972107.92857143\n",
            "step :  1129 validation loss :  9116922.5\n",
            "step :  1130 training loss :  10986171.142857144\n",
            "step :  1130 validation loss :  9208388.0\n",
            "step :  1131 training loss :  11068576.57142857\n",
            "step :  1131 validation loss :  9200942.666666666\n",
            "step :  1132 training loss :  11059992.80952381\n",
            "step :  1132 validation loss :  9193500.5\n",
            "step :  1133 training loss :  11051435.69047619\n",
            "step :  1133 validation loss :  9186095.5\n",
            "step :  1134 training loss :  11042921.42857143\n",
            "step :  1134 validation loss :  9178731.5\n",
            "step :  1135 training loss :  11034452.738095239\n",
            "step :  1135 validation loss :  9171409.5\n",
            "step :  1136 training loss :  11026030.452380951\n",
            "step :  1136 validation loss :  9164129.833333334\n",
            "step :  1137 training loss :  11017651.547619049\n",
            "step :  1137 validation loss :  9156887.5\n",
            "step :  1138 training loss :  11009314.238095239\n",
            "step :  1138 validation loss :  9149680.5\n",
            "step :  1139 training loss :  11001009.476190476\n",
            "step :  1139 validation loss :  9142388.0\n",
            "step :  1140 training loss :  11058748.0\n",
            "step :  1140 validation loss :  9194283.166666666\n",
            "step :  1141 training loss :  11051138.857142856\n",
            "step :  1141 validation loss :  9184694.5\n",
            "step :  1142 training loss :  11040077.452380951\n",
            "step :  1142 validation loss :  9175127.0\n",
            "step :  1143 training loss :  11029101.452380951\n",
            "step :  1143 validation loss :  9165663.333333334\n",
            "step :  1144 training loss :  11018241.214285715\n",
            "step :  1144 validation loss :  9156302.166666666\n",
            "step :  1145 training loss :  11007497.30952381\n",
            "step :  1145 validation loss :  9147049.0\n",
            "step :  1146 training loss :  10996871.761904761\n",
            "step :  1146 validation loss :  9137895.333333334\n",
            "step :  1147 training loss :  10986355.333333334\n",
            "step :  1147 validation loss :  9128837.833333334\n",
            "step :  1148 training loss :  10975939.714285715\n",
            "step :  1148 validation loss :  9119869.666666666\n",
            "step :  1149 training loss :  10965620.261904761\n",
            "step :  1149 validation loss :  9110987.333333334\n",
            "step :  1150 training loss :  10955393.666666666\n",
            "step :  1150 validation loss :  9102181.166666666\n",
            "step :  1151 training loss :  10945248.30952381\n",
            "step :  1151 validation loss :  9093441.5\n",
            "step :  1152 training loss :  10935165.476190476\n",
            "step :  1152 validation loss :  9084750.833333334\n",
            "step :  1153 training loss :  10925145.785714285\n",
            "step :  1153 validation loss :  9076127.0\n",
            "step :  1154 training loss :  10915164.523809524\n",
            "step :  1154 validation loss :  9067498.5\n",
            "step :  1155 training loss :  11002174.57142857\n",
            "step :  1155 validation loss :  9163087.666666666\n",
            "step :  1156 training loss :  11016277.333333334\n",
            "step :  1156 validation loss :  9155364.666666666\n",
            "step :  1157 training loss :  11007486.833333334\n",
            "step :  1157 validation loss :  9147810.166666666\n",
            "step :  1158 training loss :  10998777.952380951\n",
            "step :  1158 validation loss :  9140291.5\n",
            "step :  1159 training loss :  10990139.666666666\n",
            "step :  1159 validation loss :  9132852.666666666\n",
            "step :  1160 training loss :  10981587.738095239\n",
            "step :  1160 validation loss :  9125410.333333334\n",
            "step :  1161 training loss :  10972988.666666666\n",
            "step :  1161 validation loss :  9117857.333333334\n",
            "step :  1162 training loss :  10937643.5\n",
            "step :  1162 validation loss :  9058564.166666666\n",
            "step :  1163 training loss :  10817434.285714285\n",
            "step :  1163 validation loss :  8918846.666666666\n",
            "step :  1164 training loss :  10835935.047619049\n",
            "step :  1164 validation loss :  9124965.5\n",
            "step :  1165 training loss :  10973136.166666666\n",
            "step :  1165 validation loss :  9118991.333333334\n",
            "step :  1166 training loss :  10966176.976190476\n",
            "step :  1166 validation loss :  9112945.0\n",
            "step :  1167 training loss :  10959171.261904761\n",
            "step :  1167 validation loss :  9106766.5\n",
            "step :  1168 training loss :  11010466.023809524\n",
            "step :  1168 validation loss :  9165706.333333334\n",
            "step :  1169 training loss :  11018441.30952381\n",
            "step :  1169 validation loss :  9156445.833333334\n",
            "step :  1170 training loss :  11007727.761904761\n",
            "step :  1170 validation loss :  9147186.833333334\n",
            "step :  1171 training loss :  10997131.452380951\n",
            "step :  1171 validation loss :  9138087.666666666\n",
            "step :  1172 training loss :  10986706.19047619\n",
            "step :  1172 validation loss :  9129139.5\n",
            "step :  1173 training loss :  10976434.952380951\n",
            "step :  1173 validation loss :  9120311.5\n",
            "step :  1174 training loss :  10966339.904761905\n",
            "step :  1174 validation loss :  9111667.5\n",
            "step :  1175 training loss :  10956396.238095239\n",
            "step :  1175 validation loss :  9103097.666666666\n",
            "step :  1176 training loss :  10946531.833333334\n",
            "step :  1176 validation loss :  9094635.833333334\n",
            "step :  1177 training loss :  10936788.57142857\n",
            "step :  1177 validation loss :  9086272.0\n",
            "step :  1178 training loss :  10927163.333333334\n",
            "step :  1178 validation loss :  9078000.333333334\n",
            "step :  1179 training loss :  10917644.142857144\n",
            "step :  1179 validation loss :  9069827.333333334\n",
            "step :  1180 training loss :  10908221.642857144\n",
            "step :  1180 validation loss :  9061739.333333334\n",
            "step :  1181 training loss :  10898891.904761905\n",
            "step :  1181 validation loss :  9053734.833333334\n",
            "step :  1182 training loss :  10889651.5\n",
            "step :  1182 validation loss :  9045802.5\n",
            "step :  1183 training loss :  10880492.857142856\n",
            "step :  1183 validation loss :  9037944.166666666\n",
            "step :  1184 training loss :  10871426.142857144\n",
            "step :  1184 validation loss :  9030141.666666666\n",
            "step :  1185 training loss :  10914458.738095239\n",
            "step :  1185 validation loss :  9092722.666666666\n",
            "step :  1186 training loss :  10933535.30952381\n",
            "step :  1186 validation loss :  9082417.5\n",
            "step :  1187 training loss :  10917883.69047619\n",
            "step :  1187 validation loss :  9065904.5\n",
            "step :  1188 training loss :  10901978.38095238\n",
            "step :  1188 validation loss :  9054794.833333334\n",
            "step :  1189 training loss :  10889149.714285715\n",
            "step :  1189 validation loss :  9043795.833333334\n",
            "step :  1190 training loss :  10876470.285714285\n",
            "step :  1190 validation loss :  9032957.833333334\n",
            "step :  1191 training loss :  10863971.214285715\n",
            "step :  1191 validation loss :  9022266.5\n",
            "step :  1192 training loss :  10851616.80952381\n",
            "step :  1192 validation loss :  9011677.0\n",
            "step :  1193 training loss :  10850668.19047619\n",
            "step :  1193 validation loss :  9048105.666666666\n",
            "step :  1194 training loss :  10878405.30952381\n",
            "step :  1194 validation loss :  9031373.666666666\n",
            "step :  1195 training loss :  10858668.07142857\n",
            "step :  1195 validation loss :  9014637.166666666\n",
            "step :  1196 training loss :  10839803.761904761\n",
            "step :  1196 validation loss :  8998893.666666666\n",
            "step :  1197 training loss :  10821969.476190476\n",
            "step :  1197 validation loss :  8983934.333333334\n",
            "step :  1198 training loss :  10804921.238095239\n",
            "step :  1198 validation loss :  8969568.833333334\n",
            "step :  1199 training loss :  10788474.30952381\n",
            "step :  1199 validation loss :  8955660.666666666\n",
            "step :  1200 training loss :  10772550.80952381\n",
            "step :  1200 validation loss :  8942270.333333334\n",
            "step :  1201 training loss :  10757212.80952381\n",
            "step :  1201 validation loss :  8929162.5\n",
            "step :  1202 training loss :  10741973.357142856\n",
            "step :  1202 validation loss :  8916256.166666666\n",
            "step :  1203 training loss :  10727093.333333334\n",
            "step :  1203 validation loss :  8903565.5\n",
            "step :  1204 training loss :  10712371.404761905\n",
            "step :  1204 validation loss :  8891232.0\n",
            "step :  1205 training loss :  10698191.142857144\n",
            "step :  1205 validation loss :  8878866.5\n",
            "step :  1206 training loss :  10858890.404761905\n",
            "step :  1206 validation loss :  9210908.166666666\n",
            "step :  1207 training loss :  11068757.833333334\n",
            "step :  1207 validation loss :  9198439.0\n",
            "step :  1208 training loss :  11054418.476190476\n",
            "step :  1208 validation loss :  9186075.833333334\n",
            "step :  1209 training loss :  11040267.785714285\n",
            "step :  1209 validation loss :  9173909.666666666\n",
            "step :  1210 training loss :  11026376.19047619\n",
            "step :  1210 validation loss :  9161972.166666666\n",
            "step :  1211 training loss :  11012771.42857143\n",
            "step :  1211 validation loss :  9150281.666666666\n",
            "step :  1212 training loss :  10999345.57142857\n",
            "step :  1212 validation loss :  9138727.833333334\n",
            "step :  1213 training loss :  10986083.476190476\n",
            "step :  1213 validation loss :  9127333.833333334\n",
            "step :  1214 training loss :  10972981.5\n",
            "step :  1214 validation loss :  9116072.166666666\n",
            "step :  1215 training loss :  10960119.857142856\n",
            "step :  1215 validation loss :  9105311.5\n",
            "step :  1216 training loss :  10947667.42857143\n",
            "step :  1216 validation loss :  9094434.666666666\n",
            "step :  1217 training loss :  10935130.476190476\n",
            "step :  1217 validation loss :  9083663.333333334\n",
            "step :  1218 training loss :  10922733.80952381\n",
            "step :  1218 validation loss :  9073021.666666666\n",
            "step :  1219 training loss :  10910479.238095239\n",
            "step :  1219 validation loss :  9062506.0\n",
            "step :  1220 training loss :  10898359.904761905\n",
            "step :  1220 validation loss :  9052108.333333334\n",
            "step :  1221 training loss :  10943487.61904762\n",
            "step :  1221 validation loss :  9075049.666666666\n",
            "step :  1222 training loss :  10956734.833333334\n",
            "step :  1222 validation loss :  9105948.833333334\n",
            "step :  1223 training loss :  10945556.57142857\n",
            "step :  1223 validation loss :  9090265.166666666\n",
            "step :  1224 training loss :  10928208.5\n",
            "step :  1224 validation loss :  9075919.666666666\n",
            "step :  1225 training loss :  10912148.833333334\n",
            "step :  1225 validation loss :  9062480.166666666\n",
            "step :  1226 training loss :  10896956.785714285\n",
            "step :  1226 validation loss :  9049642.833333334\n",
            "step :  1227 training loss :  10882350.761904761\n",
            "step :  1227 validation loss :  9037311.166666666\n",
            "step :  1228 training loss :  10868247.904761905\n",
            "step :  1228 validation loss :  9025321.333333334\n",
            "step :  1229 training loss :  10854490.142857144\n",
            "step :  1229 validation loss :  9013617.833333334\n",
            "step :  1230 training loss :  10840904.38095238\n",
            "step :  1230 validation loss :  9001306.833333334\n",
            "step :  1231 training loss :  10828573.785714285\n",
            "step :  1231 validation loss :  8992505.166666666\n",
            "step :  1232 training loss :  10817036.476190476\n",
            "step :  1232 validation loss :  8981782.666666666\n",
            "step :  1233 training loss :  10804519.238095239\n",
            "step :  1233 validation loss :  8971095.166666666\n",
            "step :  1234 training loss :  10750356.023809524\n",
            "step :  1234 validation loss :  8929563.333333334\n",
            "step :  1235 training loss :  10744551.857142856\n",
            "step :  1235 validation loss :  8920029.666666666\n",
            "step :  1236 training loss :  10737286.095238095\n",
            "step :  1236 validation loss :  8917627.333333334\n",
            "step :  1237 training loss :  10730832.714285715\n",
            "step :  1237 validation loss :  8908525.166666666\n",
            "step :  1238 training loss :  10720406.07142857\n",
            "step :  1238 validation loss :  8899688.833333334\n",
            "step :  1239 training loss :  10710114.547619049\n",
            "step :  1239 validation loss :  8890926.333333334\n",
            "step :  1240 training loss :  10700017.92857143\n",
            "step :  1240 validation loss :  8882406.5\n",
            "step :  1241 training loss :  10690307.452380951\n",
            "step :  1241 validation loss :  8874378.5\n",
            "step :  1242 training loss :  10680649.904761905\n",
            "step :  1242 validation loss :  8866016.833333334\n",
            "step :  1243 training loss :  10670733.595238095\n",
            "step :  1243 validation loss :  8857622.0\n",
            "step :  1244 training loss :  10660853.095238095\n",
            "step :  1244 validation loss :  8849315.333333334\n",
            "step :  1245 training loss :  10651077.833333334\n",
            "step :  1245 validation loss :  8841028.333333334\n",
            "step :  1246 training loss :  10628604.0\n",
            "step :  1246 validation loss :  8817425.833333334\n",
            "step :  1247 training loss :  10676002.357142856\n",
            "step :  1247 validation loss :  8883770.166666666\n",
            "step :  1248 training loss :  10688595.404761905\n",
            "step :  1248 validation loss :  8870692.666666666\n",
            "step :  1249 training loss :  10673626.38095238\n",
            "step :  1249 validation loss :  8858339.5\n",
            "step :  1250 training loss :  10659244.92857143\n",
            "step :  1250 validation loss :  8846063.166666666\n",
            "step :  1251 training loss :  10644960.61904762\n",
            "step :  1251 validation loss :  8833758.166666666\n",
            "step :  1252 training loss :  10688457.69047619\n",
            "step :  1252 validation loss :  8983585.833333334\n",
            "step :  1253 training loss :  10806295.857142856\n",
            "step :  1253 validation loss :  8972308.5\n",
            "step :  1254 training loss :  10793243.595238095\n",
            "step :  1254 validation loss :  8961202.5\n",
            "step :  1255 training loss :  10780456.857142856\n",
            "step :  1255 validation loss :  8950388.333333334\n",
            "step :  1256 training loss :  10767955.07142857\n",
            "step :  1256 validation loss :  8939761.166666666\n",
            "step :  1257 training loss :  10755706.92857143\n",
            "step :  1257 validation loss :  8929478.833333334\n",
            "step :  1258 training loss :  10743704.92857143\n",
            "step :  1258 validation loss :  8919256.333333334\n",
            "step :  1259 training loss :  10731833.738095239\n",
            "step :  1259 validation loss :  8909123.0\n",
            "step :  1260 training loss :  10720075.976190476\n",
            "step :  1260 validation loss :  8899106.166666666\n",
            "step :  1261 training loss :  10708461.738095239\n",
            "step :  1261 validation loss :  8889196.5\n",
            "step :  1262 training loss :  10696909.333333334\n",
            "step :  1262 validation loss :  8879364.166666666\n",
            "step :  1263 training loss :  10685417.38095238\n",
            "step :  1263 validation loss :  8869584.166666666\n",
            "step :  1264 training loss :  10674036.714285715\n",
            "step :  1264 validation loss :  8859741.666666666\n",
            "step :  1265 training loss :  10650923.214285715\n",
            "step :  1265 validation loss :  8848230.833333334\n",
            "step :  1266 training loss :  10636662.0\n",
            "step :  1266 validation loss :  8822656.666666666\n",
            "step :  1267 training loss :  10614906.023809524\n",
            "step :  1267 validation loss :  8805749.0\n",
            "step :  1268 training loss :  10595786.904761905\n",
            "step :  1268 validation loss :  8790305.833333334\n",
            "step :  1269 training loss :  10655742.166666666\n",
            "step :  1269 validation loss :  8853496.166666666\n",
            "step :  1270 training loss :  10653382.261904761\n",
            "step :  1270 validation loss :  8840910.833333334\n",
            "step :  1271 training loss :  10638973.714285715\n",
            "step :  1271 validation loss :  8828909.166666666\n",
            "step :  1272 training loss :  10625170.5\n",
            "step :  1272 validation loss :  8817368.166666666\n",
            "step :  1273 training loss :  10611844.095238095\n",
            "step :  1273 validation loss :  8806194.5\n",
            "step :  1274 training loss :  10598898.5\n",
            "step :  1274 validation loss :  8795296.333333334\n",
            "step :  1275 training loss :  10586237.761904761\n",
            "step :  1275 validation loss :  8784641.333333334\n",
            "step :  1276 training loss :  10573856.452380951\n",
            "step :  1276 validation loss :  8774188.666666666\n",
            "step :  1277 training loss :  10561605.642857144\n",
            "step :  1277 validation loss :  8763775.166666666\n",
            "step :  1278 training loss :  10549484.595238095\n",
            "step :  1278 validation loss :  8753602.333333334\n",
            "step :  1279 training loss :  10537598.61904762\n",
            "step :  1279 validation loss :  8743456.666666666\n",
            "step :  1280 training loss :  10525429.857142856\n",
            "step :  1280 validation loss :  8731621.666666666\n",
            "step :  1281 training loss :  10513316.904761905\n",
            "step :  1281 validation loss :  8724800.333333334\n",
            "step :  1282 training loss :  10504456.904761905\n",
            "step :  1282 validation loss :  8715843.0\n",
            "step :  1283 training loss :  10493491.285714285\n",
            "step :  1283 validation loss :  8706591.166666666\n",
            "step :  1284 training loss :  10510800.166666666\n",
            "step :  1284 validation loss :  8792507.0\n",
            "step :  1285 training loss :  10584441.19047619\n",
            "step :  1285 validation loss :  8784305.0\n",
            "step :  1286 training loss :  10574830.0\n",
            "step :  1286 validation loss :  8776139.0\n",
            "step :  1287 training loss :  10565271.833333334\n",
            "step :  1287 validation loss :  8768007.333333334\n",
            "step :  1288 training loss :  10555762.214285715\n",
            "step :  1288 validation loss :  8759976.5\n",
            "step :  1289 training loss :  10546357.785714285\n",
            "step :  1289 validation loss :  8752025.833333334\n",
            "step :  1290 training loss :  10537025.357142856\n",
            "step :  1290 validation loss :  8744121.5\n",
            "step :  1291 training loss :  10527706.357142856\n",
            "step :  1291 validation loss :  8736252.166666666\n",
            "step :  1292 training loss :  10518416.666666666\n",
            "step :  1292 validation loss :  8728465.333333334\n",
            "step :  1293 training loss :  10509263.547619049\n",
            "step :  1293 validation loss :  8720721.333333334\n",
            "step :  1294 training loss :  10500188.452380951\n",
            "step :  1294 validation loss :  8712986.833333334\n",
            "step :  1295 training loss :  10491064.61904762\n",
            "step :  1295 validation loss :  8705085.166666666\n",
            "step :  1296 training loss :  10481378.42857143\n",
            "step :  1296 validation loss :  8696333.5\n",
            "step :  1297 training loss :  10580381.333333334\n",
            "step :  1297 validation loss :  9869349.0\n",
            "step :  1298 training loss :  11776787.738095239\n",
            "step :  1298 validation loss :  9791725.0\n",
            "step :  1299 training loss :  11703014.738095239\n",
            "step :  1299 validation loss :  9690385.333333334\n",
            "step :  1300 training loss :  11553601.0\n",
            "step :  1300 validation loss :  9602955.333333334\n",
            "step :  1301 training loss :  11505149.738095239\n",
            "step :  1301 validation loss :  9569886.5\n",
            "step :  1302 training loss :  11476954.095238095\n",
            "step :  1302 validation loss :  9561596.833333334\n",
            "step :  1303 training loss :  11470249.07142857\n",
            "step :  1303 validation loss :  9536696.333333334\n",
            "step :  1304 training loss :  11435482.92857143\n",
            "step :  1304 validation loss :  9513321.666666666\n",
            "step :  1305 training loss :  11409449.333333334\n",
            "step :  1305 validation loss :  9490987.0\n",
            "step :  1306 training loss :  11504171.095238095\n",
            "step :  1306 validation loss :  9636332.5\n",
            "step :  1307 training loss :  11549890.476190476\n",
            "step :  1307 validation loss :  9614829.5\n",
            "step :  1308 training loss :  11525484.61904762\n",
            "step :  1308 validation loss :  9593530.833333334\n",
            "step :  1309 training loss :  11501648.952380951\n",
            "step :  1309 validation loss :  9572883.0\n",
            "step :  1310 training loss :  11478532.88095238\n",
            "step :  1310 validation loss :  9552857.333333334\n",
            "step :  1311 training loss :  11456078.547619049\n",
            "step :  1311 validation loss :  9533396.5\n",
            "step :  1312 training loss :  11434239.61904762\n",
            "step :  1312 validation loss :  9514493.333333334\n",
            "step :  1313 training loss :  11419943.785714285\n",
            "step :  1313 validation loss :  9538679.666666666\n",
            "step :  1314 training loss :  11441007.261904761\n",
            "step :  1314 validation loss :  9521160.666666666\n",
            "step :  1315 training loss :  11421312.11904762\n",
            "step :  1315 validation loss :  9504079.833333334\n",
            "step :  1316 training loss :  11402054.357142856\n",
            "step :  1316 validation loss :  9487358.666666666\n",
            "step :  1317 training loss :  11383174.738095239\n",
            "step :  1317 validation loss :  9471404.0\n",
            "step :  1318 training loss :  11364692.666666666\n",
            "step :  1318 validation loss :  9454848.666666666\n",
            "step :  1319 training loss :  11346438.92857143\n",
            "step :  1319 validation loss :  9439031.666666666\n",
            "step :  1320 training loss :  11328528.69047619\n",
            "step :  1320 validation loss :  9423474.666666666\n",
            "step :  1321 training loss :  11316733.61904762\n",
            "step :  1321 validation loss :  9408169.0\n",
            "step :  1322 training loss :  11293557.11904762\n",
            "step :  1322 validation loss :  9393118.333333334\n",
            "step :  1323 training loss :  11276470.5\n",
            "step :  1323 validation loss :  9378284.333333334\n",
            "step :  1324 training loss :  11259621.285714285\n",
            "step :  1324 validation loss :  9363668.833333334\n",
            "step :  1325 training loss :  11413010.238095239\n",
            "step :  1325 validation loss :  9514922.333333334\n",
            "step :  1326 training loss :  11415919.357142856\n",
            "step :  1326 validation loss :  9500885.0\n",
            "step :  1327 training loss :  11399990.42857143\n",
            "step :  1327 validation loss :  9486989.166666666\n",
            "step :  1328 training loss :  11384268.142857144\n",
            "step :  1328 validation loss :  9473301.333333334\n",
            "step :  1329 training loss :  11368771.785714285\n",
            "step :  1329 validation loss :  9459812.333333334\n",
            "step :  1330 training loss :  11353491.61904762\n",
            "step :  1330 validation loss :  9446514.333333334\n",
            "step :  1331 training loss :  11338415.38095238\n",
            "step :  1331 validation loss :  9433395.833333334\n",
            "step :  1332 training loss :  11323532.642857144\n",
            "step :  1332 validation loss :  9420447.166666666\n",
            "step :  1333 training loss :  11308832.095238095\n",
            "step :  1333 validation loss :  9407660.5\n",
            "step :  1334 training loss :  11294306.261904761\n",
            "step :  1334 validation loss :  9395027.333333334\n",
            "step :  1335 training loss :  11279945.38095238\n",
            "step :  1335 validation loss :  9382541.666666666\n",
            "step :  1336 training loss :  11265741.952380951\n",
            "step :  1336 validation loss :  9370195.0\n",
            "step :  1337 training loss :  11251690.19047619\n",
            "step :  1337 validation loss :  9357983.333333334\n",
            "step :  1338 training loss :  11237782.833333334\n",
            "step :  1338 validation loss :  9345900.166666666\n",
            "step :  1339 training loss :  11224012.547619049\n",
            "step :  1339 validation loss :  9333939.5\n",
            "step :  1340 training loss :  11210373.833333334\n",
            "step :  1340 validation loss :  9322095.666666666\n",
            "step :  1341 training loss :  11196863.476190476\n",
            "step :  1341 validation loss :  9310368.166666666\n",
            "step :  1342 training loss :  11183471.523809524\n",
            "step :  1342 validation loss :  9298720.0\n",
            "step :  1343 training loss :  11170194.642857144\n",
            "step :  1343 validation loss :  9287205.0\n",
            "step :  1344 training loss :  11157040.333333334\n",
            "step :  1344 validation loss :  9275796.166666666\n",
            "step :  1345 training loss :  11143996.61904762\n",
            "step :  1345 validation loss :  9264487.5\n",
            "step :  1346 training loss :  11131058.595238095\n",
            "step :  1346 validation loss :  9253274.0\n",
            "step :  1347 training loss :  11118217.69047619\n",
            "step :  1347 validation loss :  9242148.333333334\n",
            "step :  1348 training loss :  11105459.357142856\n",
            "step :  1348 validation loss :  9231093.666666666\n",
            "step :  1349 training loss :  11135051.904761905\n",
            "step :  1349 validation loss :  9295834.666666666\n",
            "step :  1350 training loss :  11167712.42857143\n",
            "step :  1350 validation loss :  9285814.666666666\n",
            "step :  1351 training loss :  11156242.095238095\n",
            "step :  1351 validation loss :  9275853.5\n",
            "step :  1352 training loss :  11144847.285714285\n",
            "step :  1352 validation loss :  9265968.0\n",
            "step :  1353 training loss :  11133534.80952381\n",
            "step :  1353 validation loss :  9256159.666666666\n",
            "step :  1354 training loss :  11122303.095238095\n",
            "step :  1354 validation loss :  9246423.0\n",
            "step :  1355 training loss :  11111147.833333334\n",
            "step :  1355 validation loss :  9236755.5\n",
            "step :  1356 training loss :  11100067.857142856\n",
            "step :  1356 validation loss :  9227158.166666666\n",
            "step :  1357 training loss :  11089059.976190476\n",
            "step :  1357 validation loss :  9217617.5\n",
            "step :  1358 training loss :  11078090.214285715\n",
            "step :  1358 validation loss :  9208097.166666666\n",
            "step :  1359 training loss :  11067085.285714285\n",
            "step :  1359 validation loss :  9198553.166666666\n",
            "step :  1360 training loss :  11294674.642857144\n",
            "step :  1360 validation loss :  9585244.666666666\n",
            "step :  1361 training loss :  11497739.452380951\n",
            "step :  1361 validation loss :  9574390.0\n",
            "step :  1362 training loss :  11485165.285714285\n",
            "step :  1362 validation loss :  9563250.666666666\n",
            "step :  1363 training loss :  11472524.166666666\n",
            "step :  1363 validation loss :  9552166.5\n",
            "step :  1364 training loss :  11459967.5\n",
            "step :  1364 validation loss :  9541168.5\n",
            "step :  1365 training loss :  11447506.952380951\n",
            "step :  1365 validation loss :  9530259.0\n",
            "step :  1366 training loss :  11435139.214285715\n",
            "step :  1366 validation loss :  9519433.333333334\n",
            "step :  1367 training loss :  11504680.07142857\n",
            "step :  1367 validation loss :  9616302.0\n",
            "step :  1368 training loss :  11533035.904761905\n",
            "step :  1368 validation loss :  9605504.5\n",
            "step :  1369 training loss :  11520542.952380951\n",
            "step :  1369 validation loss :  9594421.333333334\n",
            "step :  1370 training loss :  11508012.595238095\n",
            "step :  1370 validation loss :  9583424.166666666\n",
            "step :  1371 training loss :  11495563.833333334\n",
            "step :  1371 validation loss :  9572510.0\n",
            "step :  1372 training loss :  11483204.261904761\n",
            "step :  1372 validation loss :  9561677.5\n",
            "step :  1373 training loss :  11470933.38095238\n",
            "step :  1373 validation loss :  9550923.5\n",
            "step :  1374 training loss :  11500891.738095239\n",
            "step :  1374 validation loss :  9540335.666666666\n",
            "step :  1375 training loss :  11446778.452380951\n",
            "step :  1375 validation loss :  9529758.833333334\n",
            "step :  1376 training loss :  11434744.333333334\n",
            "step :  1376 validation loss :  9519235.666666666\n",
            "step :  1377 training loss :  11422801.547619049\n",
            "step :  1377 validation loss :  9508780.166666666\n",
            "step :  1378 training loss :  11410939.595238095\n",
            "step :  1378 validation loss :  9498400.666666666\n",
            "step :  1379 training loss :  11399145.452380951\n",
            "step :  1379 validation loss :  9488080.166666666\n",
            "step :  1380 training loss :  11387424.42857143\n",
            "step :  1380 validation loss :  9477832.666666666\n",
            "step :  1381 training loss :  11375778.785714285\n",
            "step :  1381 validation loss :  9467651.166666666\n",
            "step :  1382 training loss :  11364204.07142857\n",
            "step :  1382 validation loss :  9457534.666666666\n",
            "step :  1383 training loss :  11352697.38095238\n",
            "step :  1383 validation loss :  9447481.166666666\n",
            "step :  1384 training loss :  11341258.0\n",
            "step :  1384 validation loss :  9437489.5\n",
            "step :  1385 training loss :  11329885.047619049\n",
            "step :  1385 validation loss :  9427558.333333334\n",
            "step :  1386 training loss :  11318576.452380951\n",
            "step :  1386 validation loss :  9417686.5\n",
            "step :  1387 training loss :  11307330.333333334\n",
            "step :  1387 validation loss :  9407873.666666666\n",
            "step :  1388 training loss :  11296146.404761905\n",
            "step :  1388 validation loss :  9398117.166666666\n",
            "step :  1389 training loss :  11285023.047619049\n",
            "step :  1389 validation loss :  9388417.333333334\n",
            "step :  1390 training loss :  11273958.904761905\n",
            "step :  1390 validation loss :  9378771.333333334\n",
            "step :  1391 training loss :  11262952.80952381\n",
            "step :  1391 validation loss :  9369179.5\n",
            "step :  1392 training loss :  11252003.785714285\n",
            "step :  1392 validation loss :  9359639.0\n",
            "step :  1393 training loss :  11241110.11904762\n",
            "step :  1393 validation loss :  9350150.333333334\n",
            "step :  1394 training loss :  11230271.047619049\n",
            "step :  1394 validation loss :  9340711.5\n",
            "step :  1395 training loss :  11219485.57142857\n",
            "step :  1395 validation loss :  9331321.333333334\n",
            "step :  1396 training loss :  11208752.0\n",
            "step :  1396 validation loss :  9321979.166666666\n",
            "step :  1397 training loss :  11198069.69047619\n",
            "step :  1397 validation loss :  9312683.5\n",
            "step :  1398 training loss :  11187438.738095239\n",
            "step :  1398 validation loss :  9303438.166666666\n",
            "step :  1399 training loss :  11176859.047619049\n",
            "step :  1399 validation loss :  9294240.333333334\n",
            "step :  1400 training loss :  11166329.42857143\n",
            "step :  1400 validation loss :  9285088.666666666\n",
            "step :  1401 training loss :  11155849.023809524\n",
            "step :  1401 validation loss :  9275981.833333334\n",
            "step :  1402 training loss :  11145416.38095238\n",
            "step :  1402 validation loss :  9266919.0\n",
            "step :  1403 training loss :  11135030.238095239\n",
            "step :  1403 validation loss :  9257898.833333334\n",
            "step :  1404 training loss :  11124691.11904762\n",
            "step :  1404 validation loss :  9248924.666666666\n",
            "step :  1405 training loss :  11114397.095238095\n",
            "step :  1405 validation loss :  9239990.833333334\n",
            "step :  1406 training loss :  11104144.07142857\n",
            "step :  1406 validation loss :  9231095.666666666\n",
            "step :  1407 training loss :  11093922.547619049\n",
            "step :  1407 validation loss :  9222227.0\n",
            "step :  1408 training loss :  11083736.0\n",
            "step :  1408 validation loss :  9213409.833333334\n",
            "step :  1409 training loss :  11073607.30952381\n",
            "step :  1409 validation loss :  9204563.0\n",
            "step :  1410 training loss :  11063478.69047619\n",
            "step :  1410 validation loss :  9195758.333333334\n",
            "step :  1411 training loss :  11528781.357142856\n",
            "step :  1411 validation loss :  9701804.666666666\n",
            "step :  1412 training loss :  11489334.214285715\n",
            "step :  1412 validation loss :  9527322.333333334\n",
            "step :  1413 training loss :  11460697.833333334\n",
            "step :  1413 validation loss :  9613541.333333334\n",
            "step :  1414 training loss :  11525509.88095238\n",
            "step :  1414 validation loss :  9595075.333333334\n",
            "step :  1415 training loss :  11505279.11904762\n",
            "step :  1415 validation loss :  9577896.666666666\n",
            "step :  1416 training loss :  11486281.642857144\n",
            "step :  1416 validation loss :  9561618.0\n",
            "step :  1417 training loss :  11468159.976190476\n",
            "step :  1417 validation loss :  9546006.833333334\n",
            "step :  1418 training loss :  11450701.238095239\n",
            "step :  1418 validation loss :  9530956.666666666\n",
            "step :  1419 training loss :  11433792.69047619\n",
            "step :  1419 validation loss :  9516297.333333334\n",
            "step :  1420 training loss :  11417299.357142856\n",
            "step :  1420 validation loss :  9501983.666666666\n",
            "step :  1421 training loss :  11401172.952380951\n",
            "step :  1421 validation loss :  9487984.0\n",
            "step :  1422 training loss :  11385369.523809524\n",
            "step :  1422 validation loss :  9474253.5\n",
            "step :  1423 training loss :  11369849.761904761\n",
            "step :  1423 validation loss :  9460754.333333334\n",
            "step :  1424 training loss :  11382119.047619049\n",
            "step :  1424 validation loss :  9566092.5\n",
            "step :  1425 training loss :  11476289.404761905\n",
            "step :  1425 validation loss :  9555868.666666666\n",
            "step :  1426 training loss :  11464652.285714285\n",
            "step :  1426 validation loss :  9545659.666666666\n",
            "step :  1427 training loss :  11453077.476190476\n",
            "step :  1427 validation loss :  9535548.5\n",
            "step :  1428 training loss :  11441581.30952381\n",
            "step :  1428 validation loss :  9525459.333333334\n",
            "step :  1429 training loss :  11430127.857142856\n",
            "step :  1429 validation loss :  9515427.166666666\n",
            "step :  1430 training loss :  11418734.761904761\n",
            "step :  1430 validation loss :  9505450.666666666\n",
            "step :  1431 training loss :  11407400.714285715\n",
            "step :  1431 validation loss :  9495528.333333334\n",
            "step :  1432 training loss :  11396124.023809524\n",
            "step :  1432 validation loss :  9485658.5\n",
            "step :  1433 training loss :  11384903.904761905\n",
            "step :  1433 validation loss :  9475842.166666666\n",
            "step :  1434 training loss :  11373733.547619049\n",
            "step :  1434 validation loss :  9466075.5\n",
            "step :  1435 training loss :  11362622.833333334\n",
            "step :  1435 validation loss :  9456359.0\n",
            "step :  1436 training loss :  11351564.738095239\n",
            "step :  1436 validation loss :  9446692.5\n",
            "step :  1437 training loss :  11353547.61904762\n",
            "step :  1437 validation loss :  9516216.333333334\n",
            "step :  1438 training loss :  11420681.047619049\n",
            "step :  1438 validation loss :  9508130.833333334\n",
            "step :  1439 training loss :  11411463.976190476\n",
            "step :  1439 validation loss :  9500045.666666666\n",
            "step :  1440 training loss :  11402269.285714285\n",
            "step :  1440 validation loss :  9491993.833333334\n",
            "step :  1441 training loss :  11393113.047619049\n",
            "step :  1441 validation loss :  9483976.5\n",
            "step :  1442 training loss :  11383994.0\n",
            "step :  1442 validation loss :  9475993.333333334\n",
            "step :  1443 training loss :  11374911.19047619\n",
            "step :  1443 validation loss :  9468044.666666666\n",
            "step :  1444 training loss :  11365864.238095239\n",
            "step :  1444 validation loss :  9460127.333333334\n",
            "step :  1445 training loss :  11356851.404761905\n",
            "step :  1445 validation loss :  9452243.5\n",
            "step :  1446 training loss :  11347873.5\n",
            "step :  1446 validation loss :  9444391.0\n",
            "step :  1447 training loss :  11338928.952380951\n",
            "step :  1447 validation loss :  9436569.333333334\n",
            "step :  1448 training loss :  11330017.476190476\n",
            "step :  1448 validation loss :  9428779.666666666\n",
            "step :  1449 training loss :  11321138.714285715\n",
            "step :  1449 validation loss :  9421019.333333334\n",
            "step :  1450 training loss :  11312291.69047619\n",
            "step :  1450 validation loss :  9413289.333333334\n",
            "step :  1451 training loss :  11303476.30952381\n",
            "step :  1451 validation loss :  9405588.666666666\n",
            "step :  1452 training loss :  11294691.666666666\n",
            "step :  1452 validation loss :  9397915.666666666\n",
            "step :  1453 training loss :  11230449.023809524\n",
            "step :  1453 validation loss :  9283167.833333334\n",
            "step :  1454 training loss :  11150910.07142857\n",
            "step :  1454 validation loss :  9269231.833333334\n",
            "step :  1455 training loss :  11135470.452380951\n",
            "step :  1455 validation loss :  9256373.333333334\n",
            "step :  1456 training loss :  11121251.38095238\n",
            "step :  1456 validation loss :  9244453.0\n",
            "step :  1457 training loss :  11107918.857142856\n",
            "step :  1457 validation loss :  9233178.166666666\n",
            "step :  1458 training loss :  11101225.285714285\n",
            "step :  1458 validation loss :  9222360.833333334\n",
            "step :  1459 training loss :  11082955.857142856\n",
            "step :  1459 validation loss :  9211842.0\n",
            "step :  1460 training loss :  11071032.88095238\n",
            "step :  1460 validation loss :  9201627.0\n",
            "step :  1461 training loss :  11059406.095238095\n",
            "step :  1461 validation loss :  9191645.166666666\n",
            "step :  1462 training loss :  11048013.0\n",
            "step :  1462 validation loss :  9181857.0\n",
            "step :  1463 training loss :  11036802.142857144\n",
            "step :  1463 validation loss :  9172209.833333334\n",
            "step :  1464 training loss :  11025708.666666666\n",
            "step :  1464 validation loss :  9162648.5\n",
            "step :  1465 training loss :  11030373.0\n",
            "step :  1465 validation loss :  9458719.833333334\n",
            "step :  1466 training loss :  11355625.261904761\n",
            "step :  1466 validation loss :  9450414.666666666\n",
            "step :  1467 training loss :  11342561.214285715\n",
            "step :  1467 validation loss :  9440015.833333334\n",
            "step :  1468 training loss :  11332514.19047619\n",
            "step :  1468 validation loss :  9429548.833333334\n",
            "step :  1469 training loss :  11320622.095238095\n",
            "step :  1469 validation loss :  9419202.166666666\n",
            "step :  1470 training loss :  11308859.095238095\n",
            "step :  1470 validation loss :  9408961.333333334\n",
            "step :  1471 training loss :  11297222.976190476\n",
            "step :  1471 validation loss :  9398843.5\n",
            "step :  1472 training loss :  11285720.857142856\n",
            "step :  1472 validation loss :  9388834.166666666\n",
            "step :  1473 training loss :  11274334.11904762\n",
            "step :  1473 validation loss :  9378923.5\n",
            "step :  1474 training loss :  11263051.952380951\n",
            "step :  1474 validation loss :  9369105.833333334\n",
            "step :  1475 training loss :  11251870.285714285\n",
            "step :  1475 validation loss :  9359363.666666666\n",
            "step :  1476 training loss :  11247299.19047619\n",
            "step :  1476 validation loss :  9389208.5\n",
            "step :  1477 training loss :  11274286.714285715\n",
            "step :  1477 validation loss :  9378305.333333334\n",
            "step :  1478 training loss :  11261746.761904761\n",
            "step :  1478 validation loss :  9367352.166666666\n",
            "step :  1479 training loss :  11249284.92857143\n",
            "step :  1479 validation loss :  9356529.833333334\n",
            "step :  1480 training loss :  11236977.57142857\n",
            "step :  1480 validation loss :  9346005.333333334\n",
            "step :  1481 training loss :  11324205.92857143\n",
            "step :  1481 validation loss :  9438046.166666666\n",
            "step :  1482 training loss :  11324463.714285715\n",
            "step :  1482 validation loss :  9417565.833333334\n",
            "step :  1483 training loss :  11302292.07142857\n",
            "step :  1483 validation loss :  9399169.666666666\n",
            "step :  1484 training loss :  11282104.452380951\n",
            "step :  1484 validation loss :  9382183.666666666\n",
            "step :  1485 training loss :  11247791.904761905\n",
            "step :  1485 validation loss :  9366796.833333334\n",
            "step :  1486 training loss :  11246488.523809524\n",
            "step :  1486 validation loss :  9351888.5\n",
            "step :  1487 training loss :  11229331.357142856\n",
            "step :  1487 validation loss :  9337125.666666666\n",
            "step :  1488 training loss :  11212596.214285715\n",
            "step :  1488 validation loss :  9322791.666666666\n",
            "step :  1489 training loss :  11196363.452380951\n",
            "step :  1489 validation loss :  9308816.166666666\n",
            "step :  1490 training loss :  11138077.738095239\n",
            "step :  1490 validation loss :  9247755.5\n",
            "step :  1491 training loss :  11105460.714285715\n",
            "step :  1491 validation loss :  9225695.5\n",
            "step :  1492 training loss :  11081362.452380951\n",
            "step :  1492 validation loss :  9205789.333333334\n",
            "step :  1493 training loss :  11059279.38095238\n",
            "step :  1493 validation loss :  9187296.0\n",
            "step :  1494 training loss :  11042371.738095239\n",
            "step :  1494 validation loss :  9243886.5\n",
            "step :  1495 training loss :  11106595.38095238\n",
            "step :  1495 validation loss :  9231448.166666666\n",
            "step :  1496 training loss :  11092326.80952381\n",
            "step :  1496 validation loss :  9259398.666666666\n",
            "step :  1497 training loss :  11119522.523809524\n",
            "step :  1497 validation loss :  9237602.166666666\n",
            "step :  1498 training loss :  11094348.666666666\n",
            "step :  1498 validation loss :  9216531.5\n",
            "step :  1499 training loss :  11071304.404761905\n",
            "step :  1499 validation loss :  9197379.5\n"
          ]
        }
      ],
      "source": [
        "mv_net.train()\n",
        "best_loss = 10000000\n",
        "loss_plot = []\n",
        "val_loss_plot = []\n",
        "for t in range(train_episodes):\n",
        "    step_loss = 0\n",
        "    for b in range(0, len(X_train), batch_size):\n",
        "        if b + batch_size > len(X_train):\n",
        "            break\n",
        "        inpt = X_train[\n",
        "            b : b + batch_size, :, :\n",
        "        ]  # /np.linalg.norm(X_train[b:b+batch_size,:,:])\n",
        "        target = y_train[\n",
        "            b : b + batch_size\n",
        "        ]  # /np.linalg.norm(y_train[b:b+batch_size])\n",
        "        if target.shape[0] != 0:\n",
        "            x_batch = (\n",
        "                torch.from_numpy(inpt).float().to(device)\n",
        "            )  # torch.tensor(inpt,dtype=torch.float32)\n",
        "            y_batch = torch.from_numpy(\n",
        "                target\n",
        "            ).float()  # torch.tensor(target,dtype=torch.float32)\n",
        "            mv_net.init_hidden(x_batch.size(0))\n",
        "            output = mv_net(x_batch)\n",
        "            loss = criterion(output.cpu().view(-1), np.transpose(y_batch))\n",
        "            # infect_dist = torch.distributions.normal.Normal(y_batch, 0.1)\n",
        "            # loss = -infect_dist.log_prob(output.squeeze().cpu()).mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            step_loss += loss.item()\n",
        "\n",
        "    # validation\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for b in range(0, len(X_valid), batch_size):\n",
        "            if b + batch_size > len(X_valid):\n",
        "                break\n",
        "            test_seq = X_valid[b : b + batch_size, :, :]  \n",
        "            label_seq = y_valid[\n",
        "                b : b + batch_size\n",
        "            ]  \n",
        "            x_batch = torch.from_numpy(test_seq).float().to(device)\n",
        "            y_batch = torch.from_numpy(label_seq).float()\n",
        "            mv_net.init_hidden(x_batch.size(0))\n",
        "            try:\n",
        "                output = mv_net(x_batch)\n",
        "                batch_val_loss = criterion(output.cpu().view(-1), np.transpose(y_batch))\n",
        "                infect_dist = torch.distributions.normal.Normal(y_batch, 0.1)\n",
        "                # batch_val_loss = -infect_dist.log_prob(\n",
        "                #     output.squeeze().cpu()\n",
        "                # ).mean()\n",
        "                val_loss += batch_val_loss.item()\n",
        "            except:\n",
        "                continue\n",
        "                \n",
        "    num_batches_train = len(X_train) // batch_size\n",
        "    num_batches_test = len(X_valid) // batch_size\n",
        "    train_loss = step_loss / num_batches_train\n",
        "    valid_loss = val_loss / num_batches_test\n",
        "    loss_plot.append(train_loss)\n",
        "    val_loss_plot.append(valid_loss)\n",
        "    if valid_loss < best_loss:\n",
        "        torch.save(mv_net.state_dict(), '/content/mse_lstm_state_dict_model.pt')\n",
        "        best_loss = valid_loss\n",
        "\n",
        "    print(\"step : \", t, \"training loss : \", train_loss)\n",
        "    print(\"step : \", t, \"validation loss : \", valid_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "6WQnbkFOfrlZ",
        "outputId": "7d600333-185c-414d-9944-44bef1841f45"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Visualize training loss\n",
        "updates = [i for i in range(1, len(loss_plot) + 1)]\n",
        "plt.plot(updates, loss_plot, label=\"Training loss\")\n",
        "plt.plot(updates, val_loss_plot, label=\"Validation loss\")\n",
        "plt.title(\"MSE Loss Curve (batch_size=128, lr=0.001)\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "noeSVIFBnvdc"
      },
      "outputs": [],
      "source": [
        "# mv_net = MV_NET(n_features,n_timesteps)\n",
        "# mv_net.load_state_dict(torch.load('/content/mle_lstm_state_dict_model.pt'))\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# mv_net.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# preds = []\n",
        "# labels = []\n",
        "# losses = []\n",
        "# running_loss = 0\n",
        "\n",
        "# for j in range(0, 250*21, 250):\n",
        "#     X_trapolate = X_test[j+100:j+250]\n",
        "#     test_seq = X_test[j:j+1]\n",
        "#     pred = []\n",
        "#     label = []\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(150):\n",
        "#             x_batch = torch.from_numpy(test_seq).float().to(device)\n",
        "#             mv_net.init_hidden(x_batch.size(0))\n",
        "#             output = mv_net(x_batch)\n",
        "#             t = output.cpu().view(-1).numpy()[0]\n",
        "#             # Produce output of the extrapolation\n",
        "#             pred.append(t)\n",
        "#             # import pdb; pdb.set_trace()\n",
        "#             label.append(X_trapolate[i][0][3])\n",
        "#             # Update test seq\n",
        "#             np_to_add = X_trapolate[i][0]\n",
        "#             np_to_add[-1] = t\n",
        "#             arr = test_seq.tolist()\n",
        "#             del arr[0][0]\n",
        "#             arr[0].append(np_to_add)\n",
        "#             test_seq = np.array(arr)\n",
        "#     infect_dist = torch.distributions.normal.Normal(torch.FloatTensor(label), 0.1)\n",
        "#     loss = -infect_dist.log_prob(torch.FloatTensor(pred).squeeze()).mean()\n",
        "#     running_loss += loss.item()\n",
        "#     #loss = mean_squared_error(pred, label)\n",
        "#     #running_loss += loss\n",
        "#     losses.append(loss)\n",
        "#     preds.append(pred)\n",
        "#     labels.append(label)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize preds vs labels\n",
        "for j in range(21):\n",
        "  pred = preds[j]\n",
        "  label = labels[j]\n",
        "  updates = [i for i in range(1, 151)]\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  plt.plot(updates, pred, label=\"Prediction\")\n",
        "  plt.plot(updates, label, label=\"Groud Truth\")\n",
        "  plt.title(\"Prediction vs Groud Truth (MLE extrapolated)\")\n",
        "  plt.xlabel(\"Steps\")\n",
        "  plt.ylabel(\"num_infect\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  #plt.savefig('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "UASEP3KZexEn",
        "outputId": "9bec0011-6512-4f08-d1bd-68f37c4fd65a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9d6c1865f87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize preds vs labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m151\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualize preds vs labels\n",
        "# updates = [i for i in range(1, len(preds) + 1)]\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# plt.plot(updates, preds[:200], label=\"Prediction\")\n",
        "# plt.plot(updates[:200], labels[:200], label=\"Groud Truth\")\n",
        "# plt.title(\"Prediction vs Groud Truth (first 200 timesteps)\")\n",
        "# plt.xlabel(\"Steps\")\n",
        "# plt.ylabel(\"Closed Price (USD)\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Krea067DYoMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnT1rvc_aKnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}