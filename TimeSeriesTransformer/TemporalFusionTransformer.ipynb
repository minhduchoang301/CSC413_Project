{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12857f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,TorchNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd14db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../bitcoin_daily.csv\")\n",
    "df.converted_ts = pd.to_datetime(df.converted_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de92dcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3375 entries, 0 to 3377\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   converted_ts       3375 non-null   datetime64[ns]\n",
      " 1   Timestamp          3375 non-null   float64       \n",
      " 2   Open               3375 non-null   float64       \n",
      " 3   High               3375 non-null   float64       \n",
      " 4   Low                3375 non-null   float64       \n",
      " 5   Close              3375 non-null   float64       \n",
      " 6   Volume_(BTC)       3375 non-null   float64       \n",
      " 7   Volume_(Currency)  3375 non-null   float64       \n",
      " 8   Weighted_Price     3375 non-null   float64       \n",
      " 9   time_idx           3375 non-null   int64         \n",
      " 10  month              3375 non-null   category      \n",
      "dtypes: category(1), datetime64[ns](1), float64(8), int64(1)\n",
      "memory usage: 293.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>converted_ts</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>month</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>1.325351e+09</td>\n",
       "      <td>4.488000</td>\n",
       "      <td>4.502000</td>\n",
       "      <td>4.488000</td>\n",
       "      <td>4.502000</td>\n",
       "      <td>19.363976</td>\n",
       "      <td>86.439900</td>\n",
       "      <td>4.493282</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1.325445e+09</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>10.050000</td>\n",
       "      <td>49.450000</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.048000</td>\n",
       "      <td>95.240000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>1.325610e+09</td>\n",
       "      <td>5.216667</td>\n",
       "      <td>5.216667</td>\n",
       "      <td>5.216667</td>\n",
       "      <td>5.216667</td>\n",
       "      <td>10.039698</td>\n",
       "      <td>52.915868</td>\n",
       "      <td>5.216667</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>1.325695e+09</td>\n",
       "      <td>5.334000</td>\n",
       "      <td>5.355000</td>\n",
       "      <td>5.334000</td>\n",
       "      <td>5.355000</td>\n",
       "      <td>11.511326</td>\n",
       "      <td>61.513860</td>\n",
       "      <td>5.341343</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>1.616774e+09</td>\n",
       "      <td>53713.961911</td>\n",
       "      <td>53744.860820</td>\n",
       "      <td>53685.498756</td>\n",
       "      <td>53715.866678</td>\n",
       "      <td>3.335749</td>\n",
       "      <td>178897.580671</td>\n",
       "      <td>53714.112556</td>\n",
       "      <td>3373</td>\n",
       "      <td>3</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>1.616861e+09</td>\n",
       "      <td>55368.343291</td>\n",
       "      <td>55394.514668</td>\n",
       "      <td>55343.783361</td>\n",
       "      <td>55370.368281</td>\n",
       "      <td>1.762685</td>\n",
       "      <td>97724.174269</td>\n",
       "      <td>55368.274704</td>\n",
       "      <td>3374</td>\n",
       "      <td>3</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>1.616947e+09</td>\n",
       "      <td>55743.054500</td>\n",
       "      <td>55766.689734</td>\n",
       "      <td>55720.313625</td>\n",
       "      <td>55744.439153</td>\n",
       "      <td>1.387384</td>\n",
       "      <td>77112.419739</td>\n",
       "      <td>55742.179404</td>\n",
       "      <td>3375</td>\n",
       "      <td>3</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>1.617034e+09</td>\n",
       "      <td>57235.672481</td>\n",
       "      <td>57267.246852</td>\n",
       "      <td>57205.425351</td>\n",
       "      <td>57236.598603</td>\n",
       "      <td>3.868826</td>\n",
       "      <td>221942.790902</td>\n",
       "      <td>57234.152701</td>\n",
       "      <td>3376</td>\n",
       "      <td>3</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>1.617113e+09</td>\n",
       "      <td>58548.495268</td>\n",
       "      <td>58573.017400</td>\n",
       "      <td>58525.093386</td>\n",
       "      <td>58550.412868</td>\n",
       "      <td>2.436852</td>\n",
       "      <td>142941.673239</td>\n",
       "      <td>58548.668066</td>\n",
       "      <td>3377</td>\n",
       "      <td>3</td>\n",
       "      <td>bitcoin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3375 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     converted_ts     Timestamp          Open          High           Low  \\\n",
       "0      2011-12-31  1.325351e+09      4.488000      4.502000      4.488000   \n",
       "1      2012-01-01  1.325445e+09      4.920000      4.920000      4.920000   \n",
       "2      2012-01-02  1.325535e+09      5.000000      5.000000      5.000000   \n",
       "3      2012-01-03  1.325610e+09      5.216667      5.216667      5.216667   \n",
       "4      2012-01-04  1.325695e+09      5.334000      5.355000      5.334000   \n",
       "...           ...           ...           ...           ...           ...   \n",
       "3373   2021-03-26  1.616774e+09  53713.961911  53744.860820  53685.498756   \n",
       "3374   2021-03-27  1.616861e+09  55368.343291  55394.514668  55343.783361   \n",
       "3375   2021-03-28  1.616947e+09  55743.054500  55766.689734  55720.313625   \n",
       "3376   2021-03-29  1.617034e+09  57235.672481  57267.246852  57205.425351   \n",
       "3377   2021-03-30  1.617113e+09  58548.495268  58573.017400  58525.093386   \n",
       "\n",
       "             Close  Volume_(BTC)  Volume_(Currency)  Weighted_Price  time_idx  \\\n",
       "0         4.502000     19.363976          86.439900        4.493282         0   \n",
       "1         4.920000     10.050000          49.450000        4.920000         1   \n",
       "2         5.000000     19.048000          95.240000        5.000000         2   \n",
       "3         5.216667     10.039698          52.915868        5.216667         3   \n",
       "4         5.355000     11.511326          61.513860        5.341343         4   \n",
       "...            ...           ...                ...             ...       ...   \n",
       "3373  53715.866678      3.335749      178897.580671    53714.112556      3373   \n",
       "3374  55370.368281      1.762685       97724.174269    55368.274704      3374   \n",
       "3375  55744.439153      1.387384       77112.419739    55742.179404      3375   \n",
       "3376  57236.598603      3.868826      221942.790902    57234.152701      3376   \n",
       "3377  58550.412868      2.436852      142941.673239    58548.668066      3377   \n",
       "\n",
       "     month constant  \n",
       "0       12  bitcoin  \n",
       "1        1  bitcoin  \n",
       "2        1  bitcoin  \n",
       "3        1  bitcoin  \n",
       "4        1  bitcoin  \n",
       "...    ...      ...  \n",
       "3373     3  bitcoin  \n",
       "3374     3  bitcoin  \n",
       "3375     3  bitcoin  \n",
       "3376     3  bitcoin  \n",
       "3377     3  bitcoin  \n",
       "\n",
       "[3375 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create time_idx\n",
    "df['time_idx'] = (df[\"converted_ts\"] - df[\"converted_ts\"].min()).dt.days\n",
    "\n",
    "#add features\n",
    "df[\"month\"] = df.converted_ts.dt.month.astype(str).astype(\"category\") \n",
    "df.dropna(inplace = True)\n",
    "df.info()\n",
    "\n",
    "#NEEDED FOR TIMESERIES DATASET\n",
    "df['constant'] = 'bitcoin'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4b79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 7\n",
    "max_encoder_length = 30\n",
    "training_cutoff = df[\"time_idx\"].max() - prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx= \"time_idx\",\n",
    "    target= \"Close\",\n",
    "    group_ids = ['constant'],\n",
    "    min_encoder_length=max_encoder_length//2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\"],  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Open\",\n",
    "        \"High\",\n",
    "        \"Low\",\n",
    "        \"Volume_(BTC)\",\n",
    "        \"Volume_(Currency)\",\n",
    "        \"Weighted_Price\",\n",
    "        \"Close\"\n",
    "    ],\n",
    "    target_normalizer=TorchNormalizer(transformation=\"softplus\"),  # use softplus and normalize\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps = True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8969f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd12726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1717.4140625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02814d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 867.7k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=128,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=SMAPE(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer='adam'\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ec1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 2750.3k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=500,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    #limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=160,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=160,\n",
    "    output_size=7,  # there are 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10, \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586cbee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 72    \n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 312 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 950 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 209 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 103 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 103 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 103 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 103 K \n",
      "11 | lstm_encoder                       | LSTM                            | 206 K \n",
      "12 | lstm_decoder                       | LSTM                            | 206 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 51.5 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 320   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 128 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 64.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 51.8 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 103 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 51.8 K\n",
      "20 | output_layer                       | Linear                          | 1.1 K \n",
      "----------------------------------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.001    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45e01108265438e85a668afddd9fc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = tft.predict(val_dataloader)\n",
    "\n",
    "#average p50 loss overall\n",
    "print((actuals - predictions).abs().mean().item())\n",
    "#average p50 loss per time series\n",
    "print((actuals - predictions).abs().mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c67a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 19:35:11,329]\u001b[0m A new study created in memory with name: no-name-135e6292-5044-4e12-849b-cc91111454c3\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_0 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 19:40:49,266]\u001b[0m Trial 0 finished with value: 1349.7696533203125 and parameters: {'gradient_clip_val': 0.4456196594106773, 'hidden_size': 36, 'dropout': 0.1502275864803421, 'hidden_continuous_size': 17, 'attention_head_size': 2, 'learning_rate': 0.015547921922026421}. Best is trial 0 with value: 1349.7696533203125.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_1 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 19:46:39,503]\u001b[0m Trial 1 finished with value: 1743.2803955078125 and parameters: {'gradient_clip_val': 0.045759551340306195, 'hidden_size': 90, 'dropout': 0.2222697890810163, 'hidden_continuous_size': 70, 'attention_head_size': 3, 'learning_rate': 0.010696477165012921}. Best is trial 0 with value: 1349.7696533203125.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_2 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:46:53,303]\u001b[0m Trial 2 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_3 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:47:07,137]\u001b[0m Trial 3 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_4 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:47:20,679]\u001b[0m Trial 4 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_5 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 19:52:31,887]\u001b[0m Trial 5 finished with value: 1048.9228515625 and parameters: {'gradient_clip_val': 0.8787121275852084, 'hidden_size': 21, 'dropout': 0.12656210603413226, 'hidden_continuous_size': 21, 'attention_head_size': 3, 'learning_rate': 0.07816406056603273}. Best is trial 5 with value: 1048.9228515625.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_6 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:54:26,943]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_7 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:54:40,496]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_8 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:54:54,369]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_9 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:55:08,665]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Tuan Nguyen\\CSC413_Project\\TimeSeriesTransformer\\optuna_test\\trial_10 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 19:55:24,646]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:55:39,027]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:55:53,179]\u001b[0m Trial 12 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:56:06,690]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 1.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:58:02,462]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:58:16,208]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:58:30,549]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:58:42,502]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:59:16,229]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:59:30,185]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:59:43,984]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 19:59:58,263]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:00:12,542]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:00:27,134]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:00:42,662]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:00:57,541]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:01:11,215]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:01:25,152]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:01:40,264]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:02:15,699]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:02:29,492]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:03:03,944]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:03:38,141]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:03:51,832]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:04:26,597]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:04:40,696]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:04:54,263]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:05:08,022]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:07:05,563]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:07:41,589]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:09:36,370]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:10:10,629]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 4.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:10:24,279]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:10:58,888]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:11:12,237]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:11:25,882]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:11:39,346]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:13:36,680]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:14:11,641]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:14:47,849]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:15:23,038]\u001b[0m Trial 50 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:15:58,037]\u001b[0m Trial 51 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:17:52,438]\u001b[0m Trial 52 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:18:06,155]\u001b[0m Trial 53 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:18:20,196]\u001b[0m Trial 54 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:18:34,036]\u001b[0m Trial 55 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:18:47,845]\u001b[0m Trial 56 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:19:23,492]\u001b[0m Trial 57 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:19:37,473]\u001b[0m Trial 58 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:19:50,859]\u001b[0m Trial 59 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:20:05,182]\u001b[0m Trial 60 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:21:59,816]\u001b[0m Trial 61 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:22:13,730]\u001b[0m Trial 62 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 20:27:49,291]\u001b[0m Trial 63 finished with value: 1649.9893798828125 and parameters: {'gradient_clip_val': 0.013370487379935896, 'hidden_size': 40, 'dropout': 0.17460040795846596, 'hidden_continuous_size': 23, 'attention_head_size': 3, 'learning_rate': 0.0905347454644839}. Best is trial 5 with value: 1048.9228515625.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 20:33:29,754]\u001b[0m Trial 64 finished with value: 1933.3031005859375 and parameters: {'gradient_clip_val': 0.013855800610551916, 'hidden_size': 42, 'dropout': 0.11665588106613115, 'hidden_continuous_size': 17, 'attention_head_size': 3, 'learning_rate': 0.09085531092227199}. Best is trial 5 with value: 1048.9228515625.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:35:28,396]\u001b[0m Trial 65 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:37:27,437]\u001b[0m Trial 66 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:39:23,695]\u001b[0m Trial 67 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:41:20,771]\u001b[0m Trial 68 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:41:33,677]\u001b[0m Trial 69 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:42:07,166]\u001b[0m Trial 70 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:42:39,817]\u001b[0m Trial 71 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:42:53,255]\u001b[0m Trial 72 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:43:06,723]\u001b[0m Trial 73 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:43:19,883]\u001b[0m Trial 74 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:43:32,842]\u001b[0m Trial 75 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:45:24,974]\u001b[0m Trial 76 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:45:38,041]\u001b[0m Trial 77 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:459: RuntimeWarning: invalid value encountered in subtract\n",
      "  score = log_l - log_g\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:45:49,993]\u001b[0m Trial 78 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:46:03,191]\u001b[0m Trial 79 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:46:16,171]\u001b[0m Trial 80 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:46:29,441]\u001b[0m Trial 81 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:46:42,736]\u001b[0m Trial 82 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:46:55,969]\u001b[0m Trial 83 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:47:09,049]\u001b[0m Trial 84 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:47:22,277]\u001b[0m Trial 85 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:47:34,091]\u001b[0m Trial 86 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:47:47,024]\u001b[0m Trial 87 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:48:19,736]\u001b[0m Trial 88 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:48:53,626]\u001b[0m Trial 89 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:49:06,709]\u001b[0m Trial 90 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:49:20,099]\u001b[0m Trial 91 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:49:33,336]\u001b[0m Trial 92 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:50:06,885]\u001b[0m Trial 93 pruned. Trial was pruned at epoch 4.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:50:20,013]\u001b[0m Trial 94 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:52:11,855]\u001b[0m Trial 95 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:52:44,480]\u001b[0m Trial 96 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:52:57,885]\u001b[0m Trial 97 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:53:11,484]\u001b[0m Trial 98 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:53:24,583]\u001b[0m Trial 99 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:53:37,778]\u001b[0m Trial 100 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:53:50,910]\u001b[0m Trial 101 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:54:04,053]\u001b[0m Trial 102 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:54:17,305]\u001b[0m Trial 103 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:56:09,014]\u001b[0m Trial 104 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:56:21,984]\u001b[0m Trial 105 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:58:14,103]\u001b[0m Trial 106 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:58:47,281]\u001b[0m Trial 107 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:59:20,979]\u001b[0m Trial 108 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 20:59:54,153]\u001b[0m Trial 109 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 21:05:33,754]\u001b[0m Trial 110 finished with value: 1831.049072265625 and parameters: {'gradient_clip_val': 0.5729167730259084, 'hidden_size': 82, 'dropout': 0.23718377696222104, 'hidden_continuous_size': 49, 'attention_head_size': 3, 'learning_rate': 0.049654351013064695}. Best is trial 5 with value: 1048.9228515625.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:06:09,344]\u001b[0m Trial 111 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:06:44,142]\u001b[0m Trial 112 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:08:42,792]\u001b[0m Trial 113 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:10:41,747]\u001b[0m Trial 114 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:10:55,956]\u001b[0m Trial 115 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:11:31,210]\u001b[0m Trial 116 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:11:45,129]\u001b[0m Trial 117 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:11:59,067]\u001b[0m Trial 118 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:12:14,409]\u001b[0m Trial 119 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:12:49,411]\u001b[0m Trial 120 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 21:18:29,377]\u001b[0m Trial 121 finished with value: 1369.8792724609375 and parameters: {'gradient_clip_val': 0.011561158558811674, 'hidden_size': 42, 'dropout': 0.10560388712700582, 'hidden_continuous_size': 16, 'attention_head_size': 2, 'learning_rate': 0.09746872821006893}. Best is trial 5 with value: 1048.9228515625.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:18:43,008]\u001b[0m Trial 122 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:18:56,601]\u001b[0m Trial 123 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:19:10,348]\u001b[0m Trial 124 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:19:24,509]\u001b[0m Trial 125 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:19:58,257]\u001b[0m Trial 126 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:20:32,386]\u001b[0m Trial 127 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:21:07,369]\u001b[0m Trial 128 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:21:21,786]\u001b[0m Trial 129 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:21:35,465]\u001b[0m Trial 130 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:23:31,134]\u001b[0m Trial 131 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:25:28,074]\u001b[0m Trial 132 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:27:24,183]\u001b[0m Trial 133 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 21:33:11,048]\u001b[0m Trial 134 finished with value: 1035.5843505859375 and parameters: {'gradient_clip_val': 0.04410108655253839, 'hidden_size': 40, 'dropout': 0.10277709390927758, 'hidden_continuous_size': 37, 'attention_head_size': 2, 'learning_rate': 0.08111225412149267}. Best is trial 134 with value: 1035.5843505859375.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:33:43,988]\u001b[0m Trial 135 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:33:56,136]\u001b[0m Trial 136 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:34:08,475]\u001b[0m Trial 137 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:34:22,146]\u001b[0m Trial 138 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:34:35,764]\u001b[0m Trial 139 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:35:08,619]\u001b[0m Trial 140 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:35:21,839]\u001b[0m Trial 141 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:35:35,155]\u001b[0m Trial 142 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:35:47,261]\u001b[0m Trial 143 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:36:00,408]\u001b[0m Trial 144 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:36:34,322]\u001b[0m Trial 145 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:36:47,686]\u001b[0m Trial 146 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:37:01,307]\u001b[0m Trial 147 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:37:14,286]\u001b[0m Trial 148 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:37:48,102]\u001b[0m Trial 149 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:38:00,008]\u001b[0m Trial 150 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:38:13,877]\u001b[0m Trial 151 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:38:47,623]\u001b[0m Trial 152 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:39:01,072]\u001b[0m Trial 153 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:39:14,541]\u001b[0m Trial 154 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:39:48,576]\u001b[0m Trial 155 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:40:22,069]\u001b[0m Trial 156 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:40:35,306]\u001b[0m Trial 157 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:41:08,255]\u001b[0m Trial 158 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:41:41,202]\u001b[0m Trial 159 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:41:54,577]\u001b[0m Trial 160 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 21:47:17,840]\u001b[0m Trial 161 finished with value: 2087.876953125 and parameters: {'gradient_clip_val': 0.014020328042655305, 'hidden_size': 42, 'dropout': 0.1241321020412649, 'hidden_continuous_size': 10, 'attention_head_size': 2, 'learning_rate': 0.08288512021557784}. Best is trial 134 with value: 1035.5843505859375.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:47:30,756]\u001b[0m Trial 162 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:47:43,410]\u001b[0m Trial 163 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:47:56,437]\u001b[0m Trial 164 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:48:09,056]\u001b[0m Trial 165 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:48:22,118]\u001b[0m Trial 166 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:48:54,207]\u001b[0m Trial 167 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:49:07,237]\u001b[0m Trial 168 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:49:20,238]\u001b[0m Trial 169 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:49:33,475]\u001b[0m Trial 170 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-03-15 21:54:47,530]\u001b[0m Trial 171 finished with value: 1447.916748046875 and parameters: {'gradient_clip_val': 0.044332418845103874, 'hidden_size': 41, 'dropout': 0.12119881168793863, 'hidden_continuous_size': 15, 'attention_head_size': 2, 'learning_rate': 0.087914594537326}. Best is trial 134 with value: 1035.5843505859375.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:56:36,857]\u001b[0m Trial 172 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:56:49,582]\u001b[0m Trial 173 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:57:02,047]\u001b[0m Trial 174 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:57:15,263]\u001b[0m Trial 175 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:57:27,836]\u001b[0m Trial 176 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:57:40,704]\u001b[0m Trial 177 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:57:53,682]\u001b[0m Trial 178 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:58:06,770]\u001b[0m Trial 179 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:58:19,629]\u001b[0m Trial 180 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:58:32,417]\u001b[0m Trial 181 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:58:45,066]\u001b[0m Trial 182 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 21:59:17,034]\u001b[0m Trial 183 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:459: RuntimeWarning: invalid value encountered in subtract\n",
      "  score = log_l - log_g\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 21:59:28,605]\u001b[0m Trial 184 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:00:00,929]\u001b[0m Trial 185 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 22:00:13,647]\u001b[0m Trial 186 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:00:26,294]\u001b[0m Trial 187 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:00:58,798]\u001b[0m Trial 188 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 22:01:11,590]\u001b[0m Trial 189 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:01:24,290]\u001b[0m Trial 190 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:01:56,180]\u001b[0m Trial 191 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 22:02:08,878]\u001b[0m Trial 192 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:02:21,634]\u001b[0m Trial 193 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:02:34,118]\u001b[0m Trial 194 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 22:02:47,089]\u001b[0m Trial 195 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:02:59,694]\u001b[0m Trial 196 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:03:32,083]\u001b[0m Trial 197 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 22:04:03,358]\u001b[0m Trial 198 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py:188: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  coefficient = 1 / z / p_accept\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Tuan Nguyen\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2023-03-15 22:05:50,242]\u001b[0m Trial 199 pruned. Trial was pruned at epoch 16.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_clip_val': 0.04410108655253839, 'hidden_size': 40, 'dropout': 0.10277709390927758, 'hidden_continuous_size': 37, 'attention_head_size': 2, 'learning_rate': 0.08111225412149267}\n"
     ]
    }
   ],
   "source": [
    "##run for HYPERPARAMETER TUNING, TAKES VERY LONG TIME\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# # create study\n",
    "# study = optimize_hyperparameters(\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     model_path=\"optuna_test\",\n",
    "#     n_trials=200,\n",
    "#     max_epochs=50,\n",
    "#     gradient_clip_val_range=(0.01, 1.0),\n",
    "#     hidden_size_range=(8, 128),\n",
    "#     hidden_continuous_size_range=(8, 128),\n",
    "#     attention_head_size_range=(1, 4),\n",
    "#     learning_rate_range=(0.001, 0.1),\n",
    "#     dropout_range=(0.1, 0.3),\n",
    "#     trainer_kwargs=dict(limit_train_batches=30),\n",
    "#     reduce_on_plateau_patience=4,\n",
    "#     use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "# )\n",
    "\n",
    "# # save study results - also we can resume tuning at a later point in time\n",
    "# with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "#     pickle.dump(study, fout)\n",
    "\n",
    "# # show best hyperparameters\n",
    "# print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb3322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'epoch=46-step=1222.ckpt'\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9738924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2820.0708)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313d9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e62aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(1):  # plot 1 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ea8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(10):  # plot 10 examples\n",
    "#     best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
